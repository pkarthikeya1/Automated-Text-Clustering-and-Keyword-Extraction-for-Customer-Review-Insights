{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\karthikeya\\\\New_Delhi_Reviews\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\karthikeya\\\\New_Delhi_Reviews'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import joblib as jl\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from src.logger import logger\n",
    "from src.components.data_ingestion import DataIngestionConfig, DataIngestion\n",
    "from src.constants import num_columns, text_columns\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file :str = os.path.join(\"artifacts\", \"preprocessor.joblib\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataColumns:\n",
    "    Numerical_Columns = num_columns\n",
    "    Text_Columns = text_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DataIngestionConfig.train_data_path)\n",
    "df.dropna(inplace=True)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [third, fourth, visit, restaurant, past, year,...\n",
       "1         [well, well, birthday, party, wanted, make, bi...\n",
       "2         [located, bang, opposite, pvr, plaza, outlet, ...\n",
       "3         [came, eat, late, lunch, group, , good, time, ...\n",
       "4         [leather, sofas, colonial, feeling, add, charm...\n",
       "                                ...                        \n",
       "103301    [heritage, zaika, nice, place, enjoy, good, fo...\n",
       "103302    [hard, locate, , good, taste, , good, menu, , ...\n",
       "103303    [want, finger, licious, south, indian, cuisine...\n",
       "103304    [great, breakfast, huge, range, careals, fruit...\n",
       "103305    [, italian, dishes, big, chill, amazing, varie...\n",
       "Name: review_full, Length: 103304, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'[^a-zA-Z]+')\n",
    "\n",
    "df['review_full']=df['review_full'].apply(lambda x : x.lower())\n",
    "df['review_full'] = df['review_full'].apply(lambda x: x.split())\n",
    "# Precompile the regular expression\n",
    "# df['review_full'] = df['review_full'].apply(lambda x : [re.sub(r'[^a-zA-Z0-9]', ' ', word) for word in x])\n",
    "\n",
    "df['review_full'] = df['review_full'].apply(lambda x: [pattern.sub('', word) for word in x])\n",
    "# df['review_full']=df['review_full'].apply(lambda x: [pttrn.sub('', word) for word in x])\n",
    "df['review_full'] = df['review_full'].apply(lambda x : [word for word in x if not word in stop_words])\n",
    "df['review_full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m lemm \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mlemm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\New_Delhi_Reviews\\venv\\lib\\site-packages\\nltk\\stem\\wordnet.py:85\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` by picking the shortest of the possible lemmas,\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    using the wordnet corpus reader's built-in _morphy function.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    :return: The shortest lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\New_Delhi_Reviews\\venv\\lib\\site-packages\\nltk\\stem\\wordnet.py:41\u001b[0m, in \u001b[0;36mWordNetLemmatizer._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m_morphy() is WordNet's _morphy lemmatizer.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mIt returns a list of all lemmas found in WordNet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m['us', 'u']\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_exceptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\New_Delhi_Reviews\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2106\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   2103\u001b[0m     forms \u001b[38;5;241m=\u001b[39m exceptions[form]\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;66;03m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[39;00m\n\u001b[1;32m-> 2106\u001b[0m     forms \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mform\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;66;03m# 2. Return all that are in the database (and check the original too)\u001b[39;00m\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filter_forms([form] \u001b[38;5;241m+\u001b[39m forms)\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\New_Delhi_Reviews\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2083\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy.<locals>.apply_rules\u001b[1;34m(forms)\u001b[0m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n\u001b[1;32m-> 2083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   2084\u001b[0m         form[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(old)] \u001b[38;5;241m+\u001b[39m new\n\u001b[0;32m   2085\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m form \u001b[38;5;129;01min\u001b[39;00m forms\n\u001b[0;32m   2086\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m old, new \u001b[38;5;129;01min\u001b[39;00m substitutions\n\u001b[0;32m   2087\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m form\u001b[38;5;241m.\u001b[39mendswith(old)\n\u001b[0;32m   2088\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\karthikeya\\New_Delhi_Reviews\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2087\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n\u001b[0;32m   2083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   2084\u001b[0m         form[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(old)] \u001b[38;5;241m+\u001b[39m new\n\u001b[0;32m   2085\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m form \u001b[38;5;129;01min\u001b[39;00m forms\n\u001b[0;32m   2086\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m old, new \u001b[38;5;129;01min\u001b[39;00m substitutions\n\u001b[1;32m-> 2087\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m(old)\n\u001b[0;32m   2088\u001b[0m     ]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "cleaned_text = re.sub(r'[0-9]+', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class remove_stop_words(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"getting set of stopwords from NLTK from English language\")\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error loading set of stopwords from NLTK library: {e}\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.X_ = X.copy()\n",
    "        try:\n",
    "            logger.info(f\"Transforming all the letters into lowercase\")\n",
    "            self.X_ = self.X_.apply(lambda x: x.str.lower())\n",
    "            logger.info(f\"Transforming all letters into lowercase is successful\")\n",
    "\n",
    "            logger.info(f\"Splitting the sentences into words (Tokenization)\")\n",
    "            self.X_ = self.X_.apply(lambda x: x.str.split())\n",
    "            logger.info(f\"Splitting the sentences into words is successful (Tokenization)\")\n",
    "\n",
    "            pattern = re.compile(r'[^a-zA-Z\\s]+')  # Allow spaces\n",
    "            logger.info(f\"Removing punctuations from text\")\n",
    "            self.X_ = self.X_.map(lambda x: [pattern.sub('', word) for word in x])\n",
    "            logger.info(f\"Removing punctuations from text successful\")\n",
    "\n",
    "\n",
    "            logger.info(f\"Removing the stopwords from the tokenized words\")\n",
    "            self.X_ = self.X_.map(lambda x: [word for word in x if not word in self.stop_words ])\n",
    "            logger.info(f\"Removing the stopwords from the tokenized words is successful\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error in pre-processing the text: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        return self.X_\n",
    "\n",
    "\n",
    "class lemmatization(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__ (self):\n",
    "        try:\n",
    "            logger.info(f\"WordNetLemmatizer from NLTK library is instantiated\")\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error initiating the lemmatizer from NLTK library: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    \n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        self.X_ = X.copy()\n",
    "        try:\n",
    "            logger.info(f\"Initiating the lemmatization of the remaining words after stopword removal\")\n",
    "            self.X_ = self.X_.map(lambda x: [self.lemmatizer.lemmatize(word) for word in x])\n",
    "            logger.info(f\"Lemmatization successful\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error in lemmatization: {e}\")\n",
    "            raise e\n",
    "        return self.X_\n",
    "    \n",
    "class make_embeddings(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            logger.info(\"Loading Gensim Word2Vec model for embedding generation\")\n",
    "            self.word2vec = KeyedVectors.load_word2vec_format(\"ft_reviews_vectors.bin\", binary=True)  # Update path as needed\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error loading Gensim Word2Vec model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.X_ = X.copy()\n",
    "        try:\n",
    "            logger.info(\"Generating embeddings for the text data\")\n",
    "            \n",
    "            # Compute embeddings using Gensim's get_mean_vector method\n",
    "            embeddings = self.X_.map(lambda words: self.word2vec.get_mean_vector(words, ignore_missing=True))\n",
    "\n",
    "            logger.info(\"Successfully generated embeddings\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during transformation: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__ (self):\n",
    "        try:\n",
    "            logger.info(f\"Initiating the DataTransformation\")\n",
    "            self.data_transformation_config = DataTransformationConfig()\n",
    "            self.data_columns = DataColumns()\n",
    "            pass\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error initiating the DataTransformation : {e}\")\n",
    "            raise e\n",
    "        \n",
    "\n",
    "    def get_transformer_object(self):\n",
    "        try:\n",
    "            num_pipeline = Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            text_pipeline = Pipeline([\n",
    "                (\"clean and preprocess\", remove_stop_words()),\n",
    "                (\"lemmatize the words\", lemmatization()),\n",
    "                (\"Create Embeddings\", make_embeddings())\n",
    "            ])\n",
    "            \n",
    "            logger.info(\"Numerical and text pipelines created\")\n",
    "            \n",
    "            preprocessor = ColumnTransformer([\n",
    "                (\"Numerical_Pipeline\", num_pipeline, self.data_columns.Numerical_Columns),\n",
    "                (\"Text_Pipeline\", text_pipeline, self.data_columns.Text_Columns)\n",
    "            ])\n",
    "\n",
    "            return preprocessor  \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def initiate_data_transformation(self, train_data_path, test_data_path):\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Initiatig data transformation pipeline\")\n",
    "            train_data = pd.read_csv(train_data_path)\n",
    "            test_data = pd.read_csv(test_data_path)\n",
    "            train_data.dropna(inplace=True, axis=0)\n",
    "            test_data.dropna(inplace=True, axis=0)\n",
    "            \n",
    "\n",
    "\n",
    "            preprocessing_obj = self.get_transformer_object()\n",
    "            input_feature_arr = preprocessing_obj.fit_transform(train_data)\n",
    "            input_test_arr = preprocessing_obj.transform(test_data)\n",
    "\n",
    "            jl.dump(preprocessing_obj, self.data_transformation_config.preprocessor_obj_file)\n",
    "            logger.info(f\"Saved fitted preprocessor to {self.data_transformation_config.preprocessor_obj_file}\")\n",
    "\n",
    "            logger.info(f\"Returning the input train feature as an array and test feature as array respectively\")\n",
    "            return input_feature_arr, input_test_arr\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error in initiating the data transformation pipeline {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-14 16:24:26,944, word2vec, INFO, collecting all words and their counts ]\n",
      "[2024-12-14 16:24:26,946, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]\n",
      "[2024-12-14 16:24:27,061, word2vec, INFO, PROGRESS: at sentence #10000, processed 408439 words, keeping 26554 word types ]\n",
      "[2024-12-14 16:24:27,163, word2vec, INFO, PROGRESS: at sentence #20000, processed 809383 words, keeping 39092 word types ]\n",
      "[2024-12-14 16:24:27,264, word2vec, INFO, PROGRESS: at sentence #30000, processed 1218278 words, keeping 49917 word types ]\n",
      "[2024-12-14 16:24:27,377, word2vec, INFO, PROGRESS: at sentence #40000, processed 1623949 words, keeping 59450 word types ]\n",
      "[2024-12-14 16:24:27,462, word2vec, INFO, PROGRESS: at sentence #50000, processed 2028560 words, keeping 67824 word types ]\n",
      "[2024-12-14 16:24:27,564, word2vec, INFO, PROGRESS: at sentence #60000, processed 2431931 words, keeping 75714 word types ]\n",
      "[2024-12-14 16:24:27,655, word2vec, INFO, PROGRESS: at sentence #70000, processed 2832678 words, keeping 83130 word types ]\n",
      "[2024-12-14 16:24:27,753, word2vec, INFO, PROGRESS: at sentence #80000, processed 3245426 words, keeping 90629 word types ]\n",
      "[2024-12-14 16:24:27,841, word2vec, INFO, PROGRESS: at sentence #90000, processed 3650408 words, keeping 97905 word types ]\n",
      "[2024-12-14 16:24:27,928, word2vec, INFO, PROGRESS: at sentence #100000, processed 4047571 words, keeping 104455 word types ]\n",
      "[2024-12-14 16:24:27,971, word2vec, INFO, collected 106574 word types from a corpus of 4176922 raw words and 103304 sentences ]\n",
      "[2024-12-14 16:24:27,972, word2vec, INFO, Creating a fresh vocabulary ]\n",
      "[2024-12-14 16:24:28,489, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 106574 unique words (100.00% of original 106574, drops 0)', 'datetime': '2024-12-14T16:24:28.489497', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]\n",
      "[2024-12-14 16:24:28,490, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4176922 word corpus (100.00% of original 4176922, drops 0)', 'datetime': '2024-12-14T16:24:28.490495', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]\n",
      "[2024-12-14 16:24:29,217, word2vec, INFO, deleting the raw counts dictionary of 106574 items ]\n",
      "[2024-12-14 16:24:29,220, word2vec, INFO, sample=0.001 downsamples 35 most-common words ]\n",
      "[2024-12-14 16:24:29,222, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 3726514.7489644596 word corpus (89.2%% of prior 4176922)', 'datetime': '2024-12-14T16:24:29.222538', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]\n",
      "[2024-12-14 16:24:31,349, fasttext, INFO, estimated required memory for 106574 words, 2000000 buckets and 25 dimensions: 297459020 bytes ]\n",
      "[2024-12-14 16:24:31,352, word2vec, INFO, resetting layer weights ]\n",
      "[2024-12-14 16:24:37,802, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-14T16:24:37.802674', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]\n",
      "[2024-12-14 16:24:37,804, utils, INFO, FastText lifecycle event {'msg': 'training model with 8 workers on 106574 vocabulary and 25 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-14T16:24:37.804669', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]\n",
      "[2024-12-14 16:24:38,815, word2vec, INFO, EPOCH 0 - PROGRESS: at 6.16% examples, 230760 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:24:39,826, word2vec, INFO, EPOCH 0 - PROGRESS: at 13.31% examples, 247417 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:40,857, word2vec, INFO, EPOCH 0 - PROGRESS: at 21.17% examples, 259742 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:41,884, word2vec, INFO, EPOCH 0 - PROGRESS: at 27.52% examples, 253301 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:42,910, word2vec, INFO, EPOCH 0 - PROGRESS: at 34.37% examples, 252809 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:43,910, word2vec, INFO, EPOCH 0 - PROGRESS: at 41.18% examples, 253561 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:44,911, word2vec, INFO, EPOCH 0 - PROGRESS: at 49.24% examples, 259115 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:45,964, word2vec, INFO, EPOCH 0 - PROGRESS: at 57.01% examples, 261460 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:46,965, word2vec, INFO, EPOCH 0 - PROGRESS: at 64.31% examples, 261971 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:47,971, word2vec, INFO, EPOCH 0 - PROGRESS: at 71.59% examples, 263159 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:49,028, word2vec, INFO, EPOCH 0 - PROGRESS: at 79.09% examples, 263736 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:50,069, word2vec, INFO, EPOCH 0 - PROGRESS: at 86.73% examples, 264543 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:51,137, word2vec, INFO, EPOCH 0 - PROGRESS: at 94.45% examples, 264619 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:51,755, word2vec, INFO, EPOCH 0: training on 4176922 raw words (3726695 effective words) took 13.9s, 267315 effective words/s ]\n",
      "[2024-12-14 16:24:52,775, word2vec, INFO, EPOCH 1 - PROGRESS: at 5.44% examples, 202600 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:53,807, word2vec, INFO, EPOCH 1 - PROGRESS: at 11.04% examples, 204570 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:54,817, word2vec, INFO, EPOCH 1 - PROGRESS: at 16.89% examples, 206733 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:24:55,825, word2vec, INFO, EPOCH 1 - PROGRESS: at 22.82% examples, 210007 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:56,852, word2vec, INFO, EPOCH 1 - PROGRESS: at 28.44% examples, 209562 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:24:57,890, word2vec, INFO, EPOCH 1 - PROGRESS: at 33.93% examples, 207413 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:58,947, word2vec, INFO, EPOCH 1 - PROGRESS: at 39.63% examples, 206558 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:24:59,999, word2vec, INFO, EPOCH 1 - PROGRESS: at 45.27% examples, 206035 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:01,001, word2vec, INFO, EPOCH 1 - PROGRESS: at 51.13% examples, 206689 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:02,006, word2vec, INFO, EPOCH 1 - PROGRESS: at 57.28% examples, 208914 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:25:03,012, word2vec, INFO, EPOCH 1 - PROGRESS: at 63.31% examples, 209951 words/s, in_qsize 14, out_qsize 1 ]\n",
      "[2024-12-14 16:25:04,021, word2vec, INFO, EPOCH 1 - PROGRESS: at 68.99% examples, 210090 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:05,024, word2vec, INFO, EPOCH 1 - PROGRESS: at 74.85% examples, 210963 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:25:06,026, word2vec, INFO, EPOCH 1 - PROGRESS: at 80.70% examples, 211733 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:07,143, word2vec, INFO, EPOCH 1 - PROGRESS: at 86.73% examples, 210793 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:08,180, word2vec, INFO, EPOCH 1 - PROGRESS: at 92.55% examples, 210444 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:09,185, word2vec, INFO, EPOCH 1 - PROGRESS: at 98.36% examples, 210535 words/s, in_qsize 7, out_qsize 1 ]\n",
      "[2024-12-14 16:25:09,310, word2vec, INFO, EPOCH 1: training on 4176922 raw words (3726671 effective words) took 17.5s, 212401 effective words/s ]\n",
      "[2024-12-14 16:25:10,370, word2vec, INFO, EPOCH 2 - PROGRESS: at 5.18% examples, 186610 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:11,391, word2vec, INFO, EPOCH 2 - PROGRESS: at 11.06% examples, 201933 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:12,445, word2vec, INFO, EPOCH 2 - PROGRESS: at 17.39% examples, 207610 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:13,457, word2vec, INFO, EPOCH 2 - PROGRESS: at 23.28% examples, 210492 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:14,621, word2vec, INFO, EPOCH 2 - PROGRESS: at 29.42% examples, 207857 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:15,630, word2vec, INFO, EPOCH 2 - PROGRESS: at 35.57% examples, 211163 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:16,674, word2vec, INFO, EPOCH 2 - PROGRESS: at 41.41% examples, 211409 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:17,779, word2vec, INFO, EPOCH 2 - PROGRESS: at 47.78% examples, 211101 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:18,783, word2vec, INFO, EPOCH 2 - PROGRESS: at 53.27% examples, 210156 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:19,795, word2vec, INFO, EPOCH 2 - PROGRESS: at 59.23% examples, 211023 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:20,902, word2vec, INFO, EPOCH 2 - PROGRESS: at 65.29% examples, 210001 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:21,953, word2vec, INFO, EPOCH 2 - PROGRESS: at 71.59% examples, 211568 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:22,969, word2vec, INFO, EPOCH 2 - PROGRESS: at 77.42% examples, 212113 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:23,972, word2vec, INFO, EPOCH 2 - PROGRESS: at 82.84% examples, 211541 words/s, in_qsize 14, out_qsize 1 ]\n",
      "[2024-12-14 16:25:25,002, word2vec, INFO, EPOCH 2 - PROGRESS: at 88.94% examples, 211814 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:26,016, word2vec, INFO, EPOCH 2 - PROGRESS: at 95.17% examples, 212763 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:26,699, word2vec, INFO, EPOCH 2: training on 4176922 raw words (3726576 effective words) took 17.4s, 214451 effective words/s ]\n",
      "[2024-12-14 16:25:27,730, word2vec, INFO, EPOCH 3 - PROGRESS: at 4.94% examples, 183203 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:28,829, word2vec, INFO, EPOCH 3 - PROGRESS: at 10.78% examples, 193092 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:29,842, word2vec, INFO, EPOCH 3 - PROGRESS: at 16.70% examples, 198744 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:30,843, word2vec, INFO, EPOCH 3 - PROGRESS: at 22.57% examples, 204237 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:31,865, word2vec, INFO, EPOCH 3 - PROGRESS: at 28.69% examples, 208598 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:32,883, word2vec, INFO, EPOCH 3 - PROGRESS: at 34.61% examples, 210145 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:33,920, word2vec, INFO, EPOCH 3 - PROGRESS: at 39.86% examples, 207003 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:34,965, word2vec, INFO, EPOCH 3 - PROGRESS: at 46.06% examples, 208739 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:25:35,968, word2vec, INFO, EPOCH 3 - PROGRESS: at 51.60% examples, 208135 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:36,997, word2vec, INFO, EPOCH 3 - PROGRESS: at 57.50% examples, 208843 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:38,069, word2vec, INFO, EPOCH 3 - PROGRESS: at 63.83% examples, 209445 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:39,135, word2vec, INFO, EPOCH 3 - PROGRESS: at 69.68% examples, 209379 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:25:40,226, word2vec, INFO, EPOCH 3 - PROGRESS: at 75.78% examples, 209607 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:41,291, word2vec, INFO, EPOCH 3 - PROGRESS: at 81.63% examples, 209533 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:42,307, word2vec, INFO, EPOCH 3 - PROGRESS: at 87.71% examples, 210121 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:43,376, word2vec, INFO, EPOCH 3 - PROGRESS: at 94.01% examples, 210469 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:44,234, word2vec, INFO, EPOCH 3: training on 4176922 raw words (3726260 effective words) took 17.5s, 212642 effective words/s ]\n",
      "[2024-12-14 16:25:45,257, word2vec, INFO, EPOCH 4 - PROGRESS: at 4.44% examples, 167431 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:46,268, word2vec, INFO, EPOCH 4 - PROGRESS: at 10.56% examples, 197998 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:47,338, word2vec, INFO, EPOCH 4 - PROGRESS: at 16.65% examples, 201253 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:48,363, word2vec, INFO, EPOCH 4 - PROGRESS: at 22.59% examples, 205058 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:49,382, word2vec, INFO, EPOCH 4 - PROGRESS: at 28.44% examples, 207601 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:50,391, word2vec, INFO, EPOCH 4 - PROGRESS: at 34.39% examples, 209622 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:51,457, word2vec, INFO, EPOCH 4 - PROGRESS: at 40.27% examples, 209467 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:52,478, word2vec, INFO, EPOCH 4 - PROGRESS: at 46.55% examples, 211522 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:53,498, word2vec, INFO, EPOCH 4 - PROGRESS: at 52.79% examples, 213066 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:54,498, word2vec, INFO, EPOCH 4 - PROGRESS: at 57.99% examples, 211307 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:55,527, word2vec, INFO, EPOCH 4 - PROGRESS: at 63.81% examples, 210938 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:56,590, word2vec, INFO, EPOCH 4 - PROGRESS: at 69.92% examples, 211486 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:25:57,627, word2vec, INFO, EPOCH 4 - PROGRESS: at 75.77% examples, 211719 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:58,650, word2vec, INFO, EPOCH 4 - PROGRESS: at 81.84% examples, 212741 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:25:59,657, word2vec, INFO, EPOCH 4 - PROGRESS: at 87.68% examples, 212672 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:00,659, word2vec, INFO, EPOCH 4 - PROGRESS: at 93.54% examples, 212668 words/s, in_qsize 16, out_qsize 1 ]\n",
      "[2024-12-14 16:26:01,640, word2vec, INFO, EPOCH 4: training on 4176922 raw words (3726535 effective words) took 17.4s, 214256 effective words/s ]\n",
      "[2024-12-14 16:26:02,712, word2vec, INFO, EPOCH 5 - PROGRESS: at 4.51% examples, 159024 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:03,748, word2vec, INFO, EPOCH 5 - PROGRESS: at 10.56% examples, 190729 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:04,806, word2vec, INFO, EPOCH 5 - PROGRESS: at 16.65% examples, 197184 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:05,822, word2vec, INFO, EPOCH 5 - PROGRESS: at 22.34% examples, 200200 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:06,850, word2vec, INFO, EPOCH 5 - PROGRESS: at 28.21% examples, 203413 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:07,865, word2vec, INFO, EPOCH 5 - PROGRESS: at 34.17% examples, 205881 words/s, in_qsize 16, out_qsize 1 ]\n",
      "[2024-12-14 16:26:08,898, word2vec, INFO, EPOCH 5 - PROGRESS: at 40.08% examples, 207225 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:09,906, word2vec, INFO, EPOCH 5 - PROGRESS: at 46.06% examples, 208773 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:10,925, word2vec, INFO, EPOCH 5 - PROGRESS: at 51.86% examples, 208742 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:11,924, word2vec, INFO, EPOCH 5 - PROGRESS: at 57.74% examples, 209985 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:13,012, word2vec, INFO, EPOCH 5 - PROGRESS: at 63.83% examples, 209423 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:14,059, word2vec, INFO, EPOCH 5 - PROGRESS: at 69.92% examples, 210386 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:15,090, word2vec, INFO, EPOCH 5 - PROGRESS: at 75.99% examples, 211445 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:16,120, word2vec, INFO, EPOCH 5 - PROGRESS: at 81.84% examples, 211755 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:17,181, word2vec, INFO, EPOCH 5 - PROGRESS: at 87.94% examples, 211583 words/s, in_qsize 16, out_qsize 1 ]\n",
      "[2024-12-14 16:26:18,241, word2vec, INFO, EPOCH 5 - PROGRESS: at 93.99% examples, 211444 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:19,199, word2vec, INFO, EPOCH 5: training on 4176922 raw words (3726571 effective words) took 17.5s, 212361 effective words/s ]\n",
      "[2024-12-14 16:26:20,258, word2vec, INFO, EPOCH 6 - PROGRESS: at 5.21% examples, 186831 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:21,295, word2vec, INFO, EPOCH 6 - PROGRESS: at 11.04% examples, 200540 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:22,339, word2vec, INFO, EPOCH 6 - PROGRESS: at 17.15% examples, 204545 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:23,376, word2vec, INFO, EPOCH 6 - PROGRESS: at 22.82% examples, 204696 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:26:24,382, word2vec, INFO, EPOCH 6 - PROGRESS: at 28.44% examples, 206135 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:26:25,401, word2vec, INFO, EPOCH 6 - PROGRESS: at 34.37% examples, 208116 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:26,404, word2vec, INFO, EPOCH 6 - PROGRESS: at 40.29% examples, 209929 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:27,408, word2vec, INFO, EPOCH 6 - PROGRESS: at 45.79% examples, 209155 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:28,423, word2vec, INFO, EPOCH 6 - PROGRESS: at 51.12% examples, 207256 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:29,508, word2vec, INFO, EPOCH 6 - PROGRESS: at 56.58% examples, 205195 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:30,517, word2vec, INFO, EPOCH 6 - PROGRESS: at 62.81% examples, 207290 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:31,541, word2vec, INFO, EPOCH 6 - PROGRESS: at 68.75% examples, 208090 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:32,549, word2vec, INFO, EPOCH 6 - PROGRESS: at 74.67% examples, 209033 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:33,602, word2vec, INFO, EPOCH 6 - PROGRESS: at 80.24% examples, 208578 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:34,606, word2vec, INFO, EPOCH 6 - PROGRESS: at 85.74% examples, 208253 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:35,617, word2vec, INFO, EPOCH 6 - PROGRESS: at 91.35% examples, 207876 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:36,620, word2vec, INFO, EPOCH 6 - PROGRESS: at 97.61% examples, 209132 words/s, in_qsize 10, out_qsize 0 ]\n",
      "[2024-12-14 16:26:36,904, word2vec, INFO, EPOCH 6: training on 4176922 raw words (3726546 effective words) took 17.7s, 210619 effective words/s ]\n",
      "[2024-12-14 16:26:37,932, word2vec, INFO, EPOCH 7 - PROGRESS: at 4.94% examples, 184016 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:38,933, word2vec, INFO, EPOCH 7 - PROGRESS: at 10.10% examples, 189661 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:39,981, word2vec, INFO, EPOCH 7 - PROGRESS: at 15.53% examples, 188471 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:41,011, word2vec, INFO, EPOCH 7 - PROGRESS: at 21.41% examples, 195215 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:42,092, word2vec, INFO, EPOCH 7 - PROGRESS: at 27.76% examples, 200841 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:43,126, word2vec, INFO, EPOCH 7 - PROGRESS: at 33.45% examples, 201788 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:44,161, word2vec, INFO, EPOCH 7 - PROGRESS: at 38.91% examples, 201086 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:26:45,161, word2vec, INFO, EPOCH 7 - PROGRESS: at 44.77% examples, 203611 words/s, in_qsize 13, out_qsize 2 ]\n",
      "[2024-12-14 16:26:46,190, word2vec, INFO, EPOCH 7 - PROGRESS: at 51.36% examples, 206851 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:47,238, word2vec, INFO, EPOCH 7 - PROGRESS: at 57.01% examples, 206451 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:48,254, word2vec, INFO, EPOCH 7 - PROGRESS: at 63.05% examples, 207490 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:49,286, word2vec, INFO, EPOCH 7 - PROGRESS: at 68.78% examples, 207421 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:50,287, word2vec, INFO, EPOCH 7 - PROGRESS: at 74.61% examples, 208515 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:51,312, word2vec, INFO, EPOCH 7 - PROGRESS: at 80.24% examples, 208506 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:52,377, word2vec, INFO, EPOCH 7 - PROGRESS: at 86.26% examples, 208525 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:53,381, word2vec, INFO, EPOCH 7 - PROGRESS: at 91.81% examples, 208192 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:54,399, word2vec, INFO, EPOCH 7 - PROGRESS: at 97.63% examples, 208241 words/s, in_qsize 10, out_qsize 0 ]\n",
      "[2024-12-14 16:26:54,688, word2vec, INFO, EPOCH 7: training on 4176922 raw words (3726795 effective words) took 17.8s, 209697 effective words/s ]\n",
      "[2024-12-14 16:26:55,704, word2vec, INFO, EPOCH 8 - PROGRESS: at 4.74% examples, 176825 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:26:56,772, word2vec, INFO, EPOCH 8 - PROGRESS: at 10.78% examples, 197380 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:57,895, word2vec, INFO, EPOCH 8 - PROGRESS: at 16.65% examples, 194801 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:58,921, word2vec, INFO, EPOCH 8 - PROGRESS: at 22.34% examples, 197859 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:26:59,957, word2vec, INFO, EPOCH 8 - PROGRESS: at 28.21% examples, 201184 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:00,969, word2vec, INFO, EPOCH 8 - PROGRESS: at 34.14% examples, 204164 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:02,167, word2vec, INFO, EPOCH 8 - PROGRESS: at 40.29% examples, 202290 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:03,222, word2vec, INFO, EPOCH 8 - PROGRESS: at 46.55% examples, 204349 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:04,231, word2vec, INFO, EPOCH 8 - PROGRESS: at 52.57% examples, 205872 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:05,267, word2vec, INFO, EPOCH 8 - PROGRESS: at 58.74% examples, 207539 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:06,313, word2vec, INFO, EPOCH 8 - PROGRESS: at 64.79% examples, 207973 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:07,320, word2vec, INFO, EPOCH 8 - PROGRESS: at 70.41% examples, 208302 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:08,336, word2vec, INFO, EPOCH 8 - PROGRESS: at 76.01% examples, 208426 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:09,415, word2vec, INFO, EPOCH 8 - PROGRESS: at 81.84% examples, 208246 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:10,444, word2vec, INFO, EPOCH 8 - PROGRESS: at 87.94% examples, 208743 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:11,458, word2vec, INFO, EPOCH 8 - PROGRESS: at 94.01% examples, 209324 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:12,369, word2vec, INFO, EPOCH 8: training on 4176922 raw words (3726910 effective words) took 17.7s, 210924 effective words/s ]\n",
      "[2024-12-14 16:27:13,395, word2vec, INFO, EPOCH 9 - PROGRESS: at 4.94% examples, 183782 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:14,397, word2vec, INFO, EPOCH 9 - PROGRESS: at 10.56% examples, 198167 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:15,430, word2vec, INFO, EPOCH 9 - PROGRESS: at 16.65% examples, 203895 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:16,444, word2vec, INFO, EPOCH 9 - PROGRESS: at 22.34% examples, 205423 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:17,473, word2vec, INFO, EPOCH 9 - PROGRESS: at 28.23% examples, 207514 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:18,474, word2vec, INFO, EPOCH 9 - PROGRESS: at 33.93% examples, 208371 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:27:19,499, word2vec, INFO, EPOCH 9 - PROGRESS: at 39.63% examples, 208319 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:20,513, word2vec, INFO, EPOCH 9 - PROGRESS: at 45.27% examples, 208567 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:21,518, word2vec, INFO, EPOCH 9 - PROGRESS: at 51.13% examples, 208899 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:22,533, word2vec, INFO, EPOCH 9 - PROGRESS: at 57.01% examples, 209825 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:27:23,542, word2vec, INFO, EPOCH 9 - PROGRESS: at 63.05% examples, 210736 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:27:24,560, word2vec, INFO, EPOCH 9 - PROGRESS: at 68.53% examples, 209890 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:25,563, word2vec, INFO, EPOCH 9 - PROGRESS: at 74.17% examples, 210097 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:26,616, word2vec, INFO, EPOCH 9 - PROGRESS: at 79.79% examples, 209556 words/s, in_qsize 16, out_qsize 0 ]\n",
      "[2024-12-14 16:27:27,627, word2vec, INFO, EPOCH 9 - PROGRESS: at 85.50% examples, 209640 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:28,629, word2vec, INFO, EPOCH 9 - PROGRESS: at 91.35% examples, 209822 words/s, in_qsize 15, out_qsize 0 ]\n",
      "[2024-12-14 16:27:29,644, word2vec, INFO, EPOCH 9 - PROGRESS: at 97.11% examples, 209802 words/s, in_qsize 12, out_qsize 0 ]\n",
      "[2024-12-14 16:27:29,986, word2vec, INFO, EPOCH 9: training on 4176922 raw words (3725855 effective words) took 17.6s, 211615 effective words/s ]\n",
      "[2024-12-14 16:27:29,987, utils, INFO, FastText lifecycle event {'msg': 'training on 41769220 raw words (37265414 effective words) took 172.2s, 216430 effective words/s', 'datetime': '2024-12-14T16:27:29.987945', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]\n",
      "[2024-12-14 16:27:34,467, utils, INFO, FastText lifecycle event {'params': 'FastText<vocab=106574, vector_size=25, alpha=0.025>', 'datetime': '2024-12-14T16:27:34.467439', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]\n",
      "[2024-12-14 16:27:34,476, keyedvectors, WARNING, destructive init_sims(replace=True) deprecated & no longer required for space-efficiency ]\n",
      "[2024-12-14 16:27:34,571, keyedvectors, INFO, storing 106574x25 projection weights into ft_reviews_vectors.bin ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karthikeya\\AppData\\Local\\Temp\\ipykernel_15100\\78524876.py:17: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.wv.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Training the FastText model\n",
    "# Define parameters like vector size, window size, and minimum word count\n",
    "model = FastText(\n",
    "    sentences=df['review_full'].to_list(),    # Input data\n",
    "    vector_size=25,      # Size of word vectors\n",
    "    window=5,             # Context window size\n",
    "    min_count=1,          # Minimum word frequency to include\n",
    "    sg=1,                 # Skip-gram model (1) or CBOW (0)\n",
    "    epochs=10,             # Number of iterations (training epochs)\n",
    "    workers=8                               # Number of CPU cores to use (set to your available cores)\n",
    "\n",
    ")\n",
    "\n",
    "# Discard unnecessary data to reduce size\n",
    "model.wv.init_sims(replace=True)\n",
    "\n",
    "# Save only the word vectors in binary format\n",
    "model.wv.save_word2vec_format(\"ft_reviews_vectors.bin\", binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-14 16:27:42,577, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]\n",
      "[2024-12-14 16:27:43,820, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:27:43.820247', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"ft_reviews_vectors.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('onnaught', 0.9587024450302124),\n",
       " ('connaughtplace', 0.9568647146224976),\n",
       " ('connnaught', 0.9519844055175781),\n",
       " ('cannaught', 0.9518898129463196),\n",
       " ('cpc', 0.9517862200737),\n",
       " ('connaught', 0.9508863091468811),\n",
       " ('barconnaught', 0.9465137720108032),\n",
       " ('citycannaught', 0.9443777799606323),\n",
       " ('connaughat', 0.9442583322525024),\n",
       " ('connaght', 0.9405683875083923),\n",
       " ('connaugt', 0.9399679899215698),\n",
       " ('connaughts', 0.9386332631111145),\n",
       " ('conaught', 0.9384315013885498),\n",
       " ('circlei', 0.9336044788360596),\n",
       " ('caunnaught', 0.9283806681632996)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"cp\", topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.data_ingestion import DataIngestion\n",
    "# from src.components.data_transformation import DataTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-14 19:07:05,793, data_ingestion, INFO, Initiating data ingestion ]\n",
      "[2024-12-14 19:07:05,794, data_ingestion, INFO, Establising Connection With SQL Database ]\n",
      "[2024-12-14 19:07:05,796, data_ingestion, INFO, Successfully connected to the SQLite database. ]\n",
      "[2024-12-14 19:07:05,797, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]\n",
      "[2024-12-14 19:07:06,248, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]\n",
      "[2024-12-14 19:07:07,972, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\\raw_data.csv ]\n",
      "[2024-12-14 19:07:07,974, data_ingestion, INFO, Initiating train test split ]\n",
      "[2024-12-14 19:07:09,746, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\\train_data.csv, artifacts\\test_data.csv ]\n",
      "[2024-12-14 19:07:09,775, 409849235, INFO, Initiating the DataTransformation ]\n",
      "[2024-12-14 19:07:09,776, 409849235, INFO, Initiatig data transformation pipeline ]\n",
      "[2024-12-14 19:07:11,308, 3789801482, INFO, getting set of stopwords from NLTK from English language ]\n",
      "[2024-12-14 19:07:11,310, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]\n",
      "[2024-12-14 19:07:11,310, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]\n",
      "[2024-12-14 19:07:11,311, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]\n",
      "[2024-12-14 19:07:12,643, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T19:07:12.643063', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]\n",
      "[2024-12-14 19:07:12,644, 409849235, INFO, Numerical and text pipelines created ]\n",
      "[2024-12-14 19:07:12,647, 3789801482, INFO, getting set of stopwords from NLTK from English language ]\n",
      "[2024-12-14 19:07:12,649, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]\n",
      "[2024-12-14 19:07:12,650, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]\n",
      "[2024-12-14 19:07:12,650, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]\n",
      "[2024-12-14 19:07:13,871, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T19:07:13.871089', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]\n",
      "[2024-12-14 19:07:13,891, 3789801482, INFO, Transforming all the letters into lowercase ]\n",
      "[2024-12-14 19:07:14,005, 3789801482, INFO, Transforming all letters into lowercase is successful ]\n",
      "[2024-12-14 19:07:14,006, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]\n",
      "[2024-12-14 19:07:15,576, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]\n",
      "[2024-12-14 19:07:15,577, 3789801482, INFO, Removing punctuations from text ]\n",
      "[2024-12-14 19:07:23,274, 3789801482, INFO, Removing punctuations from text successful ]\n",
      "[2024-12-14 19:07:23,276, 3789801482, INFO, Removing the stopwords from the tokenized words ]\n",
      "[2024-12-14 19:07:25,327, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]\n",
      "[2024-12-14 19:07:25,330, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]\n",
      "[2024-12-14 19:07:49,373, 3789801482, INFO, Lemmatization successful ]\n",
      "[2024-12-14 19:07:49,376, 3789801482, INFO, Generating embeddings for the text data ]\n",
      "[2024-12-14 19:08:24,376, 3789801482, INFO, Successfully generated embeddings ]\n",
      "[2024-12-14 19:08:24,457, 3789801482, INFO, Transforming all the letters into lowercase ]\n",
      "[2024-12-14 19:08:24,504, 3789801482, INFO, Transforming all letters into lowercase is successful ]\n",
      "[2024-12-14 19:08:24,505, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]\n",
      "[2024-12-14 19:08:24,954, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]\n",
      "[2024-12-14 19:08:24,955, 3789801482, INFO, Removing punctuations from text ]\n",
      "[2024-12-14 19:08:29,090, 3789801482, INFO, Removing punctuations from text successful ]\n",
      "[2024-12-14 19:08:29,091, 3789801482, INFO, Removing the stopwords from the tokenized words ]\n",
      "[2024-12-14 19:08:29,792, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]\n",
      "[2024-12-14 19:08:29,796, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]\n",
      "[2024-12-14 19:08:40,572, 3789801482, INFO, Lemmatization successful ]\n",
      "[2024-12-14 19:08:40,680, 3789801482, INFO, Generating embeddings for the text data ]\n",
      "[2024-12-14 19:08:55,093, 3789801482, INFO, Successfully generated embeddings ]\n",
      "[2024-12-14 19:08:59,413, 409849235, INFO, Saved fitted preprocessor to artifacts\\preprocessor.joblib ]\n",
      "[2024-12-14 19:08:59,414, 409849235, INFO, Returning the input train feature as an array and test feature as array respectively ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-2.2973046713220855,\n",
       "         array([-0.09674628, -0.04126421,  0.0886041 ,  0.15847152, -0.21739738,\n",
       "                -0.01634515,  0.41059327, -0.03164278, -0.15979128,  0.09566579,\n",
       "                -0.11684272, -0.2783509 , -0.02340975, -0.23550192, -0.18175894,\n",
       "                 0.02855133, -0.05605806,  0.1585399 , -0.13517489,  0.23204158,\n",
       "                 0.0229764 , -0.03698408,  0.03425137,  0.10117792, -0.09758179],\n",
       "               dtype=float32)                                                    ],\n",
       "        [-0.2359850230424664,\n",
       "         array([-0.03870368,  0.01651378,  0.16866484,  0.13303864, -0.1488596 ,\n",
       "                -0.01973825,  0.37854007, -0.08214758, -0.17544366,  0.07709474,\n",
       "                -0.18955915, -0.27092293,  0.04179874, -0.22067113, -0.22531962,\n",
       "                -0.02487574, -0.07554977,  0.17503466, -0.11587173,  0.17046578,\n",
       "                 0.05809536, -0.06621724,  0.1078025 ,  0.08235501, -0.06380685],\n",
       "               dtype=float32)                                                    ],\n",
       "        [-0.2359850230424664,\n",
       "         array([-0.06246375,  0.03200233,  0.14863989,  0.17586866, -0.13636394,\n",
       "                -0.01669719,  0.32039505, -0.1331741 , -0.1544483 ,  0.11363465,\n",
       "                -0.13356459, -0.26122567,  0.03435605, -0.19283247, -0.29604623,\n",
       "                 0.06139058, -0.05369882,  0.18382142, -0.0927662 ,  0.19546172,\n",
       "                 0.04064698, -0.05718048,  0.09424836,  0.08502044, -0.04399038],\n",
       "               dtype=float32)                                                    ],\n",
       "        ...,\n",
       "        [0.7946748010973431,\n",
       "         array([-0.16324578, -0.00479066,  0.23901348,  0.17538567, -0.11189035,\n",
       "                 0.02786198,  0.37080613, -0.13969153, -0.17923704,  0.02181255,\n",
       "                -0.07428104, -0.25523704,  0.05886712, -0.28418162, -0.21520214,\n",
       "                 0.06737104, -0.02684761,  0.11663637, -0.14269884,  0.17477809,\n",
       "                -0.01455034, -0.05383426,  0.12774354,  0.11555752, -0.07505187],\n",
       "               dtype=float32)                                                    ],\n",
       "        [0.7946748010973431,\n",
       "         array([-0.08129704, -0.00311676,  0.09271056,  0.08882146, -0.02634652,\n",
       "                -0.00319661,  0.4497878 , -0.16293114, -0.08966578,  0.06293353,\n",
       "                -0.04609971, -0.3109048 ,  0.11804488, -0.10441398, -0.21096729,\n",
       "                -0.08674285, -0.03174475,  0.2173365 , -0.12102365,  0.23579325,\n",
       "                 0.11583143,  0.05006016,  0.05572914, -0.01654502, -0.10523017],\n",
       "               dtype=float32)                                                    ],\n",
       "        [-0.2359850230424664,\n",
       "         array([-0.10069633,  0.02794018,  0.16856548,  0.06893027, -0.05272262,\n",
       "                -0.07204564,  0.38565117, -0.05708494, -0.09721023,  0.13429879,\n",
       "                -0.1320886 , -0.26790637,  0.13590269, -0.15095876, -0.27102503,\n",
       "                 0.00377251, -0.10219441,  0.19993088, -0.14523526,  0.20659782,\n",
       "                 0.03457001, -0.06223974,  0.12812892, -0.00310027, -0.05386566],\n",
       "               dtype=float32)                                                    ]],\n",
       "       dtype=object),\n",
       " array([[0.7946748010973431,\n",
       "         array([-0.15776332,  0.02381719,  0.12668338,  0.1490619 , -0.07184418,\n",
       "                -0.09787609,  0.37665385, -0.10449484, -0.1811822 ,  0.06117807,\n",
       "                -0.1328577 , -0.24965993,  0.13894124, -0.20228715, -0.21810684,\n",
       "                 0.07481349,  0.02297874,  0.16395488, -0.11839624,  0.17017263,\n",
       "                 0.10847089, -0.07700003,  0.01410998,  0.09805856, -0.04329788],\n",
       "               dtype=float32)                                                    ],\n",
       "        [-2.2973046713220855,\n",
       "         array([-0.1468919 ,  0.00903559,  0.21889274,  0.18288985, -0.13611816,\n",
       "                -0.07222698,  0.37836075, -0.07755065, -0.13799869,  0.13396713,\n",
       "                -0.18688472, -0.23288348,  0.04638531, -0.23255256, -0.16915339,\n",
       "                 0.02876175, -0.0371875 ,  0.17901003, -0.12080466,  0.18460964,\n",
       "                 0.05227595, -0.05081483, -0.00359689,  0.13209254, -0.06783616],\n",
       "               dtype=float32)                                                    ],\n",
       "        [-0.2359850230424664,\n",
       "         array([-0.15416428,  0.05343404,  0.08172895,  0.14626889, -0.12068304,\n",
       "                -0.0749423 ,  0.3737217 , -0.11368419, -0.14338998,  0.10200364,\n",
       "                -0.13797866, -0.2425219 ,  0.09568976, -0.20800063, -0.29028928,\n",
       "                 0.03964656, -0.10719145,  0.1074417 , -0.15790561,  0.1245088 ,\n",
       "                 0.03238327, -0.05172453,  0.06366802,  0.10801749, -0.04286531],\n",
       "               dtype=float32)                                                    ],\n",
       "        ...,\n",
       "        [-3.3279644954618948,\n",
       "         array([-0.05114971,  0.06748154,  0.16633935,  0.12464069, -0.1578118 ,\n",
       "                -0.06371696,  0.39754152, -0.12016614, -0.15451013,  0.13482137,\n",
       "                -0.1494322 , -0.26454732, -0.00325173, -0.25489837, -0.26172248,\n",
       "                -0.00301098,  0.00857849,  0.16401766, -0.09637414,  0.15442388,\n",
       "                 0.08607614, -0.02654749,  0.07694269,  0.13863316, -0.07299005],\n",
       "               dtype=float32)                                                    ],\n",
       "        [0.7946748010973431,\n",
       "         array([-0.05177368, -0.04643905,  0.01182095,  0.1911841 , -0.00228362,\n",
       "                 0.00212826,  0.5178569 , -0.10403696, -0.17072232,  0.03857843,\n",
       "                -0.06686109, -0.29772034,  0.07203881, -0.10050482, -0.19201173,\n",
       "                -0.07602782,  0.00460934,  0.16394022, -0.15596874,  0.26739684,\n",
       "                 0.03040326,  0.00759481,  0.11222249,  0.01625427, -0.08914243],\n",
       "               dtype=float32)                                                    ],\n",
       "        [-1.266644847182276,\n",
       "         array([-0.10511166, -0.03160345,  0.20833774,  0.1322452 , -0.20818207,\n",
       "                -0.01422927,  0.35905948, -0.08812769, -0.15122229,  0.07312387,\n",
       "                -0.23376685, -0.29796472,  0.02353443, -0.19479477, -0.16432081,\n",
       "                 0.00114198, -0.06632829,  0.15142085, -0.10798629,  0.1921177 ,\n",
       "                 0.04071784, -0.01686362,  0.06654524,  0.0745209 , -0.07196774],\n",
       "               dtype=float32)                                                    ]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingest = DataIngestion()\n",
    "tr_D, ts_D = data_ingest.initate_data_ingestion()\n",
    "data_trans = DataTransformation()\n",
    "data_trans.initiate_data_transformation(tr_D, ts_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
