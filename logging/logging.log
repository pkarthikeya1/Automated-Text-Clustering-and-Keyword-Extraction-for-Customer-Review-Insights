[2024-12-06 13:31:43,946, 4287805165, INFO, Establising Connection With SQL Database ]
[2024-12-06 13:31:43,949, 4287805165, INFO, Successfully connected to the SQLite database. ]
[2024-12-06 13:31:43,951, 4287805165, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-06 13:31:44,339, 4287805165, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-06 17:40:36,339, 1589442515, INFO, Establising Connection With SQL Database ]
[2024-12-06 17:40:36,343, 1589442515, INFO, Successfully connected to the SQLite database. ]
[2024-12-06 17:40:36,344, 1589442515, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-06 17:40:37,059, 1589442515, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-06 19:54:45,958, 684587357, INFO, Initiating data ingestion ]
[2024-12-06 19:54:45,960, 1589442515, INFO, Establising Connection With SQL Database ]
[2024-12-06 19:54:45,961, 1589442515, INFO, Successfully connected to the SQLite database. ]
[2024-12-06 19:54:45,962, 1589442515, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-06 19:54:46,397, 1589442515, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-06 19:57:53,523, 1281613317, INFO, Initiating data ingestion ]
[2024-12-06 19:57:53,525, 1589442515, INFO, Establising Connection With SQL Database ]
[2024-12-06 19:57:53,527, 1589442515, INFO, Successfully connected to the SQLite database. ]
[2024-12-06 19:57:53,528, 1589442515, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-06 19:57:53,918, 1589442515, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-06 19:58:06,928, 3482073804, INFO, Initiating data ingestion ]
[2024-12-06 19:58:06,930, 1589442515, INFO, Establising Connection With SQL Database ]
[2024-12-06 19:58:06,931, 1589442515, INFO, Successfully connected to the SQLite database. ]
[2024-12-06 19:58:06,933, 1589442515, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-06 19:58:07,327, 1589442515, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-06 19:58:59,157, 3482073804, INFO, Initiating data ingestion ]
[2024-12-06 19:58:59,159, 1589442515, INFO, Establising Connection With SQL Database ]
[2024-12-06 19:58:59,160, 1589442515, INFO, Successfully connected to the SQLite database. ]
[2024-12-06 19:58:59,161, 1589442515, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-06 19:58:59,540, 1589442515, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-06 20:00:12,450, 1302022090, INFO, Initiating data ingestion ]
[2024-12-06 20:00:12,452, 1589442515, INFO, Establising Connection With SQL Database ]
[2024-12-06 20:00:12,454, 1589442515, INFO, Successfully connected to the SQLite database. ]
[2024-12-06 20:00:12,455, 1589442515, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-06 20:00:12,894, 1589442515, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-06 20:00:14,080, 1302022090, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-06 20:00:14,081, 1302022090, INFO, Initiating train test split ]
[2024-12-06 20:00:15,357, 1302022090, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-11 18:10:33,698, word2vec, INFO, collecting all words and their counts ]
[2024-12-11 18:10:33,700, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-11 18:10:33,820, word2vec, INFO, PROGRESS: at sentence #10000, processed 407873 words, keeping 45074 word types ]
[2024-12-11 18:10:33,932, word2vec, INFO, PROGRESS: at sentence #20000, processed 808292 words, keeping 69250 word types ]
[2024-12-11 18:10:34,062, word2vec, INFO, PROGRESS: at sentence #30000, processed 1216627 words, keeping 90083 word types ]
[2024-12-11 18:10:34,185, word2vec, INFO, PROGRESS: at sentence #40000, processed 1621499 words, keeping 108581 word types ]
[2024-12-11 18:10:34,294, word2vec, INFO, PROGRESS: at sentence #50000, processed 2025370 words, keeping 125156 word types ]
[2024-12-11 18:10:34,403, word2vec, INFO, PROGRESS: at sentence #60000, processed 2428035 words, keeping 140773 word types ]
[2024-12-11 18:10:34,504, word2vec, INFO, PROGRESS: at sentence #70000, processed 2828048 words, keeping 155388 word types ]
[2024-12-11 18:10:34,610, word2vec, INFO, PROGRESS: at sentence #80000, processed 3240046 words, keeping 170201 word types ]
[2024-12-11 18:10:34,716, word2vec, INFO, PROGRESS: at sentence #90000, processed 3644180 words, keeping 184495 word types ]
[2024-12-11 18:10:34,819, word2vec, INFO, PROGRESS: at sentence #100000, processed 4040927 words, keeping 197609 word types ]
[2024-12-11 18:10:34,855, word2vec, INFO, collected 201843 word types from a corpus of 4170053 raw words and 103304 sentences ]
[2024-12-11 18:10:34,856, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-11 18:10:35,849, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 201843 unique words (100.00% of original 201843, drops 0)', 'datetime': '2024-12-11T18:10:35.849347', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 18:10:35,850, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4170053 word corpus (100.00% of original 4170053, drops 0)', 'datetime': '2024-12-11T18:10:35.850340', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 18:10:37,476, word2vec, INFO, deleting the raw counts dictionary of 201843 items ]
[2024-12-11 18:10:37,481, word2vec, INFO, sample=0.001 downsamples 29 most-common words ]
[2024-12-11 18:10:37,496, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 3937826.8948733485 word corpus (94.4%% of prior 4170053)', 'datetime': '2024-12-11T18:10:37.496898', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 18:10:42,033, fasttext, INFO, estimated required memory for 201843 words, 2000000 buckets and 500 dimensions: 4951844772 bytes ]
[2024-12-11 18:10:42,034, word2vec, INFO, resetting layer weights ]
[2024-12-11 18:11:01,699, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-11T18:11:01.699734', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]
[2024-12-11 18:11:01,701, utils, INFO, FastText lifecycle event {'msg': 'training model with 3 workers on 201843 vocabulary and 500 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-11T18:11:01.701732', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-11 18:11:02,908, word2vec, INFO, EPOCH 0 - PROGRESS: at 0.91% examples, 31329 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:03,912, word2vec, INFO, EPOCH 0 - PROGRESS: at 2.79% examples, 51053 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:04,913, word2vec, INFO, EPOCH 0 - PROGRESS: at 4.05% examples, 49753 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:06,362, word2vec, INFO, EPOCH 0 - PROGRESS: at 5.93% examples, 50402 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:07,368, word2vec, INFO, EPOCH 0 - PROGRESS: at 7.73% examples, 54748 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:08,771, word2vec, INFO, EPOCH 0 - PROGRESS: at 9.43% examples, 53159 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:10,189, word2vec, INFO, EPOCH 0 - PROGRESS: at 11.56% examples, 54259 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:11,209, word2vec, INFO, EPOCH 0 - PROGRESS: at 13.33% examples, 55374 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:12,662, word2vec, INFO, EPOCH 0 - PROGRESS: at 15.28% examples, 54910 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:13,697, word2vec, INFO, EPOCH 0 - PROGRESS: at 16.68% examples, 54887 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:14,789, word2vec, INFO, EPOCH 0 - PROGRESS: at 18.12% examples, 54614 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:15,832, word2vec, INFO, EPOCH 0 - PROGRESS: at 19.57% examples, 54566 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:16,853, word2vec, INFO, EPOCH 0 - PROGRESS: at 21.00% examples, 54623 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:17,959, word2vec, INFO, EPOCH 0 - PROGRESS: at 22.87% examples, 55540 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:19,257, word2vec, INFO, EPOCH 0 - PROGRESS: at 24.51% examples, 55178 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:20,277, word2vec, INFO, EPOCH 0 - PROGRESS: at 25.89% examples, 55191 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:21,413, word2vec, INFO, EPOCH 0 - PROGRESS: at 27.83% examples, 55836 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:22,683, word2vec, INFO, EPOCH 0 - PROGRESS: at 29.49% examples, 55591 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:23,766, word2vec, INFO, EPOCH 0 - PROGRESS: at 31.11% examples, 55853 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:24,806, word2vec, INFO, EPOCH 0 - PROGRESS: at 32.81% examples, 56182 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:26,082, word2vec, INFO, EPOCH 0 - PROGRESS: at 34.44% examples, 55932 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:27,083, word2vec, INFO, EPOCH 0 - PROGRESS: at 35.87% examples, 55939 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:28,118, word2vec, INFO, EPOCH 0 - PROGRESS: at 37.74% examples, 56603 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:29,419, word2vec, INFO, EPOCH 0 - PROGRESS: at 39.48% examples, 56317 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:30,464, word2vec, INFO, EPOCH 0 - PROGRESS: at 41.03% examples, 56567 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:31,789, word2vec, INFO, EPOCH 0 - PROGRESS: at 42.91% examples, 56577 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:32,810, word2vec, INFO, EPOCH 0 - PROGRESS: at 44.64% examples, 56819 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:34,084, word2vec, INFO, EPOCH 0 - PROGRESS: at 46.64% examples, 56911 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:35,173, word2vec, INFO, EPOCH 0 - PROGRESS: at 48.31% examples, 57028 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:36,448, word2vec, INFO, EPOCH 0 - PROGRESS: at 50.29% examples, 57099 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:37,565, word2vec, INFO, EPOCH 0 - PROGRESS: at 51.95% examples, 57152 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:38,862, word2vec, INFO, EPOCH 0 - PROGRESS: at 53.83% examples, 57189 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:39,943, word2vec, INFO, EPOCH 0 - PROGRESS: at 55.50% examples, 57299 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:41,219, word2vec, INFO, EPOCH 0 - PROGRESS: at 57.36% examples, 57352 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:42,301, word2vec, INFO, EPOCH 0 - PROGRESS: at 59.11% examples, 57446 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:43,628, word2vec, INFO, EPOCH 0 - PROGRESS: at 60.99% examples, 57409 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:45,034, word2vec, INFO, EPOCH 0 - PROGRESS: at 63.20% examples, 57499 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:46,487, word2vec, INFO, EPOCH 0 - PROGRESS: at 65.38% examples, 57514 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:47,896, word2vec, INFO, EPOCH 0 - PROGRESS: at 67.48% examples, 57589 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:48,900, word2vec, INFO, EPOCH 0 - PROGRESS: at 69.36% examples, 57963 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:50,201, word2vec, INFO, EPOCH 0 - PROGRESS: at 70.99% examples, 57768 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:51,253, word2vec, INFO, EPOCH 0 - PROGRESS: at 72.92% examples, 58057 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:52,618, word2vec, INFO, EPOCH 0 - PROGRESS: at 74.57% examples, 57791 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:53,648, word2vec, INFO, EPOCH 0 - PROGRESS: at 76.39% examples, 58097 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:54,950, word2vec, INFO, EPOCH 0 - PROGRESS: at 78.06% examples, 57912 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:56,092, word2vec, INFO, EPOCH 0 - PROGRESS: at 79.94% examples, 58087 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:57,339, word2vec, INFO, EPOCH 0 - PROGRESS: at 81.54% examples, 57959 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:11:58,534, word2vec, INFO, EPOCH 0 - PROGRESS: at 83.44% examples, 58062 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:11:59,649, word2vec, INFO, EPOCH 0 - PROGRESS: at 85.16% examples, 58079 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:00,716, word2vec, INFO, EPOCH 0 - PROGRESS: at 86.87% examples, 58144 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:01,721, word2vec, INFO, EPOCH 0 - PROGRESS: at 88.37% examples, 58110 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:02,853, word2vec, INFO, EPOCH 0 - PROGRESS: at 90.12% examples, 58110 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:03,946, word2vec, INFO, EPOCH 0 - PROGRESS: at 91.75% examples, 58146 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:05,075, word2vec, INFO, EPOCH 0 - PROGRESS: at 93.47% examples, 58149 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:06,223, word2vec, INFO, EPOCH 0 - PROGRESS: at 95.10% examples, 58137 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:07,299, word2vec, INFO, EPOCH 0 - PROGRESS: at 96.82% examples, 58184 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:08,407, word2vec, INFO, EPOCH 0 - PROGRESS: at 98.56% examples, 58205 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:09,278, word2vec, INFO, EPOCH 0: training on 4170053 raw words (3937516 effective words) took 67.6s, 58273 effective words/s ]
[2024-12-11 18:12:10,289, word2vec, INFO, EPOCH 1 - PROGRESS: at 1.38% examples, 56142 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:11,662, word2vec, INFO, EPOCH 1 - PROGRESS: at 3.04% examples, 51273 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:12,666, word2vec, INFO, EPOCH 1 - PROGRESS: at 4.95% examples, 58339 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:13,969, word2vec, INFO, EPOCH 1 - PROGRESS: at 6.59% examples, 56097 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:15,007, word2vec, INFO, EPOCH 1 - PROGRESS: at 8.45% examples, 59087 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:16,340, word2vec, INFO, EPOCH 1 - PROGRESS: at 10.10% examples, 57248 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:17,347, word2vec, INFO, EPOCH 1 - PROGRESS: at 11.82% examples, 58261 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:18,364, word2vec, INFO, EPOCH 1 - PROGRESS: at 13.56% examples, 58999 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:19,668, word2vec, INFO, EPOCH 1 - PROGRESS: at 15.28% examples, 57928 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:20,698, word2vec, INFO, EPOCH 1 - PROGRESS: at 16.92% examples, 58473 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:21,770, word2vec, INFO, EPOCH 1 - PROGRESS: at 18.60% examples, 58734 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:23,057, word2vec, INFO, EPOCH 1 - PROGRESS: at 20.30% examples, 58019 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:24,092, word2vec, INFO, EPOCH 1 - PROGRESS: at 21.91% examples, 58430 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:25,200, word2vec, INFO, EPOCH 1 - PROGRESS: at 23.58% examples, 58493 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:26,539, word2vec, INFO, EPOCH 1 - PROGRESS: at 25.25% examples, 57751 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:27,560, word2vec, INFO, EPOCH 1 - PROGRESS: at 26.88% examples, 58142 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:28,616, word2vec, INFO, EPOCH 1 - PROGRESS: at 28.30% examples, 57890 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:29,722, word2vec, INFO, EPOCH 1 - PROGRESS: at 29.97% examples, 57981 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:30,901, word2vec, INFO, EPOCH 1 - PROGRESS: at 31.63% examples, 57872 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:32,046, word2vec, INFO, EPOCH 1 - PROGRESS: at 33.30% examples, 57841 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:33,105, word2vec, INFO, EPOCH 1 - PROGRESS: at 34.92% examples, 58026 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:34,274, word2vec, INFO, EPOCH 1 - PROGRESS: at 36.58% examples, 57937 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:35,383, word2vec, INFO, EPOCH 1 - PROGRESS: at 38.27% examples, 57998 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:36,559, word2vec, INFO, EPOCH 1 - PROGRESS: at 39.91% examples, 57920 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:37,560, word2vec, INFO, EPOCH 1 - PROGRESS: at 41.26% examples, 57871 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:38,773, word2vec, INFO, EPOCH 1 - PROGRESS: at 42.91% examples, 57721 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:39,801, word2vec, INFO, EPOCH 1 - PROGRESS: at 44.64% examples, 57920 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:40,953, word2vec, INFO, EPOCH 1 - PROGRESS: at 46.39% examples, 57890 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:42,092, word2vec, INFO, EPOCH 1 - PROGRESS: at 48.09% examples, 57889 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:43,178, word2vec, INFO, EPOCH 1 - PROGRESS: at 49.77% examples, 57977 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:44,297, word2vec, INFO, EPOCH 1 - PROGRESS: at 51.45% examples, 58003 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:45,460, word2vec, INFO, EPOCH 1 - PROGRESS: at 53.14% examples, 57957 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:46,487, word2vec, INFO, EPOCH 1 - PROGRESS: at 54.10% examples, 57370 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:47,606, word2vec, INFO, EPOCH 1 - PROGRESS: at 54.57% examples, 56187 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:48,736, word2vec, INFO, EPOCH 1 - PROGRESS: at 55.73% examples, 55774 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:49,806, word2vec, INFO, EPOCH 1 - PROGRESS: at 57.36% examples, 55923 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:50,964, word2vec, INFO, EPOCH 1 - PROGRESS: at 59.11% examples, 55950 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:52,092, word2vec, INFO, EPOCH 1 - PROGRESS: at 60.76% examples, 56002 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:53,154, word2vec, INFO, EPOCH 1 - PROGRESS: at 62.44% examples, 56149 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:54,262, word2vec, INFO, EPOCH 1 - PROGRESS: at 64.20% examples, 56225 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:55,403, word2vec, INFO, EPOCH 1 - PROGRESS: at 65.85% examples, 56255 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:12:56,443, word2vec, INFO, EPOCH 1 - PROGRESS: at 67.48% examples, 56408 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:57,588, word2vec, INFO, EPOCH 1 - PROGRESS: at 69.11% examples, 56438 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:58,626, word2vec, INFO, EPOCH 1 - PROGRESS: at 70.53% examples, 56396 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:12:59,758, word2vec, INFO, EPOCH 1 - PROGRESS: at 71.94% examples, 56249 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:00,810, word2vec, INFO, EPOCH 1 - PROGRESS: at 73.38% examples, 56191 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:02,144, word2vec, INFO, EPOCH 1 - PROGRESS: at 74.08% examples, 55308 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:03,225, word2vec, INFO, EPOCH 1 - PROGRESS: at 74.57% examples, 54549 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:04,267, word2vec, INFO, EPOCH 1 - PROGRESS: at 75.93% examples, 54543 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:05,508, word2vec, INFO, EPOCH 1 - PROGRESS: at 77.59% examples, 54509 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:06,626, word2vec, INFO, EPOCH 1 - PROGRESS: at 79.22% examples, 54599 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:07,649, word2vec, INFO, EPOCH 1 - PROGRESS: at 80.86% examples, 54764 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:08,691, word2vec, INFO, EPOCH 1 - PROGRESS: at 82.25% examples, 54753 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:09,770, word2vec, INFO, EPOCH 1 - PROGRESS: at 83.69% examples, 54706 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:10,826, word2vec, INFO, EPOCH 1 - PROGRESS: at 85.16% examples, 54683 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:11,918, word2vec, INFO, EPOCH 1 - PROGRESS: at 86.65% examples, 54628 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:12,941, word2vec, INFO, EPOCH 1 - PROGRESS: at 87.84% examples, 54492 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:14,006, word2vec, INFO, EPOCH 1 - PROGRESS: at 89.60% examples, 54613 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:15,196, word2vec, INFO, EPOCH 1 - PROGRESS: at 91.30% examples, 54624 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:16,294, word2vec, INFO, EPOCH 1 - PROGRESS: at 92.95% examples, 54711 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:17,492, word2vec, INFO, EPOCH 1 - PROGRESS: at 94.63% examples, 54718 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:18,701, word2vec, INFO, EPOCH 1 - PROGRESS: at 96.36% examples, 54710 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:19,914, word2vec, INFO, EPOCH 1 - PROGRESS: at 98.30% examples, 54836 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:20,974, word2vec, INFO, EPOCH 1 - PROGRESS: at 100.00% examples, 54927 words/s, in_qsize 0, out_qsize 1 ]
[2024-12-11 18:13:20,975, word2vec, INFO, EPOCH 1: training on 4170053 raw words (3937749 effective words) took 71.7s, 54927 effective words/s ]
[2024-12-11 18:13:22,014, word2vec, INFO, EPOCH 2 - PROGRESS: at 0.91% examples, 36352 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:23,020, word2vec, INFO, EPOCH 2 - PROGRESS: at 2.57% examples, 50566 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:24,352, word2vec, INFO, EPOCH 2 - PROGRESS: at 4.49% examples, 52915 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:25,500, word2vec, INFO, EPOCH 2 - PROGRESS: at 5.93% examples, 51913 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:26,723, word2vec, INFO, EPOCH 2 - PROGRESS: at 6.59% examples, 45764 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:27,880, word2vec, INFO, EPOCH 2 - PROGRESS: at 7.97% examples, 46287 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:29,032, word2vec, INFO, EPOCH 2 - PROGRESS: at 9.87% examples, 49011 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:30,314, word2vec, INFO, EPOCH 2 - PROGRESS: at 11.56% examples, 49338 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:31,505, word2vec, INFO, EPOCH 2 - PROGRESS: at 13.56% examples, 50907 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:32,680, word2vec, INFO, EPOCH 2 - PROGRESS: at 15.28% examples, 51433 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:34,070, word2vec, INFO, EPOCH 2 - PROGRESS: at 16.02% examples, 48129 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:35,379, word2vec, INFO, EPOCH 2 - PROGRESS: at 16.68% examples, 45718 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:36,561, word2vec, INFO, EPOCH 2 - PROGRESS: at 18.60% examples, 47074 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:37,768, word2vec, INFO, EPOCH 2 - PROGRESS: at 20.30% examples, 47594 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:38,783, word2vec, INFO, EPOCH 2 - PROGRESS: at 21.91% examples, 48601 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:39,945, word2vec, INFO, EPOCH 2 - PROGRESS: at 23.58% examples, 49085 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:41,174, word2vec, INFO, EPOCH 2 - PROGRESS: at 25.25% examples, 49345 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:42,365, word2vec, INFO, EPOCH 2 - PROGRESS: at 27.09% examples, 50131 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:43,545, word2vec, INFO, EPOCH 2 - PROGRESS: at 28.76% examples, 50429 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:44,602, word2vec, INFO, EPOCH 2 - PROGRESS: at 30.41% examples, 50960 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:45,830, word2vec, INFO, EPOCH 2 - PROGRESS: at 32.10% examples, 51089 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:46,861, word2vec, INFO, EPOCH 2 - PROGRESS: at 33.75% examples, 51596 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:47,862, word2vec, INFO, EPOCH 2 - PROGRESS: at 35.41% examples, 52115 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:49,011, word2vec, INFO, EPOCH 2 - PROGRESS: at 37.05% examples, 52321 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:50,123, word2vec, INFO, EPOCH 2 - PROGRESS: at 38.74% examples, 52583 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:51,168, word2vec, INFO, EPOCH 2 - PROGRESS: at 40.34% examples, 52949 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:52,277, word2vec, INFO, EPOCH 2 - PROGRESS: at 41.95% examples, 53180 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:53,403, word2vec, INFO, EPOCH 2 - PROGRESS: at 43.64% examples, 53361 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:54,611, word2vec, INFO, EPOCH 2 - PROGRESS: at 45.35% examples, 53390 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:55,653, word2vec, INFO, EPOCH 2 - PROGRESS: at 47.14% examples, 53682 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:13:56,770, word2vec, INFO, EPOCH 2 - PROGRESS: at 48.82% examples, 53846 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:57,934, word2vec, INFO, EPOCH 2 - PROGRESS: at 50.53% examples, 53933 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:13:58,953, word2vec, INFO, EPOCH 2 - PROGRESS: at 52.19% examples, 54219 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:00,057, word2vec, INFO, EPOCH 2 - PROGRESS: at 53.83% examples, 54373 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:01,213, word2vec, INFO, EPOCH 2 - PROGRESS: at 55.50% examples, 54450 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:02,275, word2vec, INFO, EPOCH 2 - PROGRESS: at 57.13% examples, 54642 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:03,373, word2vec, INFO, EPOCH 2 - PROGRESS: at 58.86% examples, 54783 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:04,373, word2vec, INFO, EPOCH 2 - PROGRESS: at 60.30% examples, 54819 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:05,616, word2vec, INFO, EPOCH 2 - PROGRESS: at 61.97% examples, 54757 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:06,831, word2vec, INFO, EPOCH 2 - PROGRESS: at 63.94% examples, 54944 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:08,002, word2vec, INFO, EPOCH 2 - PROGRESS: at 65.60% examples, 54966 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:09,253, word2vec, INFO, EPOCH 2 - PROGRESS: at 67.48% examples, 55101 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:10,284, word2vec, INFO, EPOCH 2 - PROGRESS: at 68.89% examples, 55098 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:11,418, word2vec, INFO, EPOCH 2 - PROGRESS: at 70.53% examples, 55167 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:12,441, word2vec, INFO, EPOCH 2 - PROGRESS: at 72.20% examples, 55353 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:13,606, word2vec, INFO, EPOCH 2 - PROGRESS: at 73.84% examples, 55374 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:14,774, word2vec, INFO, EPOCH 2 - PROGRESS: at 75.44% examples, 55400 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:16,047, word2vec, INFO, EPOCH 2 - PROGRESS: at 76.13% examples, 54634 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:17,811, word2vec, INFO, EPOCH 2 - PROGRESS: at 76.87% examples, 53432 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:18,933, word2vec, INFO, EPOCH 2 - PROGRESS: at 78.77% examples, 53699 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:20,228, word2vec, INFO, EPOCH 2 - PROGRESS: at 80.39% examples, 53640 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:21,409, word2vec, INFO, EPOCH 2 - PROGRESS: at 82.25% examples, 53826 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:22,582, word2vec, INFO, EPOCH 2 - PROGRESS: at 83.95% examples, 53869 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:23,764, word2vec, INFO, EPOCH 2 - PROGRESS: at 85.90% examples, 54052 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:24,982, word2vec, INFO, EPOCH 2 - PROGRESS: at 87.61% examples, 54052 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:26,196, word2vec, INFO, EPOCH 2 - PROGRESS: at 89.60% examples, 54201 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:27,327, word2vec, INFO, EPOCH 2 - PROGRESS: at 91.30% examples, 54268 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:28,517, word2vec, INFO, EPOCH 2 - PROGRESS: at 93.21% examples, 54425 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:29,692, word2vec, INFO, EPOCH 2 - PROGRESS: at 94.85% examples, 54454 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:30,693, word2vec, INFO, EPOCH 2 - PROGRESS: at 96.57% examples, 54614 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:31,816, word2vec, INFO, EPOCH 2 - PROGRESS: at 98.30% examples, 54676 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:32,810, word2vec, INFO, EPOCH 2: training on 4170053 raw words (3937789 effective words) took 71.8s, 54821 effective words/s ]
[2024-12-11 18:14:34,246, word2vec, INFO, EPOCH 3 - PROGRESS: at 1.64% examples, 46032 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:35,641, word2vec, INFO, EPOCH 3 - PROGRESS: at 3.74% examples, 53228 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:36,998, word2vec, INFO, EPOCH 3 - PROGRESS: at 5.93% examples, 56144 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:38,005, word2vec, INFO, EPOCH 3 - PROGRESS: at 7.73% examples, 59759 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:39,298, word2vec, INFO, EPOCH 3 - PROGRESS: at 9.43% examples, 57974 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:40,349, word2vec, INFO, EPOCH 3 - PROGRESS: at 10.80% examples, 57396 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:41,368, word2vec, INFO, EPOCH 3 - PROGRESS: at 12.53% examples, 58249 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:42,732, word2vec, INFO, EPOCH 3 - PROGRESS: at 14.57% examples, 57837 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:43,775, word2vec, INFO, EPOCH 3 - PROGRESS: at 16.46% examples, 59206 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:45,068, word2vec, INFO, EPOCH 3 - PROGRESS: at 18.12% examples, 58334 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:46,277, word2vec, INFO, EPOCH 3 - PROGRESS: at 18.85% examples, 55189 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:47,943, word2vec, INFO, EPOCH 3 - PROGRESS: at 19.58% examples, 50970 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:48,997, word2vec, INFO, EPOCH 3 - PROGRESS: at 21.00% examples, 51141 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:50,038, word2vec, INFO, EPOCH 3 - PROGRESS: at 22.64% examples, 51879 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:51,082, word2vec, INFO, EPOCH 3 - PROGRESS: at 24.25% examples, 52507 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:52,342, word2vec, INFO, EPOCH 3 - PROGRESS: at 25.89% examples, 52500 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:53,447, word2vec, INFO, EPOCH 3 - PROGRESS: at 27.83% examples, 53342 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:54,677, word2vec, INFO, EPOCH 3 - PROGRESS: at 29.49% examples, 53348 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:55,690, word2vec, INFO, EPOCH 3 - PROGRESS: at 31.11% examples, 53874 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:56,768, word2vec, INFO, EPOCH 3 - PROGRESS: at 32.81% examples, 54196 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:14:58,059, word2vec, INFO, EPOCH 3 - PROGRESS: at 34.44% examples, 54019 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:14:59,084, word2vec, INFO, EPOCH 3 - PROGRESS: at 36.35% examples, 54769 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:00,377, word2vec, INFO, EPOCH 3 - PROGRESS: at 37.99% examples, 54593 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:01,606, word2vec, INFO, EPOCH 3 - PROGRESS: at 39.91% examples, 54878 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:02,760, word2vec, INFO, EPOCH 3 - PROGRESS: at 41.50% examples, 54967 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:03,939, word2vec, INFO, EPOCH 3 - PROGRESS: at 43.39% examples, 55301 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:04,978, word2vec, INFO, EPOCH 3 - PROGRESS: at 44.87% examples, 55255 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:06,050, word2vec, INFO, EPOCH 3 - PROGRESS: at 45.61% examples, 54323 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:07,060, word2vec, INFO, EPOCH 3 - PROGRESS: at 45.88% examples, 52991 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:08,254, word2vec, INFO, EPOCH 3 - PROGRESS: at 46.64% examples, 52002 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:09,515, word2vec, INFO, EPOCH 3 - PROGRESS: at 48.56% examples, 52268 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:10,718, word2vec, INFO, EPOCH 3 - PROGRESS: at 50.29% examples, 52343 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:11,727, word2vec, INFO, EPOCH 3 - PROGRESS: at 51.95% examples, 52676 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:12,761, word2vec, INFO, EPOCH 3 - PROGRESS: at 53.40% examples, 52728 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:13,876, word2vec, INFO, EPOCH 3 - PROGRESS: at 54.57% examples, 52447 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:15,028, word2vec, INFO, EPOCH 3 - PROGRESS: at 55.73% examples, 52133 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:16,264, word2vec, INFO, EPOCH 3 - PROGRESS: at 57.13% examples, 51946 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:17,313, word2vec, INFO, EPOCH 3 - PROGRESS: at 58.61% examples, 51989 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:18,359, word2vec, INFO, EPOCH 3 - PROGRESS: at 60.08% examples, 52034 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:19,530, word2vec, INFO, EPOCH 3 - PROGRESS: at 60.99% examples, 51525 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:20,655, word2vec, INFO, EPOCH 3 - PROGRESS: at 61.50% examples, 50706 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:21,813, word2vec, INFO, EPOCH 3 - PROGRESS: at 62.22% examples, 50087 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:22,981, word2vec, INFO, EPOCH 3 - PROGRESS: at 63.94% examples, 50229 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:23,982, word2vec, INFO, EPOCH 3 - PROGRESS: at 65.63% examples, 50529 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:25,180, word2vec, INFO, EPOCH 3 - PROGRESS: at 67.28% examples, 50627 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:26,330, word2vec, INFO, EPOCH 3 - PROGRESS: at 68.89% examples, 50775 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:27,444, word2vec, INFO, EPOCH 3 - PROGRESS: at 70.29% examples, 50774 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:28,447, word2vec, INFO, EPOCH 3 - PROGRESS: at 71.70% examples, 50873 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:29,582, word2vec, INFO, EPOCH 3 - PROGRESS: at 72.92% examples, 50681 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:30,624, word2vec, INFO, EPOCH 3 - PROGRESS: at 74.32% examples, 50745 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:31,649, word2vec, INFO, EPOCH 3 - PROGRESS: at 75.67% examples, 50820 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:32,687, word2vec, INFO, EPOCH 3 - PROGRESS: at 77.32% examples, 51039 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:33,707, word2vec, INFO, EPOCH 3 - PROGRESS: at 79.01% examples, 51268 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:34,964, word2vec, INFO, EPOCH 3 - PROGRESS: at 80.62% examples, 51291 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:35,972, word2vec, INFO, EPOCH 3 - PROGRESS: at 82.25% examples, 51510 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:37,265, word2vec, INFO, EPOCH 3 - PROGRESS: at 84.20% examples, 51642 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:38,293, word2vec, INFO, EPOCH 3 - PROGRESS: at 85.91% examples, 51833 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:39,646, word2vec, INFO, EPOCH 3 - PROGRESS: at 87.84% examples, 51911 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:40,650, word2vec, INFO, EPOCH 3 - PROGRESS: at 89.84% examples, 52251 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:41,978, word2vec, INFO, EPOCH 3 - PROGRESS: at 91.51% examples, 52199 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:43,029, word2vec, INFO, EPOCH 3 - PROGRESS: at 93.47% examples, 52487 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:44,035, word2vec, INFO, EPOCH 3 - PROGRESS: at 94.85% examples, 52540 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:45,206, word2vec, INFO, EPOCH 3 - PROGRESS: at 96.57% examples, 52598 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:46,441, word2vec, INFO, EPOCH 3 - PROGRESS: at 98.56% examples, 52736 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:47,302, word2vec, INFO, EPOCH 3: training on 4170053 raw words (3938043 effective words) took 74.5s, 52869 effective words/s ]
[2024-12-11 18:15:48,344, word2vec, INFO, EPOCH 4 - PROGRESS: at 1.38% examples, 54424 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:49,365, word2vec, INFO, EPOCH 4 - PROGRESS: at 2.79% examples, 54676 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:50,863, word2vec, INFO, EPOCH 4 - PROGRESS: at 4.51% examples, 50187 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:52,301, word2vec, INFO, EPOCH 4 - PROGRESS: at 6.59% examples, 52633 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:53,709, word2vec, INFO, EPOCH 4 - PROGRESS: at 8.70% examples, 54280 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:54,737, word2vec, INFO, EPOCH 4 - PROGRESS: at 10.57% examples, 56895 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:56,176, word2vec, INFO, EPOCH 4 - PROGRESS: at 12.31% examples, 55100 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:15:57,655, word2vec, INFO, EPOCH 4 - PROGRESS: at 14.57% examples, 55422 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:15:59,078, word2vec, INFO, EPOCH 4 - PROGRESS: at 16.68% examples, 55915 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:00,116, word2vec, INFO, EPOCH 4 - PROGRESS: at 18.60% examples, 57263 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:01,215, word2vec, INFO, EPOCH 4 - PROGRESS: at 20.06% examples, 56781 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:02,231, word2vec, INFO, EPOCH 4 - PROGRESS: at 21.00% examples, 55442 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:03,259, word2vec, INFO, EPOCH 4 - PROGRESS: at 22.39% examples, 55418 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:04,378, word2vec, INFO, EPOCH 4 - PROGRESS: at 23.81% examples, 55100 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:05,435, word2vec, INFO, EPOCH 4 - PROGRESS: at 25.25% examples, 54981 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:06,505, word2vec, INFO, EPOCH 4 - PROGRESS: at 26.88% examples, 55357 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:08,474, word2vec, INFO, EPOCH 4 - PROGRESS: at 28.76% examples, 53769 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:10,001, word2vec, INFO, EPOCH 4 - PROGRESS: at 29.49% examples, 51389 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:11,215, word2vec, INFO, EPOCH 4 - PROGRESS: at 30.87% examples, 51142 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:12,279, word2vec, INFO, EPOCH 4 - PROGRESS: at 32.60% examples, 51593 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:13,287, word2vec, INFO, EPOCH 4 - PROGRESS: at 34.23% examples, 52111 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:14,546, word2vec, INFO, EPOCH 4 - PROGRESS: at 35.87% examples, 52112 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:15,638, word2vec, INFO, EPOCH 4 - PROGRESS: at 37.50% examples, 52431 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:16,654, word2vec, INFO, EPOCH 4 - PROGRESS: at 39.25% examples, 52859 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:17,846, word2vec, INFO, EPOCH 4 - PROGRESS: at 40.80% examples, 52960 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:18,969, word2vec, INFO, EPOCH 4 - PROGRESS: at 42.45% examples, 53162 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:20,224, word2vec, INFO, EPOCH 4 - PROGRESS: at 44.37% examples, 53406 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:21,263, word2vec, INFO, EPOCH 4 - PROGRESS: at 46.14% examples, 53706 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:22,267, word2vec, INFO, EPOCH 4 - PROGRESS: at 47.61% examples, 53780 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:23,272, word2vec, INFO, EPOCH 4 - PROGRESS: at 49.06% examples, 53849 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:24,475, word2vec, INFO, EPOCH 4 - PROGRESS: at 50.74% examples, 53879 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:25,497, word2vec, INFO, EPOCH 4 - PROGRESS: at 52.19% examples, 53908 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:26,549, word2vec, INFO, EPOCH 4 - PROGRESS: at 53.64% examples, 53901 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:27,723, word2vec, INFO, EPOCH 4 - PROGRESS: at 55.26% examples, 53971 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:28,867, word2vec, INFO, EPOCH 4 - PROGRESS: at 56.90% examples, 54068 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:30,097, word2vec, INFO, EPOCH 4 - PROGRESS: at 58.12% examples, 53614 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:31,697, word2vec, INFO, EPOCH 4 - PROGRESS: at 58.86% examples, 52319 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:32,711, word2vec, INFO, EPOCH 4 - PROGRESS: at 60.54% examples, 52600 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:34,010, word2vec, INFO, EPOCH 4 - PROGRESS: at 62.44% examples, 52736 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:35,086, word2vec, INFO, EPOCH 4 - PROGRESS: at 64.20% examples, 52922 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:36,433, word2vec, INFO, EPOCH 4 - PROGRESS: at 66.06% examples, 52999 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:37,449, word2vec, INFO, EPOCH 4 - PROGRESS: at 67.73% examples, 53235 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:38,778, word2vec, INFO, EPOCH 4 - PROGRESS: at 69.58% examples, 53328 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:39,819, word2vec, INFO, EPOCH 4 - PROGRESS: at 71.46% examples, 53704 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:41,086, word2vec, INFO, EPOCH 4 - PROGRESS: at 73.16% examples, 53661 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:42,176, word2vec, INFO, EPOCH 4 - PROGRESS: at 74.76% examples, 53799 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:43,527, word2vec, INFO, EPOCH 4 - PROGRESS: at 76.64% examples, 53845 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:44,537, word2vec, INFO, EPOCH 4 - PROGRESS: at 78.53% examples, 54210 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:45,809, word2vec, INFO, EPOCH 4 - PROGRESS: at 80.16% examples, 54161 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:46,900, word2vec, INFO, EPOCH 4 - PROGRESS: at 81.77% examples, 54265 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:48,163, word2vec, INFO, EPOCH 4 - PROGRESS: at 83.69% examples, 54374 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:49,216, word2vec, INFO, EPOCH 4 - PROGRESS: at 85.42% examples, 54511 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:50,555, word2vec, INFO, EPOCH 4 - PROGRESS: at 87.36% examples, 54546 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:51,571, word2vec, INFO, EPOCH 4 - PROGRESS: at 89.12% examples, 54708 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:52,831, word2vec, INFO, EPOCH 4 - PROGRESS: at 91.07% examples, 54805 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:53,919, word2vec, INFO, EPOCH 4 - PROGRESS: at 92.70% examples, 54897 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:55,290, word2vec, INFO, EPOCH 4 - PROGRESS: at 94.15% examples, 54620 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:56,482, word2vec, INFO, EPOCH 4 - PROGRESS: at 94.63% examples, 53952 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:57,636, word2vec, INFO, EPOCH 4 - PROGRESS: at 95.38% examples, 53466 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:16:58,850, word2vec, INFO, EPOCH 4 - PROGRESS: at 97.31% examples, 53608 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:16:59,993, word2vec, INFO, EPOCH 4 - PROGRESS: at 99.02% examples, 53672 words/s, in_qsize 4, out_qsize 0 ]
[2024-12-11 18:17:00,522, word2vec, INFO, EPOCH 4: training on 4170053 raw words (3937731 effective words) took 73.2s, 53783 effective words/s ]
[2024-12-11 18:17:01,949, word2vec, INFO, EPOCH 5 - PROGRESS: at 1.64% examples, 46314 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:03,308, word2vec, INFO, EPOCH 5 - PROGRESS: at 3.74% examples, 53991 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:04,732, word2vec, INFO, EPOCH 5 - PROGRESS: at 5.93% examples, 55787 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:06,329, word2vec, INFO, EPOCH 5 - PROGRESS: at 6.59% examples, 45292 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:07,486, word2vec, INFO, EPOCH 5 - PROGRESS: at 7.97% examples, 45872 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:08,502, word2vec, INFO, EPOCH 5 - PROGRESS: at 9.68% examples, 48285 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:09,962, word2vec, INFO, EPOCH 5 - PROGRESS: at 11.56% examples, 48788 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:11,447, word2vec, INFO, EPOCH 5 - PROGRESS: at 13.81% examples, 49910 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:13,125, word2vec, INFO, EPOCH 5 - PROGRESS: at 14.54% examples, 45499 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:14,317, word2vec, INFO, EPOCH 5 - PROGRESS: at 15.28% examples, 43616 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:15,769, word2vec, INFO, EPOCH 5 - PROGRESS: at 17.41% examples, 45024 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:17,175, word2vec, INFO, EPOCH 5 - PROGRESS: at 19.58% examples, 46300 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:18,181, word2vec, INFO, EPOCH 5 - PROGRESS: at 21.44% examples, 47939 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:19,310, word2vec, INFO, EPOCH 5 - PROGRESS: at 21.68% examples, 45562 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:20,482, word2vec, INFO, EPOCH 5 - PROGRESS: at 22.39% examples, 44295 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:21,499, word2vec, INFO, EPOCH 5 - PROGRESS: at 24.25% examples, 45726 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:22,858, word2vec, INFO, EPOCH 5 - PROGRESS: at 25.89% examples, 45893 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:23,863, word2vec, INFO, EPOCH 5 - PROGRESS: at 27.58% examples, 46745 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:24,873, word2vec, INFO, EPOCH 5 - PROGRESS: at 29.22% examples, 47511 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:26,144, word2vec, INFO, EPOCH 5 - PROGRESS: at 30.87% examples, 47727 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:27,188, word2vec, INFO, EPOCH 5 - PROGRESS: at 32.81% examples, 48675 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:28,555, word2vec, INFO, EPOCH 5 - PROGRESS: at 34.44% examples, 48638 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:29,567, word2vec, INFO, EPOCH 5 - PROGRESS: at 36.35% examples, 49523 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:30,964, word2vec, INFO, EPOCH 5 - PROGRESS: at 37.99% examples, 49415 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:32,352, word2vec, INFO, EPOCH 5 - PROGRESS: at 40.14% examples, 49922 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:33,392, word2vec, INFO, EPOCH 5 - PROGRESS: at 41.70% examples, 50350 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:34,408, word2vec, INFO, EPOCH 5 - PROGRESS: at 43.39% examples, 50780 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:35,629, word2vec, INFO, EPOCH 5 - PROGRESS: at 45.09% examples, 50874 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:36,715, word2vec, INFO, EPOCH 5 - PROGRESS: at 46.88% examples, 51166 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:37,946, word2vec, INFO, EPOCH 5 - PROGRESS: at 48.82% examples, 51496 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:39,098, word2vec, INFO, EPOCH 5 - PROGRESS: at 50.53% examples, 51663 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:40,360, word2vec, INFO, EPOCH 5 - PROGRESS: at 52.42% examples, 51913 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:41,469, word2vec, INFO, EPOCH 5 - PROGRESS: at 54.10% examples, 52118 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:42,547, word2vec, INFO, EPOCH 5 - PROGRESS: at 55.73% examples, 52352 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:44,009, word2vec, INFO, EPOCH 5 - PROGRESS: at 57.36% examples, 52105 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:45,021, word2vec, INFO, EPOCH 5 - PROGRESS: at 57.87% examples, 51343 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:46,188, word2vec, INFO, EPOCH 5 - PROGRESS: at 58.36% examples, 50442 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:47,209, word2vec, INFO, EPOCH 5 - PROGRESS: at 60.08% examples, 50749 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:48,374, word2vec, INFO, EPOCH 5 - PROGRESS: at 61.73% examples, 50880 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:49,433, word2vec, INFO, EPOCH 5 - PROGRESS: at 63.48% examples, 51123 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:50,633, word2vec, INFO, EPOCH 5 - PROGRESS: at 65.16% examples, 51209 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:51,752, word2vec, INFO, EPOCH 5 - PROGRESS: at 66.80% examples, 51371 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:52,787, word2vec, INFO, EPOCH 5 - PROGRESS: at 68.42% examples, 51616 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:54,029, word2vec, INFO, EPOCH 5 - PROGRESS: at 70.05% examples, 51654 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:55,153, word2vec, INFO, EPOCH 5 - PROGRESS: at 71.70% examples, 51794 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:56,470, word2vec, INFO, EPOCH 5 - PROGRESS: at 73.60% examples, 51915 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:17:57,637, word2vec, INFO, EPOCH 5 - PROGRESS: at 75.21% examples, 52012 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:17:58,859, word2vec, INFO, EPOCH 5 - PROGRESS: at 77.09% examples, 52212 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:00,045, word2vec, INFO, EPOCH 5 - PROGRESS: at 78.77% examples, 52280 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:01,066, word2vec, INFO, EPOCH 5 - PROGRESS: at 80.39% examples, 52490 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:02,217, word2vec, INFO, EPOCH 5 - PROGRESS: at 82.00% examples, 52568 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:03,438, word2vec, INFO, EPOCH 5 - PROGRESS: at 83.69% examples, 52595 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:04,677, word2vec, INFO, EPOCH 5 - PROGRESS: at 85.65% examples, 52751 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:05,920, word2vec, INFO, EPOCH 5 - PROGRESS: at 87.36% examples, 52754 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:06,941, word2vec, INFO, EPOCH 5 - PROGRESS: at 89.12% examples, 52934 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:08,094, word2vec, INFO, EPOCH 5 - PROGRESS: at 90.85% examples, 53003 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:09,143, word2vec, INFO, EPOCH 5 - PROGRESS: at 92.22% examples, 53014 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:10,346, word2vec, INFO, EPOCH 5 - PROGRESS: at 93.92% examples, 53046 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:11,428, word2vec, INFO, EPOCH 5 - PROGRESS: at 95.63% examples, 53165 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:12,587, word2vec, INFO, EPOCH 5 - PROGRESS: at 97.31% examples, 53219 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:13,731, word2vec, INFO, EPOCH 5 - PROGRESS: at 99.02% examples, 53289 words/s, in_qsize 4, out_qsize 0 ]
[2024-12-11 18:18:14,295, word2vec, INFO, EPOCH 5: training on 4170053 raw words (3937384 effective words) took 73.8s, 53376 effective words/s ]
[2024-12-11 18:18:15,690, word2vec, INFO, EPOCH 6 - PROGRESS: at 1.61% examples, 47352 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:17,141, word2vec, INFO, EPOCH 6 - PROGRESS: at 3.74% examples, 52900 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:18,512, word2vec, INFO, EPOCH 6 - PROGRESS: at 5.93% examples, 55727 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:19,902, word2vec, INFO, EPOCH 6 - PROGRESS: at 7.97% examples, 57016 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:20,934, word2vec, INFO, EPOCH 6 - PROGRESS: at 9.87% examples, 59485 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:22,289, word2vec, INFO, EPOCH 6 - PROGRESS: at 11.56% examples, 57639 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:23,321, word2vec, INFO, EPOCH 6 - PROGRESS: at 13.56% examples, 59399 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:24,660, word2vec, INFO, EPOCH 6 - PROGRESS: at 15.28% examples, 58078 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:25,759, word2vec, INFO, EPOCH 6 - PROGRESS: at 17.17% examples, 59077 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:26,988, word2vec, INFO, EPOCH 6 - PROGRESS: at 18.85% examples, 58547 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:28,029, word2vec, INFO, EPOCH 6 - PROGRESS: at 20.53% examples, 58894 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:29,134, word2vec, INFO, EPOCH 6 - PROGRESS: at 22.14% examples, 58960 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:30,416, word2vec, INFO, EPOCH 6 - PROGRESS: at 23.81% examples, 58365 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:31,478, word2vec, INFO, EPOCH 6 - PROGRESS: at 25.47% examples, 58579 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:32,687, word2vec, INFO, EPOCH 6 - PROGRESS: at 26.61% examples, 57292 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:33,755, word2vec, INFO, EPOCH 6 - PROGRESS: at 28.07% examples, 57052 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:34,795, word2vec, INFO, EPOCH 6 - PROGRESS: at 29.49% examples, 56905 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:35,834, word2vec, INFO, EPOCH 6 - PROGRESS: at 31.11% examples, 57226 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:37,005, word2vec, INFO, EPOCH 6 - PROGRESS: at 32.81% examples, 57173 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:38,135, word2vec, INFO, EPOCH 6 - PROGRESS: at 34.44% examples, 57208 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:39,181, word2vec, INFO, EPOCH 6 - PROGRESS: at 36.11% examples, 57438 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:40,333, word2vec, INFO, EPOCH 6 - PROGRESS: at 37.74% examples, 57433 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:41,651, word2vec, INFO, EPOCH 6 - PROGRESS: at 38.74% examples, 56036 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:42,667, word2vec, INFO, EPOCH 6 - PROGRESS: at 39.48% examples, 55028 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:43,689, word2vec, INFO, EPOCH 6 - PROGRESS: at 41.03% examples, 55359 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:44,780, word2vec, INFO, EPOCH 6 - PROGRESS: at 42.68% examples, 55537 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:45,967, word2vec, INFO, EPOCH 6 - PROGRESS: at 44.37% examples, 55519 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:47,082, word2vec, INFO, EPOCH 6 - PROGRESS: at 46.14% examples, 55640 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:48,123, word2vec, INFO, EPOCH 6 - PROGRESS: at 47.85% examples, 55875 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:49,239, word2vec, INFO, EPOCH 6 - PROGRESS: at 49.52% examples, 55976 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:50,431, word2vec, INFO, EPOCH 6 - PROGRESS: at 51.22% examples, 55954 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:51,438, word2vec, INFO, EPOCH 6 - PROGRESS: at 52.91% examples, 56208 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:52,475, word2vec, INFO, EPOCH 6 - PROGRESS: at 54.57% examples, 56410 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:53,735, word2vec, INFO, EPOCH 6 - PROGRESS: at 56.22% examples, 56280 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:18:54,846, word2vec, INFO, EPOCH 6 - PROGRESS: at 57.87% examples, 56362 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:56,064, word2vec, INFO, EPOCH 6 - PROGRESS: at 59.84% examples, 56519 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:57,143, word2vec, INFO, EPOCH 6 - PROGRESS: at 61.50% examples, 56622 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:58,172, word2vec, INFO, EPOCH 6 - PROGRESS: at 63.20% examples, 56794 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:18:59,302, word2vec, INFO, EPOCH 6 - PROGRESS: at 64.90% examples, 56825 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:00,378, word2vec, INFO, EPOCH 6 - PROGRESS: at 66.56% examples, 56923 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:01,431, word2vec, INFO, EPOCH 6 - PROGRESS: at 68.18% examples, 57050 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:02,682, word2vec, INFO, EPOCH 6 - PROGRESS: at 69.83% examples, 56940 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:03,705, word2vec, INFO, EPOCH 6 - PROGRESS: at 71.46% examples, 57095 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:04,734, word2vec, INFO, EPOCH 6 - PROGRESS: at 72.92% examples, 57048 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:05,750, word2vec, INFO, EPOCH 6 - PROGRESS: at 74.57% examples, 57202 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:07,001, word2vec, INFO, EPOCH 6 - PROGRESS: at 76.13% examples, 57096 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:08,031, word2vec, INFO, EPOCH 6 - PROGRESS: at 77.81% examples, 57226 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:09,069, word2vec, INFO, EPOCH 6 - PROGRESS: at 79.22% examples, 57177 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:10,075, word2vec, INFO, EPOCH 6 - PROGRESS: at 80.86% examples, 57322 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:11,227, word2vec, INFO, EPOCH 6 - PROGRESS: at 82.25% examples, 57151 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:12,297, word2vec, INFO, EPOCH 6 - PROGRESS: at 82.51% examples, 56259 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:13,334, word2vec, INFO, EPOCH 6 - PROGRESS: at 83.44% examples, 55908 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:14,354, word2vec, INFO, EPOCH 6 - PROGRESS: at 85.16% examples, 56051 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:15,495, word2vec, INFO, EPOCH 6 - PROGRESS: at 86.87% examples, 56083 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:16,638, word2vec, INFO, EPOCH 6 - PROGRESS: at 88.60% examples, 56111 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:17,678, word2vec, INFO, EPOCH 6 - PROGRESS: at 90.37% examples, 56227 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:18,894, word2vec, INFO, EPOCH 6 - PROGRESS: at 91.97% examples, 56191 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:20,101, word2vec, INFO, EPOCH 6 - PROGRESS: at 93.72% examples, 56159 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:21,157, word2vec, INFO, EPOCH 6 - PROGRESS: at 95.10% examples, 56117 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:22,207, word2vec, INFO, EPOCH 6 - PROGRESS: at 96.82% examples, 56217 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:23,446, word2vec, INFO, EPOCH 6 - PROGRESS: at 98.56% examples, 56162 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:24,301, word2vec, INFO, EPOCH 6: training on 4170053 raw words (3938776 effective words) took 70.0s, 56268 effective words/s ]
[2024-12-11 18:19:25,701, word2vec, INFO, EPOCH 7 - PROGRESS: at 1.62% examples, 47230 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:26,712, word2vec, INFO, EPOCH 7 - PROGRESS: at 3.53% examples, 58557 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:27,715, word2vec, INFO, EPOCH 7 - PROGRESS: at 4.95% examples, 57964 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:28,781, word2vec, INFO, EPOCH 7 - PROGRESS: at 5.93% examples, 52472 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:29,851, word2vec, INFO, EPOCH 7 - PROGRESS: at 7.28% examples, 52537 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:30,854, word2vec, INFO, EPOCH 7 - PROGRESS: at 8.70% examples, 53110 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:31,858, word2vec, INFO, EPOCH 7 - PROGRESS: at 10.32% examples, 54776 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:33,219, word2vec, INFO, EPOCH 7 - PROGRESS: at 12.31% examples, 54855 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:34,236, word2vec, INFO, EPOCH 7 - PROGRESS: at 14.04% examples, 55872 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:35,280, word2vec, INFO, EPOCH 7 - PROGRESS: at 15.76% examples, 56567 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:36,530, word2vec, INFO, EPOCH 7 - PROGRESS: at 17.41% examples, 56173 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:37,586, word2vec, INFO, EPOCH 7 - PROGRESS: at 19.32% examples, 57369 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:39,035, word2vec, INFO, EPOCH 7 - PROGRESS: at 21.00% examples, 56196 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:40,483, word2vec, INFO, EPOCH 7 - PROGRESS: at 23.12% examples, 56413 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:41,877, word2vec, INFO, EPOCH 7 - PROGRESS: at 25.25% examples, 56740 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:42,928, word2vec, INFO, EPOCH 7 - PROGRESS: at 27.09% examples, 57594 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:44,298, word2vec, INFO, EPOCH 7 - PROGRESS: at 28.76% examples, 56945 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:45,368, word2vec, INFO, EPOCH 7 - PROGRESS: at 30.64% examples, 57628 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:46,400, word2vec, INFO, EPOCH 7 - PROGRESS: at 32.10% examples, 57488 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:47,626, word2vec, INFO, EPOCH 7 - PROGRESS: at 32.35% examples, 54868 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:49,159, word2vec, INFO, EPOCH 7 - PROGRESS: at 33.07% examples, 52612 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:50,289, word2vec, INFO, EPOCH 7 - PROGRESS: at 34.48% examples, 52487 words/s, in_qsize 6, out_qsize 1 ]
[2024-12-11 18:19:51,309, word2vec, INFO, EPOCH 7 - PROGRESS: at 36.35% examples, 53287 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:52,670, word2vec, INFO, EPOCH 7 - PROGRESS: at 37.99% examples, 53055 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:53,685, word2vec, INFO, EPOCH 7 - PROGRESS: at 39.68% examples, 53464 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:54,736, word2vec, INFO, EPOCH 7 - PROGRESS: at 40.80% examples, 53169 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:19:55,809, word2vec, INFO, EPOCH 7 - PROGRESS: at 42.45% examples, 53450 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:56,879, word2vec, INFO, EPOCH 7 - PROGRESS: at 43.39% examples, 52849 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:58,199, word2vec, INFO, EPOCH 7 - PROGRESS: at 43.91% examples, 51342 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:19:59,294, word2vec, INFO, EPOCH 7 - PROGRESS: at 44.64% examples, 50531 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:00,315, word2vec, INFO, EPOCH 7 - PROGRESS: at 45.88% examples, 50403 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:01,319, word2vec, INFO, EPOCH 7 - PROGRESS: at 47.37% examples, 50561 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:02,537, word2vec, INFO, EPOCH 7 - PROGRESS: at 49.06% examples, 50674 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:03,655, word2vec, INFO, EPOCH 7 - PROGRESS: at 50.74% examples, 50909 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:04,770, word2vec, INFO, EPOCH 7 - PROGRESS: at 52.19% examples, 50897 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:05,846, word2vec, INFO, EPOCH 7 - PROGRESS: at 53.64% examples, 50936 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:07,019, word2vec, INFO, EPOCH 7 - PROGRESS: at 55.26% examples, 51084 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:08,184, word2vec, INFO, EPOCH 7 - PROGRESS: at 56.90% examples, 51223 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:09,432, word2vec, INFO, EPOCH 7 - PROGRESS: at 57.36% examples, 50225 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:10,766, word2vec, INFO, EPOCH 7 - PROGRESS: at 58.12% examples, 49392 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:12,282, word2vec, INFO, EPOCH 7 - PROGRESS: at 59.58% examples, 49009 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:13,731, word2vec, INFO, EPOCH 7 - PROGRESS: at 60.30% examples, 48144 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:14,808, word2vec, INFO, EPOCH 7 - PROGRESS: at 61.73% examples, 48224 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:16,737, word2vec, INFO, EPOCH 7 - PROGRESS: at 62.44% examples, 46987 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:17,810, word2vec, INFO, EPOCH 7 - PROGRESS: at 63.98% examples, 47097 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:18,841, word2vec, INFO, EPOCH 7 - PROGRESS: at 65.38% examples, 47238 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:19,999, word2vec, INFO, EPOCH 7 - PROGRESS: at 67.28% examples, 47603 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:21,373, word2vec, INFO, EPOCH 7 - PROGRESS: at 68.89% examples, 47615 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:22,536, word2vec, INFO, EPOCH 7 - PROGRESS: at 70.29% examples, 47635 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:23,607, word2vec, INFO, EPOCH 7 - PROGRESS: at 71.70% examples, 47727 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:24,689, word2vec, INFO, EPOCH 7 - PROGRESS: at 73.16% examples, 47805 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:25,890, word2vec, INFO, EPOCH 7 - PROGRESS: at 74.08% examples, 47483 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:27,486, word2vec, INFO, EPOCH 7 - PROGRESS: at 74.57% examples, 46581 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:28,495, word2vec, INFO, EPOCH 7 - PROGRESS: at 75.93% examples, 46730 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:29,571, word2vec, INFO, EPOCH 7 - PROGRESS: at 77.59% examples, 46967 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:30,580, word2vec, INFO, EPOCH 7 - PROGRESS: at 79.01% examples, 47106 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:31,623, word2vec, INFO, EPOCH 7 - PROGRESS: at 80.39% examples, 47218 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:32,722, word2vec, INFO, EPOCH 7 - PROGRESS: at 82.00% examples, 47412 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:33,981, word2vec, INFO, EPOCH 7 - PROGRESS: at 83.44% examples, 47365 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:35,102, word2vec, INFO, EPOCH 7 - PROGRESS: at 83.69% examples, 46749 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:36,833, word2vec, INFO, EPOCH 7 - PROGRESS: at 84.41% examples, 46023 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:37,908, word2vec, INFO, EPOCH 7 - PROGRESS: at 85.90% examples, 46116 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:39,066, word2vec, INFO, EPOCH 7 - PROGRESS: at 87.36% examples, 46157 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:40,278, word2vec, INFO, EPOCH 7 - PROGRESS: at 88.86% examples, 46163 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:41,981, word2vec, INFO, EPOCH 7 - PROGRESS: at 89.60% examples, 45515 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:43,200, word2vec, INFO, EPOCH 7 - PROGRESS: at 90.37% examples, 45167 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:44,502, word2vec, INFO, EPOCH 7 - PROGRESS: at 91.75% examples, 45140 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:45,871, word2vec, INFO, EPOCH 7 - PROGRESS: at 93.21% examples, 45073 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:47,023, word2vec, INFO, EPOCH 7 - PROGRESS: at 94.63% examples, 45129 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:48,154, word2vec, INFO, EPOCH 7 - PROGRESS: at 96.13% examples, 45193 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:49,276, word2vec, INFO, EPOCH 7 - PROGRESS: at 97.55% examples, 45257 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:20:50,468, word2vec, INFO, EPOCH 7 - PROGRESS: at 99.02% examples, 45287 words/s, in_qsize 4, out_qsize 0 ]
[2024-12-11 18:20:50,985, word2vec, INFO, EPOCH 7: training on 4170053 raw words (3938464 effective words) took 86.7s, 45438 effective words/s ]
[2024-12-11 18:20:52,010, word2vec, INFO, EPOCH 8 - PROGRESS: at 0.91% examples, 36886 words/s, in_qsize 6, out_qsize 0 ]
[2024-12-11 18:20:53,208, word2vec, INFO, EPOCH 8 - PROGRESS: at 2.36% examples, 42417 words/s, in_qsize 5, out_qsize 0 ]
[2024-12-11 18:21:16,302, word2vec, INFO, collecting all words and their counts ]
[2024-12-11 18:21:16,304, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-11 18:21:16,418, word2vec, INFO, PROGRESS: at sentence #10000, processed 407873 words, keeping 45074 word types ]
[2024-12-11 18:21:16,528, word2vec, INFO, PROGRESS: at sentence #20000, processed 808292 words, keeping 69250 word types ]
[2024-12-11 18:21:16,639, word2vec, INFO, PROGRESS: at sentence #30000, processed 1216627 words, keeping 90083 word types ]
[2024-12-11 18:21:16,753, word2vec, INFO, PROGRESS: at sentence #40000, processed 1621499 words, keeping 108581 word types ]
[2024-12-11 18:21:16,865, word2vec, INFO, PROGRESS: at sentence #50000, processed 2025370 words, keeping 125156 word types ]
[2024-12-11 18:21:16,977, word2vec, INFO, PROGRESS: at sentence #60000, processed 2428035 words, keeping 140773 word types ]
[2024-12-11 18:21:17,086, word2vec, INFO, PROGRESS: at sentence #70000, processed 2828048 words, keeping 155388 word types ]
[2024-12-11 18:21:17,210, word2vec, INFO, PROGRESS: at sentence #80000, processed 3240046 words, keeping 170201 word types ]
[2024-12-11 18:21:17,331, word2vec, INFO, PROGRESS: at sentence #90000, processed 3644180 words, keeping 184495 word types ]
[2024-12-11 18:21:17,445, word2vec, INFO, PROGRESS: at sentence #100000, processed 4040927 words, keeping 197609 word types ]
[2024-12-11 18:21:17,481, word2vec, INFO, collected 201843 word types from a corpus of 4170053 raw words and 103304 sentences ]
[2024-12-11 18:21:17,482, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-11 18:21:18,499, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 201843 unique words (100.00% of original 201843, drops 0)', 'datetime': '2024-12-11T18:21:18.499707', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 18:21:18,500, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4170053 word corpus (100.00% of original 4170053, drops 0)', 'datetime': '2024-12-11T18:21:18.500705', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 18:21:20,099, word2vec, INFO, deleting the raw counts dictionary of 201843 items ]
[2024-12-11 18:21:20,104, word2vec, INFO, sample=0.001 downsamples 29 most-common words ]
[2024-12-11 18:21:20,106, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 3937826.8948733485 word corpus (94.4%% of prior 4170053)', 'datetime': '2024-12-11T18:21:20.106290', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 18:21:24,570, fasttext, INFO, estimated required memory for 201843 words, 2000000 buckets and 500 dimensions: 4951844772 bytes ]
[2024-12-11 18:21:24,571, word2vec, INFO, resetting layer weights ]
[2024-12-11 18:21:44,184, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-11T18:21:44.184887', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]
[2024-12-11 18:21:44,185, utils, INFO, FastText lifecycle event {'msg': 'training model with 8 workers on 201843 vocabulary and 500 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-11T18:21:44.185883', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-11 18:21:45,441, word2vec, INFO, EPOCH 0 - PROGRESS: at 0.23% examples, 7599 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:21:47,240, word2vec, INFO, EPOCH 0 - PROGRESS: at 2.12% examples, 27853 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:21:48,533, word2vec, INFO, EPOCH 0 - PROGRESS: at 4.01% examples, 36827 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:21:49,675, word2vec, INFO, EPOCH 0 - PROGRESS: at 6.38% examples, 46269 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:21:50,860, word2vec, INFO, EPOCH 0 - PROGRESS: at 8.45% examples, 50733 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:21:51,917, word2vec, INFO, EPOCH 0 - PROGRESS: at 10.57% examples, 54767 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:21:52,984, word2vec, INFO, EPOCH 0 - PROGRESS: at 12.81% examples, 57756 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:21:54,000, word2vec, INFO, EPOCH 0 - PROGRESS: at 14.81% examples, 59453 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:21:55,353, word2vec, INFO, EPOCH 0 - PROGRESS: at 17.41% examples, 61516 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:21:57,366, word2vec, INFO, EPOCH 0 - PROGRESS: at 19.32% examples, 57830 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:21:58,643, word2vec, INFO, EPOCH 0 - PROGRESS: at 20.53% examples, 55966 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:21:59,715, word2vec, INFO, EPOCH 0 - PROGRESS: at 21.21% examples, 53929 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:00,811, word2vec, INFO, EPOCH 0 - PROGRESS: at 23.58% examples, 56037 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:01,961, word2vec, INFO, EPOCH 0 - PROGRESS: at 25.69% examples, 57160 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:03,027, word2vec, INFO, EPOCH 0 - PROGRESS: at 27.83% examples, 58432 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:04,074, word2vec, INFO, EPOCH 0 - PROGRESS: at 30.19% examples, 60084 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:05,078, word2vec, INFO, EPOCH 0 - PROGRESS: at 31.62% examples, 59895 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:06,106, word2vec, INFO, EPOCH 0 - PROGRESS: at 33.52% examples, 60503 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:07,133, word2vec, INFO, EPOCH 0 - PROGRESS: at 35.64% examples, 61492 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:08,139, word2vec, INFO, EPOCH 0 - PROGRESS: at 37.99% examples, 62825 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:09,145, word2vec, INFO, EPOCH 0 - PROGRESS: at 40.14% examples, 63689 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:10,259, word2vec, INFO, EPOCH 0 - PROGRESS: at 42.45% examples, 64582 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:11,552, word2vec, INFO, EPOCH 0 - PROGRESS: at 44.64% examples, 64609 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:12,677, word2vec, INFO, EPOCH 0 - PROGRESS: at 47.14% examples, 65356 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:13,694, word2vec, INFO, EPOCH 0 - PROGRESS: at 49.31% examples, 65972 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:14,857, word2vec, INFO, EPOCH 0 - PROGRESS: at 51.45% examples, 66229 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:15,913, word2vec, INFO, EPOCH 0 - PROGRESS: at 53.59% examples, 66698 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:16,965, word2vec, INFO, EPOCH 0 - PROGRESS: at 55.98% examples, 67429 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:18,148, word2vec, INFO, EPOCH 0 - PROGRESS: at 58.12% examples, 67570 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:19,239, word2vec, INFO, EPOCH 0 - PROGRESS: at 60.30% examples, 67883 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:20,288, word2vec, INFO, EPOCH 0 - PROGRESS: at 62.47% examples, 68242 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:21,377, word2vec, INFO, EPOCH 0 - PROGRESS: at 64.43% examples, 68260 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:22,448, word2vec, INFO, EPOCH 0 - PROGRESS: at 66.80% examples, 68801 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:23,532, word2vec, INFO, EPOCH 0 - PROGRESS: at 68.64% examples, 68820 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:24,711, word2vec, INFO, EPOCH 0 - PROGRESS: at 70.78% examples, 68911 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:25,716, word2vec, INFO, EPOCH 0 - PROGRESS: at 72.92% examples, 69282 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:26,749, word2vec, INFO, EPOCH 0 - PROGRESS: at 75.00% examples, 69588 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:27,826, word2vec, INFO, EPOCH 0 - PROGRESS: at 77.32% examples, 70029 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:28,920, word2vec, INFO, EPOCH 0 - PROGRESS: at 79.46% examples, 70218 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:30,018, word2vec, INFO, EPOCH 0 - PROGRESS: at 81.54% examples, 70374 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:31,386, word2vec, INFO, EPOCH 0 - PROGRESS: at 83.69% examples, 70123 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:32,423, word2vec, INFO, EPOCH 0 - PROGRESS: at 85.91% examples, 70365 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:33,438, word2vec, INFO, EPOCH 0 - PROGRESS: at 88.37% examples, 70827 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:34,483, word2vec, INFO, EPOCH 0 - PROGRESS: at 90.60% examples, 71036 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:35,519, word2vec, INFO, EPOCH 0 - PROGRESS: at 92.70% examples, 71252 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:36,768, word2vec, INFO, EPOCH 0 - PROGRESS: at 95.10% examples, 71351 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:38,106, word2vec, INFO, EPOCH 0 - PROGRESS: at 97.31% examples, 71142 words/s, in_qsize 11, out_qsize 0 ]
[2024-12-11 18:22:38,881, word2vec, INFO, EPOCH 0: training on 4170053 raw words (3937679 effective words) took 54.7s, 72007 effective words/s ]
[2024-12-11 18:22:40,707, word2vec, INFO, EPOCH 1 - PROGRESS: at 2.12% examples, 46535 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:41,720, word2vec, INFO, EPOCH 1 - PROGRESS: at 4.50% examples, 63000 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:42,723, word2vec, INFO, EPOCH 1 - PROGRESS: at 6.37% examples, 66064 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:43,808, word2vec, INFO, EPOCH 1 - PROGRESS: at 9.20% examples, 74455 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:45,505, word2vec, INFO, EPOCH 1 - PROGRESS: at 11.56% examples, 69585 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:46,525, word2vec, INFO, EPOCH 1 - PROGRESS: at 13.82% examples, 71386 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:22:47,544, word2vec, INFO, EPOCH 1 - PROGRESS: at 16.46% examples, 74959 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:48,659, word2vec, INFO, EPOCH 1 - PROGRESS: at 18.85% examples, 76020 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:49,679, word2vec, INFO, EPOCH 1 - PROGRESS: at 20.77% examples, 75795 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:22:50,690, word2vec, INFO, EPOCH 1 - PROGRESS: at 22.87% examples, 76486 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:52,093, word2vec, INFO, EPOCH 1 - PROGRESS: at 25.00% examples, 74755 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:53,243, word2vec, INFO, EPOCH 1 - PROGRESS: at 27.09% examples, 74678 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:54,321, word2vec, INFO, EPOCH 1 - PROGRESS: at 29.27% examples, 74945 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:55,364, word2vec, INFO, EPOCH 1 - PROGRESS: at 30.87% examples, 74207 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:56,485, word2vec, INFO, EPOCH 1 - PROGRESS: at 32.81% examples, 73749 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:22:57,658, word2vec, INFO, EPOCH 1 - PROGRESS: at 34.68% examples, 73125 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:58,855, word2vec, INFO, EPOCH 1 - PROGRESS: at 36.58% examples, 72500 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:22:59,870, word2vec, INFO, EPOCH 1 - PROGRESS: at 39.25% examples, 73931 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:23:01,074, word2vec, INFO, EPOCH 1 - PROGRESS: at 41.50% examples, 74163 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:02,090, word2vec, INFO, EPOCH 1 - PROGRESS: at 43.39% examples, 74159 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:03,132, word2vec, INFO, EPOCH 1 - PROGRESS: at 45.62% examples, 74441 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:04,463, word2vec, INFO, EPOCH 1 - PROGRESS: at 48.09% examples, 74248 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:05,538, word2vec, INFO, EPOCH 1 - PROGRESS: at 50.29% examples, 74434 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:06,594, word2vec, INFO, EPOCH 1 - PROGRESS: at 52.42% examples, 74647 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:07,617, word2vec, INFO, EPOCH 1 - PROGRESS: at 54.32% examples, 74613 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:08,822, word2vec, INFO, EPOCH 1 - PROGRESS: at 56.90% examples, 75069 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:09,833, word2vec, INFO, EPOCH 1 - PROGRESS: at 58.86% examples, 75045 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:10,856, word2vec, INFO, EPOCH 1 - PROGRESS: at 60.99% examples, 75272 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:11,858, word2vec, INFO, EPOCH 1 - PROGRESS: at 62.69% examples, 74984 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:13,062, word2vec, INFO, EPOCH 1 - PROGRESS: at 64.90% examples, 74810 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:14,195, word2vec, INFO, EPOCH 1 - PROGRESS: at 66.80% examples, 74534 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:15,265, word2vec, INFO, EPOCH 1 - PROGRESS: at 68.67% examples, 74413 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:16,343, word2vec, INFO, EPOCH 1 - PROGRESS: at 70.78% examples, 74537 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:17,427, word2vec, INFO, EPOCH 1 - PROGRESS: at 72.94% examples, 74632 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:18,458, word2vec, INFO, EPOCH 1 - PROGRESS: at 74.76% examples, 74595 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:19,506, word2vec, INFO, EPOCH 1 - PROGRESS: at 76.39% examples, 74293 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:20,587, word2vec, INFO, EPOCH 1 - PROGRESS: at 78.29% examples, 74170 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:21,680, word2vec, INFO, EPOCH 1 - PROGRESS: at 79.94% examples, 73817 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:23:22,819, word2vec, INFO, EPOCH 1 - PROGRESS: at 81.54% examples, 73391 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:23,963, word2vec, INFO, EPOCH 1 - PROGRESS: at 83.21% examples, 72983 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:25,126, word2vec, INFO, EPOCH 1 - PROGRESS: at 85.42% examples, 72974 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:26,387, word2vec, INFO, EPOCH 1 - PROGRESS: at 87.61% examples, 72819 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:27,530, word2vec, INFO, EPOCH 1 - PROGRESS: at 89.60% examples, 72656 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:28,635, word2vec, INFO, EPOCH 1 - PROGRESS: at 91.55% examples, 72555 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:29,655, word2vec, INFO, EPOCH 1 - PROGRESS: at 93.47% examples, 72579 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:30,693, word2vec, INFO, EPOCH 1 - PROGRESS: at 95.62% examples, 72760 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:31,768, word2vec, INFO, EPOCH 1 - PROGRESS: at 97.55% examples, 72698 words/s, in_qsize 10, out_qsize 0 ]
[2024-12-11 18:23:32,748, word2vec, INFO, EPOCH 1: training on 4170053 raw words (3937283 effective words) took 53.9s, 73105 effective words/s ]
[2024-12-11 18:23:33,889, word2vec, INFO, EPOCH 2 - PROGRESS: at 0.20% examples, 8515 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:35,011, word2vec, INFO, EPOCH 2 - PROGRESS: at 2.12% examples, 37836 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:36,214, word2vec, INFO, EPOCH 2 - PROGRESS: at 3.98% examples, 46393 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:37,347, word2vec, INFO, EPOCH 2 - PROGRESS: at 5.93% examples, 51274 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:38,868, word2vec, INFO, EPOCH 2 - PROGRESS: at 7.73% examples, 50848 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:40,264, word2vec, INFO, EPOCH 2 - PROGRESS: at 9.87% examples, 52664 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:41,376, word2vec, INFO, EPOCH 2 - PROGRESS: at 11.56% examples, 53509 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:42,437, word2vec, INFO, EPOCH 2 - PROGRESS: at 13.56% examples, 55425 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:43,625, word2vec, INFO, EPOCH 2 - PROGRESS: at 15.78% examples, 57164 words/s, in_qsize 15, out_qsize 1 ]
[2024-12-11 18:23:44,627, word2vec, INFO, EPOCH 2 - PROGRESS: at 18.35% examples, 61081 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:45,654, word2vec, INFO, EPOCH 2 - PROGRESS: at 20.77% examples, 63497 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:46,668, word2vec, INFO, EPOCH 2 - PROGRESS: at 22.87% examples, 64970 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:48,209, word2vec, INFO, EPOCH 2 - PROGRESS: at 25.00% examples, 63954 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:49,271, word2vec, INFO, EPOCH 2 - PROGRESS: at 27.09% examples, 64978 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:50,281, word2vec, INFO, EPOCH 2 - PROGRESS: at 29.00% examples, 65533 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:51,282, word2vec, INFO, EPOCH 2 - PROGRESS: at 31.14% examples, 66564 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:52,388, word2vec, INFO, EPOCH 2 - PROGRESS: at 33.52% examples, 67598 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:53,412, word2vec, INFO, EPOCH 2 - PROGRESS: at 35.64% examples, 68324 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:54,437, word2vec, INFO, EPOCH 2 - PROGRESS: at 37.74% examples, 69007 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:55,479, word2vec, INFO, EPOCH 2 - PROGRESS: at 39.71% examples, 69158 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:56,838, word2vec, INFO, EPOCH 2 - PROGRESS: at 41.95% examples, 69161 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:57,848, word2vec, INFO, EPOCH 2 - PROGRESS: at 44.14% examples, 69732 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:23:58,929, word2vec, INFO, EPOCH 2 - PROGRESS: at 46.39% examples, 70079 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:23:59,963, word2vec, INFO, EPOCH 2 - PROGRESS: at 48.09% examples, 69835 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:01,094, word2vec, INFO, EPOCH 2 - PROGRESS: at 50.29% examples, 70037 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:02,100, word2vec, INFO, EPOCH 2 - PROGRESS: at 52.19% examples, 70195 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:03,138, word2vec, INFO, EPOCH 2 - PROGRESS: at 54.57% examples, 70904 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:04,227, word2vec, INFO, EPOCH 2 - PROGRESS: at 56.70% examples, 71135 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:05,242, word2vec, INFO, EPOCH 2 - PROGRESS: at 58.61% examples, 71234 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:06,294, word2vec, INFO, EPOCH 2 - PROGRESS: at 60.76% examples, 71501 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:07,445, word2vec, INFO, EPOCH 2 - PROGRESS: at 62.94% examples, 71573 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:08,448, word2vec, INFO, EPOCH 2 - PROGRESS: at 64.90% examples, 71661 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:09,601, word2vec, INFO, EPOCH 2 - PROGRESS: at 67.28% examples, 71966 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:10,653, word2vec, INFO, EPOCH 2 - PROGRESS: at 69.36% examples, 72209 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:11,660, word2vec, INFO, EPOCH 2 - PROGRESS: at 71.21% examples, 72277 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:12,695, word2vec, INFO, EPOCH 2 - PROGRESS: at 73.38% examples, 72516 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:13,727, word2vec, INFO, EPOCH 2 - PROGRESS: at 75.21% examples, 72529 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:14,863, word2vec, INFO, EPOCH 2 - PROGRESS: at 77.32% examples, 72581 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:15,935, word2vec, INFO, EPOCH 2 - PROGRESS: at 79.46% examples, 72745 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:16,940, word2vec, INFO, EPOCH 2 - PROGRESS: at 81.30% examples, 72787 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:18,090, word2vec, INFO, EPOCH 2 - PROGRESS: at 83.44% examples, 72803 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:19,272, word2vec, INFO, EPOCH 2 - PROGRESS: at 85.68% examples, 72768 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:20,298, word2vec, INFO, EPOCH 2 - PROGRESS: at 87.61% examples, 72784 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:21,350, word2vec, INFO, EPOCH 2 - PROGRESS: at 90.37% examples, 73336 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:22,733, word2vec, INFO, EPOCH 2 - PROGRESS: at 92.70% examples, 73188 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:23,957, word2vec, INFO, EPOCH 2 - PROGRESS: at 94.85% examples, 73097 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:25,045, word2vec, INFO, EPOCH 2 - PROGRESS: at 97.05% examples, 73188 words/s, in_qsize 12, out_qsize 0 ]
[2024-12-11 18:24:26,057, word2vec, INFO, EPOCH 2 - PROGRESS: at 99.73% examples, 73719 words/s, in_qsize 1, out_qsize 1 ]
[2024-12-11 18:24:26,091, word2vec, INFO, EPOCH 2: training on 4170053 raw words (3937549 effective words) took 53.3s, 73849 effective words/s ]
[2024-12-11 18:24:27,137, word2vec, INFO, EPOCH 3 - PROGRESS: at 1.14% examples, 45511 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:28,146, word2vec, INFO, EPOCH 3 - PROGRESS: at 3.03% examples, 59647 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:29,850, word2vec, INFO, EPOCH 3 - PROGRESS: at 5.93% examples, 62590 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:30,934, word2vec, INFO, EPOCH 3 - PROGRESS: at 7.97% examples, 66082 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:31,961, word2vec, INFO, EPOCH 3 - PROGRESS: at 9.87% examples, 67323 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:24:33,018, word2vec, INFO, EPOCH 3 - PROGRESS: at 12.31% examples, 70641 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:34,022, word2vec, INFO, EPOCH 3 - PROGRESS: at 14.78% examples, 73568 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:35,066, word2vec, INFO, EPOCH 3 - PROGRESS: at 17.17% examples, 75498 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:36,203, word2vec, INFO, EPOCH 3 - PROGRESS: at 19.10% examples, 74442 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:37,360, word2vec, INFO, EPOCH 3 - PROGRESS: at 20.30% examples, 70963 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:38,363, word2vec, INFO, EPOCH 3 - PROGRESS: at 21.91% examples, 70564 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:39,376, word2vec, INFO, EPOCH 3 - PROGRESS: at 23.58% examples, 70134 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:40,394, word2vec, INFO, EPOCH 3 - PROGRESS: at 25.25% examples, 69729 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:41,677, word2vec, INFO, EPOCH 3 - PROGRESS: at 27.09% examples, 68827 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:42,887, word2vec, INFO, EPOCH 3 - PROGRESS: at 28.30% examples, 66673 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:43,976, word2vec, INFO, EPOCH 3 - PROGRESS: at 29.22% examples, 64718 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:44,980, word2vec, INFO, EPOCH 3 - PROGRESS: at 31.12% examples, 65270 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:46,002, word2vec, INFO, EPOCH 3 - PROGRESS: at 33.75% examples, 67111 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:47,203, word2vec, INFO, EPOCH 3 - PROGRESS: at 35.87% examples, 67272 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:48,257, word2vec, INFO, EPOCH 3 - PROGRESS: at 37.99% examples, 67901 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:49,400, word2vec, INFO, EPOCH 3 - PROGRESS: at 40.14% examples, 68208 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:50,599, word2vec, INFO, EPOCH 3 - PROGRESS: at 42.21% examples, 68327 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:51,755, word2vec, INFO, EPOCH 3 - PROGRESS: at 44.37% examples, 68528 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:52,927, word2vec, INFO, EPOCH 3 - PROGRESS: at 46.90% examples, 69036 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:53,963, word2vec, INFO, EPOCH 3 - PROGRESS: at 49.06% examples, 69507 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:55,010, word2vec, INFO, EPOCH 3 - PROGRESS: at 51.22% examples, 69921 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:24:56,041, word2vec, INFO, EPOCH 3 - PROGRESS: at 53.64% examples, 70651 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:57,103, word2vec, INFO, EPOCH 3 - PROGRESS: at 55.50% examples, 70669 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:58,337, word2vec, INFO, EPOCH 3 - PROGRESS: at 57.61% examples, 70589 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:24:59,372, word2vec, INFO, EPOCH 3 - PROGRESS: at 59.81% examples, 70941 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:00,373, word2vec, INFO, EPOCH 3 - PROGRESS: at 61.75% examples, 71044 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:01,539, word2vec, INFO, EPOCH 3 - PROGRESS: at 64.20% examples, 71355 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:02,656, word2vec, INFO, EPOCH 3 - PROGRESS: at 66.30% examples, 71481 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:03,684, word2vec, INFO, EPOCH 3 - PROGRESS: at 68.64% examples, 72032 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:05,063, word2vec, INFO, EPOCH 3 - PROGRESS: at 70.78% examples, 71657 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:06,127, word2vec, INFO, EPOCH 3 - PROGRESS: at 72.92% examples, 71866 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:07,148, word2vec, INFO, EPOCH 3 - PROGRESS: at 74.81% examples, 71911 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:08,313, word2vec, INFO, EPOCH 3 - PROGRESS: at 77.10% examples, 72161 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:09,336, word2vec, INFO, EPOCH 3 - PROGRESS: at 79.22% examples, 72417 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:10,345, word2vec, INFO, EPOCH 3 - PROGRESS: at 81.07% examples, 72461 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:11,381, word2vec, INFO, EPOCH 3 - PROGRESS: at 83.44% examples, 72877 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:12,504, word2vec, INFO, EPOCH 3 - PROGRESS: at 85.65% examples, 72936 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:13,539, word2vec, INFO, EPOCH 3 - PROGRESS: at 87.61% examples, 72930 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:14,725, word2vec, INFO, EPOCH 3 - PROGRESS: at 89.84% examples, 72894 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:15,794, word2vec, INFO, EPOCH 3 - PROGRESS: at 91.97% examples, 73028 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:16,898, word2vec, INFO, EPOCH 3 - PROGRESS: at 94.15% examples, 73103 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:18,230, word2vec, INFO, EPOCH 3 - PROGRESS: at 95.63% examples, 72315 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:19,515, word2vec, INFO, EPOCH 3 - PROGRESS: at 97.55% examples, 71981 words/s, in_qsize 10, out_qsize 0 ]
[2024-12-11 18:25:20,566, word2vec, INFO, EPOCH 3 - PROGRESS: at 99.50% examples, 71957 words/s, in_qsize 2, out_qsize 1 ]
[2024-12-11 18:25:20,580, word2vec, INFO, EPOCH 3: training on 4170053 raw words (3937795 effective words) took 54.5s, 72282 effective words/s ]
[2024-12-11 18:25:21,990, word2vec, INFO, EPOCH 4 - PROGRESS: at 0.23% examples, 6836 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:25:23,338, word2vec, INFO, EPOCH 4 - PROGRESS: at 2.12% examples, 31282 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:24,493, word2vec, INFO, EPOCH 4 - PROGRESS: at 4.72% examples, 48629 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:25,665, word2vec, INFO, EPOCH 4 - PROGRESS: at 6.81% examples, 54083 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:26,684, word2vec, INFO, EPOCH 4 - PROGRESS: at 8.95% examples, 58948 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:27,762, word2vec, INFO, EPOCH 4 - PROGRESS: at 11.05% examples, 61922 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:28,887, word2vec, INFO, EPOCH 4 - PROGRESS: at 13.56% examples, 64881 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:29,894, word2vec, INFO, EPOCH 4 - PROGRESS: at 15.52% examples, 65956 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:31,073, word2vec, INFO, EPOCH 4 - PROGRESS: at 17.65% examples, 66620 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:32,074, word2vec, INFO, EPOCH 4 - PROGRESS: at 19.58% examples, 67363 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:33,098, word2vec, INFO, EPOCH 4 - PROGRESS: at 21.44% examples, 67880 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:34,116, word2vec, INFO, EPOCH 4 - PROGRESS: at 23.33% examples, 68342 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:35,128, word2vec, INFO, EPOCH 4 - PROGRESS: at 25.47% examples, 69387 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:36,131, word2vec, INFO, EPOCH 4 - PROGRESS: at 27.58% examples, 70378 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:37,180, word2vec, INFO, EPOCH 4 - PROGRESS: at 29.71% examples, 71025 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:38,199, word2vec, INFO, EPOCH 4 - PROGRESS: at 31.87% examples, 71726 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:39,210, word2vec, INFO, EPOCH 4 - PROGRESS: at 33.75% examples, 71867 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:40,221, word2vec, INFO, EPOCH 4 - PROGRESS: at 36.11% examples, 72934 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:41,845, word2vec, INFO, EPOCH 4 - PROGRESS: at 38.27% examples, 71340 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:42,951, word2vec, INFO, EPOCH 4 - PROGRESS: at 40.14% examples, 71181 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:43,984, word2vec, INFO, EPOCH 4 - PROGRESS: at 41.95% examples, 71258 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:45,024, word2vec, INFO, EPOCH 4 - PROGRESS: at 43.91% examples, 71296 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:46,069, word2vec, INFO, EPOCH 4 - PROGRESS: at 45.88% examples, 71305 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:47,162, word2vec, INFO, EPOCH 4 - PROGRESS: at 48.07% examples, 71559 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:48,169, word2vec, INFO, EPOCH 4 - PROGRESS: at 50.04% examples, 71671 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:25:49,178, word2vec, INFO, EPOCH 4 - PROGRESS: at 52.42% examples, 72432 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:50,378, word2vec, INFO, EPOCH 4 - PROGRESS: at 54.79% examples, 72677 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:51,388, word2vec, INFO, EPOCH 4 - PROGRESS: at 56.70% examples, 72734 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:52,414, word2vec, INFO, EPOCH 4 - PROGRESS: at 58.61% examples, 72758 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:53,478, word2vec, INFO, EPOCH 4 - PROGRESS: at 60.54% examples, 72691 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:54,584, word2vec, INFO, EPOCH 4 - PROGRESS: at 62.69% examples, 72797 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:55,697, word2vec, INFO, EPOCH 4 - PROGRESS: at 64.90% examples, 72894 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:56,771, word2vec, INFO, EPOCH 4 - PROGRESS: at 66.80% examples, 72803 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:25:57,967, word2vec, INFO, EPOCH 4 - PROGRESS: at 68.64% examples, 72488 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:25:59,104, word2vec, INFO, EPOCH 4 - PROGRESS: at 70.74% examples, 72547 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:00,183, word2vec, INFO, EPOCH 4 - PROGRESS: at 72.92% examples, 72708 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:01,379, word2vec, INFO, EPOCH 4 - PROGRESS: at 75.00% examples, 72653 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:02,418, word2vec, INFO, EPOCH 4 - PROGRESS: at 77.09% examples, 72874 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:03,482, word2vec, INFO, EPOCH 4 - PROGRESS: at 79.01% examples, 72824 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:04,540, word2vec, INFO, EPOCH 4 - PROGRESS: at 81.30% examples, 73209 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:05,619, word2vec, INFO, EPOCH 4 - PROGRESS: at 83.44% examples, 73332 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:07,085, word2vec, INFO, EPOCH 4 - PROGRESS: at 85.65% examples, 72840 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:08,090, word2vec, INFO, EPOCH 4 - PROGRESS: at 87.84% examples, 73080 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:09,160, word2vec, INFO, EPOCH 4 - PROGRESS: at 90.37% examples, 73405 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:10,182, word2vec, INFO, EPOCH 4 - PROGRESS: at 92.22% examples, 73409 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:11,215, word2vec, INFO, EPOCH 4 - PROGRESS: at 94.38% examples, 73588 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:12,215, word2vec, INFO, EPOCH 4 - PROGRESS: at 96.36% examples, 73614 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:13,283, word2vec, INFO, EPOCH 4 - PROGRESS: at 98.56% examples, 73728 words/s, in_qsize 6, out_qsize 1 ]
[2024-12-11 18:26:13,847, word2vec, INFO, EPOCH 4: training on 4170053 raw words (3937293 effective words) took 53.2s, 73986 effective words/s ]
[2024-12-11 18:26:16,307, word2vec, INFO, EPOCH 5 - PROGRESS: at 0.23% examples, 3850 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:17,318, word2vec, INFO, EPOCH 5 - PROGRESS: at 2.55% examples, 29774 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:26:18,327, word2vec, INFO, EPOCH 5 - PROGRESS: at 4.49% examples, 39907 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:19,334, word2vec, INFO, EPOCH 5 - PROGRESS: at 7.28% examples, 53141 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:20,975, word2vec, INFO, EPOCH 5 - PROGRESS: at 9.68% examples, 54079 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:22,117, word2vec, INFO, EPOCH 5 - PROGRESS: at 11.81% examples, 56862 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:23,119, word2vec, INFO, EPOCH 5 - PROGRESS: at 14.05% examples, 59847 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:24,125, word2vec, INFO, EPOCH 5 - PROGRESS: at 16.24% examples, 62228 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:25,150, word2vec, INFO, EPOCH 5 - PROGRESS: at 18.60% examples, 64922 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:26,158, word2vec, INFO, EPOCH 5 - PROGRESS: at 20.53% examples, 65709 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:27,191, word2vec, INFO, EPOCH 5 - PROGRESS: at 22.64% examples, 66980 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:28,256, word2vec, INFO, EPOCH 5 - PROGRESS: at 24.74% examples, 67896 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:29,730, word2vec, INFO, EPOCH 5 - PROGRESS: at 26.88% examples, 66931 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:30,808, word2vec, INFO, EPOCH 5 - PROGRESS: at 29.00% examples, 67674 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:31,837, word2vec, INFO, EPOCH 5 - PROGRESS: at 30.87% examples, 67989 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:32,848, word2vec, INFO, EPOCH 5 - PROGRESS: at 33.03% examples, 68825 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:33,914, word2vec, INFO, EPOCH 5 - PROGRESS: at 35.40% examples, 69823 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:34,921, word2vec, INFO, EPOCH 5 - PROGRESS: at 37.29% examples, 70059 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:35,927, word2vec, INFO, EPOCH 5 - PROGRESS: at 39.25% examples, 70284 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:37,084, word2vec, INFO, EPOCH 5 - PROGRESS: at 41.27% examples, 70434 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:38,125, word2vec, INFO, EPOCH 5 - PROGRESS: at 43.39% examples, 70898 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:39,165, word2vec, INFO, EPOCH 5 - PROGRESS: at 45.35% examples, 70943 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:40,178, word2vec, INFO, EPOCH 5 - PROGRESS: at 47.85% examples, 71779 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:41,225, word2vec, INFO, EPOCH 5 - PROGRESS: at 49.77% examples, 71787 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:42,492, word2vec, INFO, EPOCH 5 - PROGRESS: at 52.19% examples, 71892 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:43,708, word2vec, INFO, EPOCH 5 - PROGRESS: at 54.32% examples, 71807 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:44,903, word2vec, INFO, EPOCH 5 - PROGRESS: at 56.44% examples, 71766 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:45,923, word2vec, INFO, EPOCH 5 - PROGRESS: at 58.61% examples, 72127 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:46,969, word2vec, INFO, EPOCH 5 - PROGRESS: at 60.76% examples, 72386 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:48,098, word2vec, INFO, EPOCH 5 - PROGRESS: at 62.95% examples, 72475 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:26:49,102, word2vec, INFO, EPOCH 5 - PROGRESS: at 65.13% examples, 72800 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:50,189, word2vec, INFO, EPOCH 5 - PROGRESS: at 67.48% examples, 73208 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:51,751, word2vec, INFO, EPOCH 5 - PROGRESS: at 69.61% examples, 72428 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:52,909, word2vec, INFO, EPOCH 5 - PROGRESS: at 71.94% examples, 72694 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:53,930, word2vec, INFO, EPOCH 5 - PROGRESS: at 73.83% examples, 72716 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:55,004, word2vec, INFO, EPOCH 5 - PROGRESS: at 75.67% examples, 72653 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:26:56,032, word2vec, INFO, EPOCH 5 - PROGRESS: at 77.81% examples, 72891 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:57,078, word2vec, INFO, EPOCH 5 - PROGRESS: at 79.94% examples, 73092 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:58,130, word2vec, INFO, EPOCH 5 - PROGRESS: at 82.25% examples, 73466 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:26:59,206, word2vec, INFO, EPOCH 5 - PROGRESS: at 84.41% examples, 73589 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:00,256, word2vec, INFO, EPOCH 5 - PROGRESS: at 86.41% examples, 73543 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:01,590, word2vec, INFO, EPOCH 5 - PROGRESS: at 88.60% examples, 73262 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:02,628, word2vec, INFO, EPOCH 5 - PROGRESS: at 90.85% examples, 73433 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:03,859, word2vec, INFO, EPOCH 5 - PROGRESS: at 92.95% examples, 73319 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:04,979, word2vec, INFO, EPOCH 5 - PROGRESS: at 95.10% examples, 73372 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:06,004, word2vec, INFO, EPOCH 5 - PROGRESS: at 97.05% examples, 73367 words/s, in_qsize 12, out_qsize 0 ]
[2024-12-11 18:27:07,018, word2vec, INFO, EPOCH 5 - PROGRESS: at 99.51% examples, 73736 words/s, in_qsize 2, out_qsize 1 ]
[2024-12-11 18:27:07,073, word2vec, INFO, EPOCH 5: training on 4170053 raw words (3937566 effective words) took 53.2s, 73992 effective words/s ]
[2024-12-11 18:27:08,100, word2vec, INFO, EPOCH 6 - PROGRESS: at 0.96% examples, 37415 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:27:09,100, word2vec, INFO, EPOCH 6 - PROGRESS: at 2.57% examples, 51416 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:10,112, word2vec, INFO, EPOCH 6 - PROGRESS: at 4.95% examples, 65448 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:27:11,129, word2vec, INFO, EPOCH 6 - PROGRESS: at 6.60% examples, 65217 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:12,132, word2vec, INFO, EPOCH 6 - PROGRESS: at 8.95% examples, 70891 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:13,141, word2vec, INFO, EPOCH 6 - PROGRESS: at 11.05% examples, 73069 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:14,168, word2vec, INFO, EPOCH 6 - PROGRESS: at 13.05% examples, 73091 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:15,192, word2vec, INFO, EPOCH 6 - PROGRESS: at 15.28% examples, 74308 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:16,669, word2vec, INFO, EPOCH 6 - PROGRESS: at 17.41% examples, 71690 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:17,800, word2vec, INFO, EPOCH 6 - PROGRESS: at 19.58% examples, 72020 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:18,848, word2vec, INFO, EPOCH 6 - PROGRESS: at 21.44% examples, 72013 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:19,882, word2vec, INFO, EPOCH 6 - PROGRESS: at 23.33% examples, 72083 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:21,015, word2vec, INFO, EPOCH 6 - PROGRESS: at 25.69% examples, 72959 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:22,051, word2vec, INFO, EPOCH 6 - PROGRESS: at 27.58% examples, 72954 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:23,106, word2vec, INFO, EPOCH 6 - PROGRESS: at 29.49% examples, 72848 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:24,120, word2vec, INFO, EPOCH 6 - PROGRESS: at 31.63% examples, 73495 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:25,139, word2vec, INFO, EPOCH 6 - PROGRESS: at 33.75% examples, 74021 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:26,265, word2vec, INFO, EPOCH 6 - PROGRESS: at 35.87% examples, 74061 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:27,336, word2vec, INFO, EPOCH 6 - PROGRESS: at 37.75% examples, 73863 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:27:28,346, word2vec, INFO, EPOCH 6 - PROGRESS: at 39.68% examples, 73896 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:29,396, word2vec, INFO, EPOCH 6 - PROGRESS: at 41.70% examples, 74225 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:30,429, word2vec, INFO, EPOCH 6 - PROGRESS: at 43.91% examples, 74557 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:31,722, word2vec, INFO, EPOCH 6 - PROGRESS: at 46.14% examples, 74060 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:32,916, word2vec, INFO, EPOCH 6 - PROGRESS: at 48.31% examples, 73914 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:33,969, word2vec, INFO, EPOCH 6 - PROGRESS: at 50.50% examples, 74170 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:34,978, word2vec, INFO, EPOCH 6 - PROGRESS: at 52.65% examples, 74518 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:36,177, word2vec, INFO, EPOCH 6 - PROGRESS: at 54.79% examples, 74361 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:37,241, word2vec, INFO, EPOCH 6 - PROGRESS: at 56.90% examples, 74543 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:38,266, word2vec, INFO, EPOCH 6 - PROGRESS: at 58.86% examples, 74507 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:39,348, word2vec, INFO, EPOCH 6 - PROGRESS: at 60.99% examples, 74616 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:40,348, word2vec, INFO, EPOCH 6 - PROGRESS: at 62.94% examples, 74636 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:41,397, word2vec, INFO, EPOCH 6 - PROGRESS: at 65.13% examples, 74813 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:42,430, word2vec, INFO, EPOCH 6 - PROGRESS: at 67.28% examples, 75015 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:43,527, word2vec, INFO, EPOCH 6 - PROGRESS: at 69.36% examples, 75087 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:44,544, word2vec, INFO, EPOCH 6 - PROGRESS: at 71.21% examples, 75055 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:45,869, word2vec, INFO, EPOCH 6 - PROGRESS: at 73.38% examples, 74669 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:27:46,893, word2vec, INFO, EPOCH 6 - PROGRESS: at 75.44% examples, 74880 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:47,929, word2vec, INFO, EPOCH 6 - PROGRESS: at 77.32% examples, 74822 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:48,947, word2vec, INFO, EPOCH 6 - PROGRESS: at 79.46% examples, 75029 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:49,997, word2vec, INFO, EPOCH 6 - PROGRESS: at 81.31% examples, 74942 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:27:51,303, word2vec, INFO, EPOCH 6 - PROGRESS: at 83.90% examples, 75063 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:52,358, word2vec, INFO, EPOCH 6 - PROGRESS: at 86.41% examples, 75386 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:53,402, word2vec, INFO, EPOCH 6 - PROGRESS: at 88.37% examples, 75312 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:54,642, word2vec, INFO, EPOCH 6 - PROGRESS: at 90.60% examples, 75126 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:55,855, word2vec, INFO, EPOCH 6 - PROGRESS: at 92.70% examples, 74993 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:56,906, word2vec, INFO, EPOCH 6 - PROGRESS: at 95.13% examples, 75300 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:27:57,925, word2vec, INFO, EPOCH 6 - PROGRESS: at 97.08% examples, 75267 words/s, in_qsize 12, out_qsize 0 ]
[2024-12-11 18:27:59,062, word2vec, INFO, EPOCH 6 - PROGRESS: at 99.51% examples, 75431 words/s, in_qsize 2, out_qsize 1 ]
[2024-12-11 18:27:59,223, word2vec, INFO, EPOCH 6: training on 4170053 raw words (3937674 effective words) took 52.1s, 75538 effective words/s ]
[2024-12-11 18:28:00,270, word2vec, INFO, EPOCH 7 - PROGRESS: at 1.42% examples, 54864 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:01,293, word2vec, INFO, EPOCH 7 - PROGRESS: at 3.06% examples, 59454 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:02,354, word2vec, INFO, EPOCH 7 - PROGRESS: at 5.66% examples, 72449 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:03,960, word2vec, INFO, EPOCH 7 - PROGRESS: at 7.73% examples, 65728 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:05,026, word2vec, INFO, EPOCH 7 - PROGRESS: at 9.68% examples, 66600 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:06,062, word2vec, INFO, EPOCH 7 - PROGRESS: at 11.56% examples, 67534 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:07,122, word2vec, INFO, EPOCH 7 - PROGRESS: at 13.80% examples, 69205 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:08,164, word2vec, INFO, EPOCH 7 - PROGRESS: at 16.02% examples, 70613 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:28:09,258, word2vec, INFO, EPOCH 7 - PROGRESS: at 17.90% examples, 70420 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:28:10,308, word2vec, INFO, EPOCH 7 - PROGRESS: at 20.06% examples, 71372 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:11,337, word2vec, INFO, EPOCH 7 - PROGRESS: at 22.14% examples, 72321 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:12,481, word2vec, INFO, EPOCH 7 - PROGRESS: at 24.51% examples, 73157 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:13,508, word2vec, INFO, EPOCH 7 - PROGRESS: at 26.61% examples, 73839 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:15,082, word2vec, INFO, EPOCH 7 - PROGRESS: at 28.76% examples, 71853 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:16,082, word2vec, INFO, EPOCH 7 - PROGRESS: at 30.64% examples, 72056 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:17,123, word2vec, INFO, EPOCH 7 - PROGRESS: at 32.60% examples, 72067 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:28:18,168, word2vec, INFO, EPOCH 7 - PROGRESS: at 35.40% examples, 74010 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:19,212, word2vec, INFO, EPOCH 7 - PROGRESS: at 37.50% examples, 74387 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:20,242, word2vec, INFO, EPOCH 7 - PROGRESS: at 39.68% examples, 74774 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:21,253, word2vec, INFO, EPOCH 7 - PROGRESS: at 41.50% examples, 74767 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:22,274, word2vec, INFO, EPOCH 7 - PROGRESS: at 43.64% examples, 75121 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:23,301, word2vec, INFO, EPOCH 7 - PROGRESS: at 45.61% examples, 75025 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:24,753, word2vec, INFO, EPOCH 7 - PROGRESS: at 47.85% examples, 74065 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:25,773, word2vec, INFO, EPOCH 7 - PROGRESS: at 50.02% examples, 74411 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:26,786, word2vec, INFO, EPOCH 7 - PROGRESS: at 51.95% examples, 74399 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:28,105, word2vec, INFO, EPOCH 7 - PROGRESS: at 54.10% examples, 73939 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:29,151, word2vec, INFO, EPOCH 7 - PROGRESS: at 56.24% examples, 74191 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:30,184, word2vec, INFO, EPOCH 7 - PROGRESS: at 58.36% examples, 74446 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:31,242, word2vec, INFO, EPOCH 7 - PROGRESS: at 60.76% examples, 74907 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:32,279, word2vec, INFO, EPOCH 7 - PROGRESS: at 62.69% examples, 74834 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:33,329, word2vec, INFO, EPOCH 7 - PROGRESS: at 64.90% examples, 75006 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:34,379, word2vec, INFO, EPOCH 7 - PROGRESS: at 66.80% examples, 74901 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:35,436, word2vec, INFO, EPOCH 7 - PROGRESS: at 68.89% examples, 75057 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:36,593, word2vec, INFO, EPOCH 7 - PROGRESS: at 70.99% examples, 74999 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:37,630, word2vec, INFO, EPOCH 7 - PROGRESS: at 72.92% examples, 74930 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:38,691, word2vec, INFO, EPOCH 7 - PROGRESS: at 74.76% examples, 74824 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:39,924, word2vec, INFO, EPOCH 7 - PROGRESS: at 76.87% examples, 74638 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:41,135, word2vec, INFO, EPOCH 7 - PROGRESS: at 79.01% examples, 74505 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:42,247, word2vec, INFO, EPOCH 7 - PROGRESS: at 81.07% examples, 74542 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:43,253, word2vec, INFO, EPOCH 7 - PROGRESS: at 82.99% examples, 74544 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:44,298, word2vec, INFO, EPOCH 7 - PROGRESS: at 85.16% examples, 74692 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:45,323, word2vec, INFO, EPOCH 7 - PROGRESS: at 87.14% examples, 74664 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:46,438, word2vec, INFO, EPOCH 7 - PROGRESS: at 89.60% examples, 74894 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:47,451, word2vec, INFO, EPOCH 7 - PROGRESS: at 91.55% examples, 74877 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:48,467, word2vec, INFO, EPOCH 7 - PROGRESS: at 93.72% examples, 75050 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:49,523, word2vec, INFO, EPOCH 7 - PROGRESS: at 95.87% examples, 75158 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:50,773, word2vec, INFO, EPOCH 7 - PROGRESS: at 98.05% examples, 74974 words/s, in_qsize 8, out_qsize 0 ]
[2024-12-11 18:28:51,548, word2vec, INFO, EPOCH 7: training on 4170053 raw words (3937584 effective words) took 52.3s, 75281 effective words/s ]
[2024-12-11 18:28:52,607, word2vec, INFO, EPOCH 8 - PROGRESS: at 0.23% examples, 8995 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:53,754, word2vec, INFO, EPOCH 8 - PROGRESS: at 2.12% examples, 38473 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:28:54,777, word2vec, INFO, EPOCH 8 - PROGRESS: at 4.01% examples, 49548 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:55,825, word2vec, INFO, EPOCH 8 - PROGRESS: at 5.93% examples, 54953 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:56,835, word2vec, INFO, EPOCH 8 - PROGRESS: at 8.22% examples, 62252 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:28:57,842, word2vec, INFO, EPOCH 8 - PROGRESS: at 9.87% examples, 62761 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:58,901, word2vec, INFO, EPOCH 8 - PROGRESS: at 12.06% examples, 65225 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:28:59,916, word2vec, INFO, EPOCH 8 - PROGRESS: at 14.04% examples, 66331 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:00,945, word2vec, INFO, EPOCH 8 - PROGRESS: at 16.46% examples, 69087 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:01,949, word2vec, INFO, EPOCH 8 - PROGRESS: at 18.35% examples, 69651 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:03,130, word2vec, INFO, EPOCH 8 - PROGRESS: at 21.00% examples, 71477 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:04,218, word2vec, INFO, EPOCH 8 - PROGRESS: at 22.87% examples, 71299 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:05,322, word2vec, INFO, EPOCH 8 - PROGRESS: at 24.74% examples, 71035 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:06,356, word2vec, INFO, EPOCH 8 - PROGRESS: at 26.61% examples, 71172 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:07,421, word2vec, INFO, EPOCH 8 - PROGRESS: at 28.51% examples, 71146 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:08,478, word2vec, INFO, EPOCH 8 - PROGRESS: at 30.41% examples, 71150 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:09,507, word2vec, INFO, EPOCH 8 - PROGRESS: at 32.35% examples, 71262 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:10,746, word2vec, INFO, EPOCH 8 - PROGRESS: at 34.44% examples, 71057 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:11,783, word2vec, INFO, EPOCH 8 - PROGRESS: at 36.35% examples, 71116 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:12,884, word2vec, INFO, EPOCH 8 - PROGRESS: at 38.27% examples, 70971 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:13,885, word2vec, INFO, EPOCH 8 - PROGRESS: at 40.14% examples, 71169 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:15,000, word2vec, INFO, EPOCH 8 - PROGRESS: at 42.21% examples, 71395 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:16,127, word2vec, INFO, EPOCH 8 - PROGRESS: at 44.37% examples, 71546 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:17,149, word2vec, INFO, EPOCH 8 - PROGRESS: at 46.39% examples, 71629 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:18,235, word2vec, INFO, EPOCH 8 - PROGRESS: at 48.82% examples, 72242 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:19,278, word2vec, INFO, EPOCH 8 - PROGRESS: at 50.79% examples, 72237 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:29:20,278, word2vec, INFO, EPOCH 8 - PROGRESS: at 52.68% examples, 72334 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:21,416, word2vec, INFO, EPOCH 8 - PROGRESS: at 55.03% examples, 72735 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:22,439, word2vec, INFO, EPOCH 8 - PROGRESS: at 56.90% examples, 72757 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:23,481, word2vec, INFO, EPOCH 8 - PROGRESS: at 58.86% examples, 72745 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:24,710, word2vec, INFO, EPOCH 8 - PROGRESS: at 61.02% examples, 72580 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:25,824, word2vec, INFO, EPOCH 8 - PROGRESS: at 63.20% examples, 72691 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 18:29:26,870, word2vec, INFO, EPOCH 8 - PROGRESS: at 65.38% examples, 72927 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:27,872, word2vec, INFO, EPOCH 8 - PROGRESS: at 67.28% examples, 72985 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:29,015, word2vec, INFO, EPOCH 8 - PROGRESS: at 69.34% examples, 73025 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:30,018, word2vec, INFO, EPOCH 8 - PROGRESS: at 71.21% examples, 73076 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:31,141, word2vec, INFO, EPOCH 8 - PROGRESS: at 73.60% examples, 73375 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:32,181, word2vec, INFO, EPOCH 8 - PROGRESS: at 75.44% examples, 73356 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:33,224, word2vec, INFO, EPOCH 8 - PROGRESS: at 77.32% examples, 73324 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:34,331, word2vec, INFO, EPOCH 8 - PROGRESS: at 79.46% examples, 73413 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:35,361, word2vec, INFO, EPOCH 8 - PROGRESS: at 81.54% examples, 73610 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:36,437, word2vec, INFO, EPOCH 8 - PROGRESS: at 83.44% examples, 73517 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:37,468, word2vec, INFO, EPOCH 8 - PROGRESS: at 85.65% examples, 73707 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:38,526, word2vec, INFO, EPOCH 8 - PROGRESS: at 87.84% examples, 73849 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:39,546, word2vec, INFO, EPOCH 8 - PROGRESS: at 89.84% examples, 73850 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:40,585, word2vec, INFO, EPOCH 8 - PROGRESS: at 91.97% examples, 74010 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:41,756, word2vec, INFO, EPOCH 8 - PROGRESS: at 94.15% examples, 73968 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:42,805, word2vec, INFO, EPOCH 8 - PROGRESS: at 96.13% examples, 73923 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:43,811, word2vec, INFO, EPOCH 8 - PROGRESS: at 98.05% examples, 73937 words/s, in_qsize 8, out_qsize 0 ]
[2024-12-11 18:29:44,557, word2vec, INFO, EPOCH 8: training on 4170053 raw words (3937646 effective words) took 53.0s, 74295 effective words/s ]
[2024-12-11 18:29:45,583, word2vec, INFO, EPOCH 9 - PROGRESS: at 1.88% examples, 73870 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:46,597, word2vec, INFO, EPOCH 9 - PROGRESS: at 3.31% examples, 64631 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:47,663, word2vec, INFO, EPOCH 9 - PROGRESS: at 5.66% examples, 72824 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:48,674, word2vec, INFO, EPOCH 9 - PROGRESS: at 7.50% examples, 73197 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:50,283, word2vec, INFO, EPOCH 9 - PROGRESS: at 9.68% examples, 67360 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:51,330, word2vec, INFO, EPOCH 9 - PROGRESS: at 12.06% examples, 70856 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:52,349, word2vec, INFO, EPOCH 9 - PROGRESS: at 14.05% examples, 71258 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:53,364, word2vec, INFO, EPOCH 9 - PROGRESS: at 15.98% examples, 71605 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:29:54,426, word2vec, INFO, EPOCH 9 - PROGRESS: at 18.35% examples, 73431 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:55,500, word2vec, INFO, EPOCH 9 - PROGRESS: at 20.53% examples, 73948 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:56,515, word2vec, INFO, EPOCH 9 - PROGRESS: at 22.39% examples, 73984 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:29:57,636, word2vec, INFO, EPOCH 9 - PROGRESS: at 24.49% examples, 74109 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:58,665, word2vec, INFO, EPOCH 9 - PROGRESS: at 26.36% examples, 74041 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:29:59,703, word2vec, INFO, EPOCH 9 - PROGRESS: at 28.29% examples, 73939 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:00,739, word2vec, INFO, EPOCH 9 - PROGRESS: at 30.41% examples, 74439 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:30:01,740, word2vec, INFO, EPOCH 9 - PROGRESS: at 32.35% examples, 74477 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:02,956, word2vec, INFO, EPOCH 9 - PROGRESS: at 34.44% examples, 74135 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:03,957, word2vec, INFO, EPOCH 9 - PROGRESS: at 36.35% examples, 74174 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:05,142, word2vec, INFO, EPOCH 9 - PROGRESS: at 38.50% examples, 74016 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:30:06,148, word2vec, INFO, EPOCH 9 - PROGRESS: at 40.60% examples, 74494 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:07,424, word2vec, INFO, EPOCH 9 - PROGRESS: at 42.91% examples, 74455 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:08,426, word2vec, INFO, EPOCH 9 - PROGRESS: at 44.87% examples, 74456 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:09,466, word2vec, INFO, EPOCH 9 - PROGRESS: at 46.90% examples, 74372 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:10,528, word2vec, INFO, EPOCH 9 - PROGRESS: at 49.06% examples, 74592 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:11,552, word2vec, INFO, EPOCH 9 - PROGRESS: at 51.00% examples, 74548 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:12,740, word2vec, INFO, EPOCH 9 - PROGRESS: at 53.64% examples, 75072 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:13,990, word2vec, INFO, EPOCH 9 - PROGRESS: at 55.74% examples, 74766 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:15,022, word2vec, INFO, EPOCH 9 - PROGRESS: at 58.12% examples, 75323 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:16,045, word2vec, INFO, EPOCH 9 - PROGRESS: at 60.08% examples, 75265 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:17,542, word2vec, INFO, EPOCH 9 - PROGRESS: at 62.22% examples, 74402 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:18,562, word2vec, INFO, EPOCH 9 - PROGRESS: at 64.67% examples, 74930 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:19,568, word2vec, INFO, EPOCH 9 - PROGRESS: at 66.56% examples, 74922 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 18:30:20,615, word2vec, INFO, EPOCH 9 - PROGRESS: at 68.42% examples, 74834 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:21,657, word2vec, INFO, EPOCH 9 - PROGRESS: at 70.54% examples, 75020 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:22,672, word2vec, INFO, EPOCH 9 - PROGRESS: at 72.44% examples, 74993 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:30:23,681, word2vec, INFO, EPOCH 9 - PROGRESS: at 74.32% examples, 74981 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:30:24,710, word2vec, INFO, EPOCH 9 - PROGRESS: at 76.13% examples, 74941 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:25,742, word2vec, INFO, EPOCH 9 - PROGRESS: at 78.06% examples, 74889 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:26,777, word2vec, INFO, EPOCH 9 - PROGRESS: at 79.94% examples, 74842 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:27,789, word2vec, INFO, EPOCH 9 - PROGRESS: at 81.77% examples, 74821 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:28,842, word2vec, INFO, EPOCH 9 - PROGRESS: at 83.93% examples, 74953 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:29,854, word2vec, INFO, EPOCH 9 - PROGRESS: at 85.90% examples, 74937 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:30,898, word2vec, INFO, EPOCH 9 - PROGRESS: at 88.11% examples, 75074 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 18:30:31,924, word2vec, INFO, EPOCH 9 - PROGRESS: at 89.84% examples, 74839 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:32,957, word2vec, INFO, EPOCH 9 - PROGRESS: at 92.48% examples, 75377 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:33,970, word2vec, INFO, EPOCH 9 - PROGRESS: at 93.91% examples, 74977 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 18:30:35,138, word2vec, INFO, EPOCH 9 - PROGRESS: at 96.57% examples, 75289 words/s, in_qsize 14, out_qsize 0 ]
[2024-12-11 18:30:36,330, word2vec, INFO, EPOCH 9 - PROGRESS: at 98.80% examples, 75190 words/s, in_qsize 5, out_qsize 1 ]
[2024-12-11 18:30:36,766, word2vec, INFO, EPOCH 9: training on 4170053 raw words (3938036 effective words) took 52.2s, 75443 effective words/s ]
[2024-12-11 18:30:36,769, utils, INFO, FastText lifecycle event {'msg': 'training on 41700530 raw words (39376105 effective words) took 532.6s, 73934 effective words/s', 'datetime': '2024-12-11T18:30:36.769018', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-11 18:30:50,233, utils, INFO, FastText lifecycle event {'params': 'FastText<vocab=201843, vector_size=500, alpha=0.025>', 'datetime': '2024-12-11T18:30:50.233635', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-11 18:30:50,242, utils, INFO, FastText lifecycle event {'fname_or_handle': 'fasttext_reviews.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-12-11T18:30:50.242192', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'} ]
[2024-12-11 18:30:50,253, utils, INFO, not storing attribute vectors ]
[2024-12-11 18:30:50,254, utils, INFO, storing np array 'vectors_vocab' to fasttext_reviews.model.wv.vectors_vocab.npy ]
[2024-12-11 18:30:51,500, utils, INFO, storing np array 'vectors_ngrams' to fasttext_reviews.model.wv.vectors_ngrams.npy ]
[2024-12-11 18:31:11,557, utils, INFO, not storing attribute buckets_word ]
[2024-12-11 18:31:11,559, utils, INFO, storing np array 'syn1neg' to fasttext_reviews.model.syn1neg.npy ]
[2024-12-11 18:31:14,260, utils, INFO, not storing attribute cum_table ]
[2024-12-11 18:31:14,393, utils, INFO, saved fasttext_reviews.model ]
[2024-12-11 20:32:20,364, utils, INFO, loading FastText object from fasttext_reviews.model ]
[2024-12-11 20:32:20,480, utils, INFO, loading wv recursively from fasttext_reviews.model.wv.* with mmap=None ]
[2024-12-11 20:32:20,481, utils, INFO, loading vectors_vocab from fasttext_reviews.model.wv.vectors_vocab.npy with mmap=None ]
[2024-12-11 20:32:21,538, utils, INFO, loading vectors_ngrams from fasttext_reviews.model.wv.vectors_ngrams.npy with mmap=None ]
[2024-12-11 20:32:52,712, utils, INFO, setting ignored attribute vectors to None ]
[2024-12-11 20:32:52,918, utils, INFO, setting ignored attribute buckets_word to None ]
[2024-12-11 20:33:07,930, utils, INFO, loading syn1neg from fasttext_reviews.model.syn1neg.npy with mmap=None ]
[2024-12-11 20:33:09,520, utils, INFO, setting ignored attribute cum_table to None ]
[2024-12-11 20:33:11,576, utils, INFO, FastText lifecycle event {'fname': 'fasttext_reviews.model', 'datetime': '2024-12-11T20:33:11.574555', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-11 21:09:36,931, word2vec, INFO, collecting all words and their counts ]
[2024-12-11 21:09:36,932, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-11 21:09:37,058, word2vec, INFO, PROGRESS: at sentence #10000, processed 417356 words, keeping 38269 word types ]
[2024-12-11 21:09:37,163, word2vec, INFO, PROGRESS: at sentence #20000, processed 827291 words, keeping 57463 word types ]
[2024-12-11 21:09:37,277, word2vec, INFO, PROGRESS: at sentence #30000, processed 1245251 words, keeping 73816 word types ]
[2024-12-11 21:09:37,400, word2vec, INFO, PROGRESS: at sentence #40000, processed 1659818 words, keeping 88317 word types ]
[2024-12-11 21:09:37,535, word2vec, INFO, PROGRESS: at sentence #50000, processed 2073395 words, keeping 101105 word types ]
[2024-12-11 21:09:37,637, word2vec, INFO, PROGRESS: at sentence #60000, processed 2485740 words, keeping 113234 word types ]
[2024-12-11 21:09:37,740, word2vec, INFO, PROGRESS: at sentence #70000, processed 2895180 words, keeping 124533 word types ]
[2024-12-11 21:09:37,843, word2vec, INFO, PROGRESS: at sentence #80000, processed 3316945 words, keeping 135881 word types ]
[2024-12-11 21:09:37,947, word2vec, INFO, PROGRESS: at sentence #90000, processed 3730627 words, keeping 146822 word types ]
[2024-12-11 21:09:38,084, word2vec, INFO, PROGRESS: at sentence #100000, processed 4136518 words, keeping 156920 word types ]
[2024-12-11 21:09:38,134, word2vec, INFO, collected 160129 word types from a corpus of 4268692 raw words and 103304 sentences ]
[2024-12-11 21:09:38,135, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-11 21:09:38,997, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 160129 unique words (100.00% of original 160129, drops 0)', 'datetime': '2024-12-11T21:09:38.997116', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:09:38,998, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4268692 word corpus (100.00% of original 4268692, drops 0)', 'datetime': '2024-12-11T21:09:38.998114', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:09:40,061, word2vec, INFO, deleting the raw counts dictionary of 160129 items ]
[2024-12-11 21:09:40,065, word2vec, INFO, sample=0.001 downsamples 31 most-common words ]
[2024-12-11 21:09:40,066, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 3978110.9017322585 word corpus (93.2%% of prior 4268692)', 'datetime': '2024-12-11T21:09:40.066152', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:09:43,359, fasttext, INFO, estimated required memory for 160129 words, 2000000 buckets and 500 dimensions: 4756101440 bytes ]
[2024-12-11 21:09:43,360, word2vec, INFO, resetting layer weights ]
[2024-12-11 21:11:34,454, word2vec, INFO, collecting all words and their counts ]
[2024-12-11 21:11:34,456, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-11 21:11:34,598, word2vec, INFO, PROGRESS: at sentence #10000, processed 417356 words, keeping 38269 word types ]
[2024-12-11 21:11:34,768, word2vec, INFO, PROGRESS: at sentence #20000, processed 827291 words, keeping 57463 word types ]
[2024-12-11 21:11:34,899, word2vec, INFO, PROGRESS: at sentence #30000, processed 1245251 words, keeping 73816 word types ]
[2024-12-11 21:11:35,012, word2vec, INFO, PROGRESS: at sentence #40000, processed 1659818 words, keeping 88317 word types ]
[2024-12-11 21:11:35,140, word2vec, INFO, PROGRESS: at sentence #50000, processed 2073395 words, keeping 101105 word types ]
[2024-12-11 21:11:35,249, word2vec, INFO, PROGRESS: at sentence #60000, processed 2485740 words, keeping 113234 word types ]
[2024-12-11 21:11:35,362, word2vec, INFO, PROGRESS: at sentence #70000, processed 2895180 words, keeping 124533 word types ]
[2024-12-11 21:11:35,480, word2vec, INFO, PROGRESS: at sentence #80000, processed 3316945 words, keeping 135881 word types ]
[2024-12-11 21:11:35,585, word2vec, INFO, PROGRESS: at sentence #90000, processed 3730627 words, keeping 146822 word types ]
[2024-12-11 21:11:35,697, word2vec, INFO, PROGRESS: at sentence #100000, processed 4136518 words, keeping 156920 word types ]
[2024-12-11 21:11:35,735, word2vec, INFO, collected 160129 word types from a corpus of 4268692 raw words and 103304 sentences ]
[2024-12-11 21:11:35,736, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-11 21:11:36,542, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 160129 unique words (100.00% of original 160129, drops 0)', 'datetime': '2024-12-11T21:11:36.542152', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:11:36,544, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4268692 word corpus (100.00% of original 4268692, drops 0)', 'datetime': '2024-12-11T21:11:36.544153', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:11:37,762, word2vec, INFO, deleting the raw counts dictionary of 160129 items ]
[2024-12-11 21:11:37,766, word2vec, INFO, sample=0.001 downsamples 31 most-common words ]
[2024-12-11 21:11:37,768, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 3978110.9017322585 word corpus (93.2%% of prior 4268692)', 'datetime': '2024-12-11T21:11:37.768872', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:11:41,443, fasttext, INFO, estimated required memory for 160129 words, 2000000 buckets and 500 dimensions: 4756101440 bytes ]
[2024-12-11 21:11:41,444, word2vec, INFO, resetting layer weights ]
[2024-12-11 21:12:02,179, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-11T21:12:02.179769', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]
[2024-12-11 21:12:02,181, utils, INFO, FastText lifecycle event {'msg': 'training model with 8 workers on 160129 vocabulary and 500 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-11T21:12:02.181727', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-11 21:19:35,726, word2vec, INFO, collecting all words and their counts ]
[2024-12-11 21:19:35,728, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-11 21:19:35,869, word2vec, INFO, PROGRESS: at sentence #10000, processed 417356 words, keeping 38269 word types ]
[2024-12-11 21:19:35,966, word2vec, INFO, PROGRESS: at sentence #20000, processed 827291 words, keeping 57463 word types ]
[2024-12-11 21:19:36,071, word2vec, INFO, PROGRESS: at sentence #30000, processed 1245251 words, keeping 73816 word types ]
[2024-12-11 21:19:36,201, word2vec, INFO, PROGRESS: at sentence #40000, processed 1659818 words, keeping 88317 word types ]
[2024-12-11 21:19:36,337, word2vec, INFO, PROGRESS: at sentence #50000, processed 2073395 words, keeping 101105 word types ]
[2024-12-11 21:19:36,469, word2vec, INFO, PROGRESS: at sentence #60000, processed 2485740 words, keeping 113234 word types ]
[2024-12-11 21:19:36,583, word2vec, INFO, PROGRESS: at sentence #70000, processed 2895180 words, keeping 124533 word types ]
[2024-12-11 21:19:36,700, word2vec, INFO, PROGRESS: at sentence #80000, processed 3316945 words, keeping 135881 word types ]
[2024-12-11 21:19:36,812, word2vec, INFO, PROGRESS: at sentence #90000, processed 3730627 words, keeping 146822 word types ]
[2024-12-11 21:19:36,924, word2vec, INFO, PROGRESS: at sentence #100000, processed 4136518 words, keeping 156920 word types ]
[2024-12-11 21:19:36,965, word2vec, INFO, collected 160129 word types from a corpus of 4268692 raw words and 103304 sentences ]
[2024-12-11 21:19:36,966, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-11 21:19:37,752, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 160129 unique words (100.00% of original 160129, drops 0)', 'datetime': '2024-12-11T21:19:37.752660', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:19:37,753, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4268692 word corpus (100.00% of original 4268692, drops 0)', 'datetime': '2024-12-11T21:19:37.753638', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:19:38,851, word2vec, INFO, deleting the raw counts dictionary of 160129 items ]
[2024-12-11 21:19:38,855, word2vec, INFO, sample=0.001 downsamples 31 most-common words ]
[2024-12-11 21:19:38,856, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 3978110.9017322585 word corpus (93.2%% of prior 4268692)', 'datetime': '2024-12-11T21:19:38.856374', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-11 21:19:42,025, fasttext, INFO, estimated required memory for 160129 words, 2000000 buckets and 500 dimensions: 4756101440 bytes ]
[2024-12-11 21:19:42,026, word2vec, INFO, resetting layer weights ]
[2024-12-11 21:20:00,402, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-11T21:20:00.402727', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]
[2024-12-11 21:20:00,409, utils, INFO, FastText lifecycle event {'msg': 'training model with 8 workers on 160129 vocabulary and 500 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-11T21:20:00.409141', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-11 21:20:01,742, word2vec, INFO, EPOCH 0 - PROGRESS: at 1.38% examples, 53602 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:02,737, word2vec, INFO, EPOCH 0 - PROGRESS: at 3.66% examples, 72670 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:04,218, word2vec, INFO, EPOCH 0 - PROGRESS: at 5.78% examples, 65777 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:05,230, word2vec, INFO, EPOCH 0 - PROGRESS: at 8.49% examples, 75756 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:06,261, word2vec, INFO, EPOCH 0 - PROGRESS: at 10.56% examples, 76716 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:07,467, word2vec, INFO, EPOCH 0 - PROGRESS: at 12.97% examples, 76774 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:08,558, word2vec, INFO, EPOCH 0 - PROGRESS: at 15.16% examples, 76741 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:09,589, word2vec, INFO, EPOCH 0 - PROGRESS: at 17.71% examples, 79337 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:10,666, word2vec, INFO, EPOCH 0 - PROGRESS: at 19.82% examples, 79132 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:11,831, word2vec, INFO, EPOCH 0 - PROGRESS: at 21.87% examples, 78379 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:12,841, word2vec, INFO, EPOCH 0 - PROGRESS: at 24.16% examples, 79492 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:13,853, word2vec, INFO, EPOCH 0 - PROGRESS: at 26.19% examples, 79660 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:15,120, word2vec, INFO, EPOCH 0 - PROGRESS: at 28.76% examples, 79736 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:16,235, word2vec, INFO, EPOCH 0 - PROGRESS: at 31.03% examples, 79980 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:17,246, word2vec, INFO, EPOCH 0 - PROGRESS: at 33.18% examples, 80113 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:18,375, word2vec, INFO, EPOCH 0 - PROGRESS: at 35.48% examples, 80263 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:19,483, word2vec, INFO, EPOCH 0 - PROGRESS: at 37.53% examples, 79969 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:20,509, word2vec, INFO, EPOCH 0 - PROGRESS: at 39.89% examples, 80512 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:21,780, word2vec, INFO, EPOCH 0 - PROGRESS: at 42.34% examples, 80502 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:22,839, word2vec, INFO, EPOCH 0 - PROGRESS: at 44.48% examples, 80422 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 21:20:24,034, word2vec, INFO, EPOCH 0 - PROGRESS: at 47.18% examples, 80662 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:25,069, word2vec, INFO, EPOCH 0 - PROGRESS: at 49.30% examples, 80671 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:26,208, word2vec, INFO, EPOCH 0 - PROGRESS: at 51.39% examples, 80344 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:27,259, word2vec, INFO, EPOCH 0 - PROGRESS: at 53.95% examples, 81008 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:28,425, word2vec, INFO, EPOCH 0 - PROGRESS: at 56.30% examples, 80948 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:29,740, word2vec, INFO, EPOCH 0 - PROGRESS: at 58.59% examples, 80466 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:30,808, word2vec, INFO, EPOCH 0 - PROGRESS: at 61.19% examples, 81007 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:31,856, word2vec, INFO, EPOCH 0 - PROGRESS: at 63.33% examples, 80950 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:32,965, word2vec, INFO, EPOCH 0 - PROGRESS: at 65.47% examples, 80752 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:34,025, word2vec, INFO, EPOCH 0 - PROGRESS: at 67.52% examples, 80696 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:35,059, word2vec, INFO, EPOCH 0 - PROGRESS: at 69.78% examples, 80969 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:36,115, word2vec, INFO, EPOCH 0 - PROGRESS: at 71.88% examples, 80916 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:37,326, word2vec, INFO, EPOCH 0 - PROGRESS: at 74.21% examples, 80774 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:38,340, word2vec, INFO, EPOCH 0 - PROGRESS: at 76.23% examples, 80817 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:39,434, word2vec, INFO, EPOCH 0 - PROGRESS: at 78.54% examples, 80931 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:40,794, word2vec, INFO, EPOCH 0 - PROGRESS: at 81.02% examples, 80736 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:41,819, word2vec, INFO, EPOCH 0 - PROGRESS: at 83.34% examples, 80984 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:42,847, word2vec, INFO, EPOCH 0 - PROGRESS: at 85.50% examples, 80984 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:43,878, word2vec, INFO, EPOCH 0 - PROGRESS: at 87.40% examples, 80768 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:44,946, word2vec, INFO, EPOCH 0 - PROGRESS: at 89.37% examples, 80495 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:46,023, word2vec, INFO, EPOCH 0 - PROGRESS: at 91.45% examples, 80418 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:47,082, word2vec, INFO, EPOCH 0 - PROGRESS: at 93.83% examples, 80575 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:48,158, word2vec, INFO, EPOCH 0 - PROGRESS: at 95.97% examples, 80512 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:49,161, word2vec, INFO, EPOCH 0 - PROGRESS: at 98.09% examples, 80571 words/s, in_qsize 8, out_qsize 0 ]
[2024-12-11 21:20:49,783, word2vec, INFO, EPOCH 0: training on 4268692 raw words (3978540 effective words) took 49.1s, 81058 effective words/s ]
[2024-12-11 21:20:50,839, word2vec, INFO, EPOCH 1 - PROGRESS: at 1.56% examples, 62008 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:51,862, word2vec, INFO, EPOCH 1 - PROGRESS: at 3.66% examples, 71571 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:53,343, word2vec, INFO, EPOCH 1 - PROGRESS: at 5.74% examples, 65201 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:54,504, word2vec, INFO, EPOCH 1 - PROGRESS: at 8.49% examples, 72829 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:55,560, word2vec, INFO, EPOCH 1 - PROGRESS: at 10.78% examples, 75572 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:20:56,566, word2vec, INFO, EPOCH 1 - PROGRESS: at 12.97% examples, 76677 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:57,822, word2vec, INFO, EPOCH 1 - PROGRESS: at 15.16% examples, 75082 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:20:58,913, word2vec, INFO, EPOCH 1 - PROGRESS: at 16.98% examples, 74245 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:00,014, word2vec, INFO, EPOCH 1 - PROGRESS: at 18.88% examples, 73508 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:01,074, word2vec, INFO, EPOCH 1 - PROGRESS: at 20.73% examples, 73174 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:02,309, word2vec, INFO, EPOCH 1 - PROGRESS: at 22.80% examples, 72652 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:03,339, word2vec, INFO, EPOCH 1 - PROGRESS: at 24.87% examples, 73269 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:04,373, word2vec, INFO, EPOCH 1 - PROGRESS: at 26.89% examples, 73740 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:05,561, word2vec, INFO, EPOCH 1 - PROGRESS: at 28.99% examples, 73488 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:06,734, word2vec, INFO, EPOCH 1 - PROGRESS: at 30.82% examples, 72773 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:07,768, word2vec, INFO, EPOCH 1 - PROGRESS: at 33.18% examples, 73728 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:08,857, word2vec, INFO, EPOCH 1 - PROGRESS: at 35.00% examples, 73421 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:09,993, word2vec, INFO, EPOCH 1 - PROGRESS: at 37.09% examples, 73417 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:11,055, word2vec, INFO, EPOCH 1 - PROGRESS: at 38.98% examples, 73244 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:12,324, word2vec, INFO, EPOCH 1 - PROGRESS: at 40.96% examples, 72815 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:13,457, word2vec, INFO, EPOCH 1 - PROGRESS: at 43.04% examples, 72868 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:14,686, word2vec, INFO, EPOCH 1 - PROGRESS: at 45.19% examples, 72614 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:15,770, word2vec, INFO, EPOCH 1 - PROGRESS: at 47.18% examples, 72428 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:16,932, word2vec, INFO, EPOCH 1 - PROGRESS: at 49.25% examples, 72412 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:18,135, word2vec, INFO, EPOCH 1 - PROGRESS: at 51.39% examples, 72282 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:19,256, word2vec, INFO, EPOCH 1 - PROGRESS: at 53.29% examples, 72050 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:21:20,292, word2vec, INFO, EPOCH 1 - PROGRESS: at 55.10% examples, 72033 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:21,359, word2vec, INFO, EPOCH 1 - PROGRESS: at 56.94% examples, 71951 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:22,431, word2vec, INFO, EPOCH 1 - PROGRESS: at 59.09% examples, 72136 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:23,464, word2vec, INFO, EPOCH 1 - PROGRESS: at 60.94% examples, 72128 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:24,629, word2vec, INFO, EPOCH 1 - PROGRESS: at 63.08% examples, 72107 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:25,826, word2vec, INFO, EPOCH 1 - PROGRESS: at 65.23% examples, 72024 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:26,845, word2vec, INFO, EPOCH 1 - PROGRESS: at 67.08% examples, 72053 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:28,128, word2vec, INFO, EPOCH 1 - PROGRESS: at 69.11% examples, 71825 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:29,282, word2vec, INFO, EPOCH 1 - PROGRESS: at 70.93% examples, 71609 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:30,341, word2vec, INFO, EPOCH 1 - PROGRESS: at 73.07% examples, 71800 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:31,552, word2vec, INFO, EPOCH 1 - PROGRESS: at 75.07% examples, 71722 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:32,564, word2vec, INFO, EPOCH 1 - PROGRESS: at 77.13% examples, 71970 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:33,579, word2vec, INFO, EPOCH 1 - PROGRESS: at 79.20% examples, 72214 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:34,686, word2vec, INFO, EPOCH 1 - PROGRESS: at 81.24% examples, 72294 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:35,823, word2vec, INFO, EPOCH 1 - PROGRESS: at 83.12% examples, 72120 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:36,825, word2vec, INFO, EPOCH 1 - PROGRESS: at 85.01% examples, 72157 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:37,931, word2vec, INFO, EPOCH 1 - PROGRESS: at 87.15% examples, 72233 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:39,168, word2vec, INFO, EPOCH 1 - PROGRESS: at 89.59% examples, 72304 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:40,257, word2vec, INFO, EPOCH 1 - PROGRESS: at 91.88% examples, 72567 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:41,391, word2vec, INFO, EPOCH 1 - PROGRESS: at 94.24% examples, 72773 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:42,442, word2vec, INFO, EPOCH 1 - PROGRESS: at 96.64% examples, 73092 words/s, in_qsize 14, out_qsize 0 ]
[2024-12-11 21:21:43,477, word2vec, INFO, EPOCH 1 - PROGRESS: at 98.81% examples, 73238 words/s, in_qsize 5, out_qsize 1 ]
[2024-12-11 21:21:43,941, word2vec, INFO, EPOCH 1: training on 4268692 raw words (3977767 effective words) took 54.1s, 73461 effective words/s ]
[2024-12-11 21:21:45,222, word2vec, INFO, EPOCH 2 - PROGRESS: at 0.87% examples, 29543 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:46,392, word2vec, INFO, EPOCH 2 - PROGRESS: at 2.75% examples, 45653 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:47,456, word2vec, INFO, EPOCH 2 - PROGRESS: at 4.64% examples, 52957 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:48,514, word2vec, INFO, EPOCH 2 - PROGRESS: at 6.46% examples, 56893 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:49,551, word2vec, INFO, EPOCH 2 - PROGRESS: at 8.50% examples, 61331 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 21:21:50,571, word2vec, INFO, EPOCH 2 - PROGRESS: at 10.78% examples, 65919 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:51,668, word2vec, INFO, EPOCH 2 - PROGRESS: at 12.25% examples, 63754 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:52,689, word2vec, INFO, EPOCH 2 - PROGRESS: at 14.71% examples, 66956 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:53,693, word2vec, INFO, EPOCH 2 - PROGRESS: at 16.53% examples, 67669 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:54,915, word2vec, INFO, EPOCH 2 - PROGRESS: at 18.39% examples, 66896 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:21:56,022, word2vec, INFO, EPOCH 2 - PROGRESS: at 20.06% examples, 66139 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:57,074, word2vec, INFO, EPOCH 2 - PROGRESS: at 21.40% examples, 65086 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:58,088, word2vec, INFO, EPOCH 2 - PROGRESS: at 23.51% examples, 66320 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:21:59,144, word2vec, INFO, EPOCH 2 - PROGRESS: at 25.96% examples, 68393 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:00,153, word2vec, INFO, EPOCH 2 - PROGRESS: at 27.85% examples, 68698 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:01,160, word2vec, INFO, EPOCH 2 - PROGRESS: at 29.93% examples, 69532 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:02,323, word2vec, INFO, EPOCH 2 - PROGRESS: at 32.00% examples, 69652 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:03,444, word2vec, INFO, EPOCH 2 - PROGRESS: at 34.10% examples, 69933 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:04,548, word2vec, INFO, EPOCH 2 - PROGRESS: at 36.40% examples, 70689 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:05,631, word2vec, INFO, EPOCH 2 - PROGRESS: at 38.52% examples, 71010 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:06,662, word2vec, INFO, EPOCH 2 - PROGRESS: at 40.50% examples, 71460 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:07,767, word2vec, INFO, EPOCH 2 - PROGRESS: at 42.59% examples, 71654 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:08,861, word2vec, INFO, EPOCH 2 - PROGRESS: at 44.73% examples, 71855 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:09,899, word2vec, INFO, EPOCH 2 - PROGRESS: at 46.94% examples, 72192 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:11,011, word2vec, INFO, EPOCH 2 - PROGRESS: at 49.05% examples, 72311 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:12,190, word2vec, INFO, EPOCH 2 - PROGRESS: at 51.39% examples, 72578 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:13,327, word2vec, INFO, EPOCH 2 - PROGRESS: at 53.51% examples, 72608 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:14,422, word2vec, INFO, EPOCH 2 - PROGRESS: at 55.78% examples, 73048 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:15,560, word2vec, INFO, EPOCH 2 - PROGRESS: at 58.12% examples, 73341 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:16,654, word2vec, INFO, EPOCH 2 - PROGRESS: at 60.25% examples, 73448 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:17,687, word2vec, INFO, EPOCH 2 - PROGRESS: at 62.35% examples, 73669 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:18,763, word2vec, INFO, EPOCH 2 - PROGRESS: at 64.30% examples, 73519 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:19,784, word2vec, INFO, EPOCH 2 - PROGRESS: at 66.16% examples, 73494 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:20,929, word2vec, INFO, EPOCH 2 - PROGRESS: at 68.44% examples, 73729 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 21:22:21,965, word2vec, INFO, EPOCH 2 - PROGRESS: at 70.69% examples, 74165 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:23,029, word2vec, INFO, EPOCH 2 - PROGRESS: at 72.85% examples, 74286 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:24,098, word2vec, INFO, EPOCH 2 - PROGRESS: at 74.85% examples, 74387 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:25,173, word2vec, INFO, EPOCH 2 - PROGRESS: at 76.90% examples, 74463 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:26,217, word2vec, INFO, EPOCH 2 - PROGRESS: at 79.00% examples, 74603 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:27,393, word2vec, INFO, EPOCH 2 - PROGRESS: at 81.24% examples, 74722 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:28,769, word2vec, INFO, EPOCH 2 - PROGRESS: at 83.57% examples, 74498 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:29,984, word2vec, INFO, EPOCH 2 - PROGRESS: at 86.23% examples, 74744 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:31,031, word2vec, INFO, EPOCH 2 - PROGRESS: at 88.39% examples, 74859 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:32,051, word2vec, INFO, EPOCH 2 - PROGRESS: at 90.83% examples, 75189 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:33,214, word2vec, INFO, EPOCH 2 - PROGRESS: at 92.83% examples, 75100 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:34,287, word2vec, INFO, EPOCH 2 - PROGRESS: at 94.97% examples, 75164 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:35,312, word2vec, INFO, EPOCH 2 - PROGRESS: at 97.09% examples, 75292 words/s, in_qsize 12, out_qsize 0 ]
[2024-12-11 21:22:36,418, word2vec, INFO, EPOCH 2 - PROGRESS: at 99.77% examples, 75660 words/s, in_qsize 1, out_qsize 1 ]
[2024-12-11 21:22:36,444, word2vec, INFO, EPOCH 2: training on 4268692 raw words (3977695 effective words) took 52.5s, 75790 effective words/s ]
[2024-12-11 21:22:37,488, word2vec, INFO, EPOCH 3 - PROGRESS: at 1.39% examples, 53639 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:38,500, word2vec, INFO, EPOCH 3 - PROGRESS: at 3.66% examples, 72306 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:40,090, word2vec, INFO, EPOCH 3 - PROGRESS: at 5.74% examples, 63660 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:41,236, word2vec, INFO, EPOCH 3 - PROGRESS: at 8.26% examples, 69789 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:42,264, word2vec, INFO, EPOCH 3 - PROGRESS: at 10.32% examples, 71852 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:43,377, word2vec, INFO, EPOCH 3 - PROGRESS: at 12.25% examples, 71011 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:44,460, word2vec, INFO, EPOCH 3 - PROGRESS: at 14.43% examples, 71839 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:45,516, word2vec, INFO, EPOCH 3 - PROGRESS: at 16.54% examples, 72677 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:46,595, word2vec, INFO, EPOCH 3 - PROGRESS: at 18.62% examples, 73191 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:47,705, word2vec, INFO, EPOCH 3 - PROGRESS: at 20.75% examples, 73383 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:48,722, word2vec, INFO, EPOCH 3 - PROGRESS: at 23.04% examples, 74887 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:49,780, word2vec, INFO, EPOCH 3 - PROGRESS: at 24.87% examples, 74512 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:50,823, word2vec, INFO, EPOCH 3 - PROGRESS: at 26.69% examples, 74206 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:51,832, word2vec, INFO, EPOCH 3 - PROGRESS: at 28.52% examples, 74166 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:52,843, word2vec, INFO, EPOCH 3 - PROGRESS: at 30.59% examples, 74690 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:53,868, word2vec, INFO, EPOCH 3 - PROGRESS: at 32.69% examples, 75067 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:54,897, word2vec, INFO, EPOCH 3 - PROGRESS: at 35.02% examples, 75914 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:55,958, word2vec, INFO, EPOCH 3 - PROGRESS: at 37.09% examples, 76065 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:22:57,381, word2vec, INFO, EPOCH 3 - PROGRESS: at 39.18% examples, 74889 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:58,497, word2vec, INFO, EPOCH 3 - PROGRESS: at 41.45% examples, 75304 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:22:59,574, word2vec, INFO, EPOCH 3 - PROGRESS: at 44.02% examples, 76222 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:00,817, word2vec, INFO, EPOCH 3 - PROGRESS: at 46.20% examples, 75736 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:01,826, word2vec, INFO, EPOCH 3 - PROGRESS: at 48.11% examples, 75653 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:02,843, word2vec, INFO, EPOCH 3 - PROGRESS: at 50.48% examples, 76248 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:03,875, word2vec, INFO, EPOCH 3 - PROGRESS: at 52.33% examples, 76084 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:04,887, word2vec, INFO, EPOCH 3 - PROGRESS: at 54.43% examples, 76319 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:05,890, word2vec, INFO, EPOCH 3 - PROGRESS: at 56.52% examples, 76560 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:06,946, word2vec, INFO, EPOCH 3 - PROGRESS: at 58.35% examples, 76331 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:08,139, word2vec, INFO, EPOCH 3 - PROGRESS: at 60.73% examples, 76387 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:09,354, word2vec, INFO, EPOCH 3 - PROGRESS: at 63.08% examples, 76378 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:10,405, word2vec, INFO, EPOCH 3 - PROGRESS: at 64.99% examples, 76194 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:11,449, word2vec, INFO, EPOCH 3 - PROGRESS: at 67.31% examples, 76578 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:12,498, word2vec, INFO, EPOCH 3 - PROGRESS: at 69.11% examples, 76412 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:13,654, word2vec, INFO, EPOCH 3 - PROGRESS: at 71.16% examples, 76285 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:14,666, word2vec, INFO, EPOCH 3 - PROGRESS: at 73.29% examples, 76446 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:15,910, word2vec, INFO, EPOCH 3 - PROGRESS: at 75.30% examples, 76159 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:16,978, word2vec, INFO, EPOCH 3 - PROGRESS: at 77.57% examples, 76431 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:18,018, word2vec, INFO, EPOCH 3 - PROGRESS: at 79.43% examples, 76307 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:19,105, word2vec, INFO, EPOCH 3 - PROGRESS: at 81.49% examples, 76320 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:20,273, word2vec, INFO, EPOCH 3 - PROGRESS: at 83.81% examples, 76406 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:21,374, word2vec, INFO, EPOCH 3 - PROGRESS: at 86.00% examples, 76389 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:23:22,385, word2vec, INFO, EPOCH 3 - PROGRESS: at 88.39% examples, 76729 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:23,428, word2vec, INFO, EPOCH 3 - PROGRESS: at 90.34% examples, 76605 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:24,456, word2vec, INFO, EPOCH 3 - PROGRESS: at 92.14% examples, 76489 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:25,468, word2vec, INFO, EPOCH 3 - PROGRESS: at 94.24% examples, 76615 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:26,478, word2vec, INFO, EPOCH 3 - PROGRESS: at 96.20% examples, 76559 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:27,648, word2vec, INFO, EPOCH 3 - PROGRESS: at 98.58% examples, 76624 words/s, in_qsize 6, out_qsize 1 ]
[2024-12-11 21:23:28,059, word2vec, INFO, EPOCH 3: training on 4268692 raw words (3978163 effective words) took 51.6s, 77086 effective words/s ]
[2024-12-11 21:23:29,078, word2vec, INFO, EPOCH 4 - PROGRESS: at 1.38% examples, 55122 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:30,088, word2vec, INFO, EPOCH 4 - PROGRESS: at 3.20% examples, 64174 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:31,179, word2vec, INFO, EPOCH 4 - PROGRESS: at 5.07% examples, 65550 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:32,268, word2vec, INFO, EPOCH 4 - PROGRESS: at 7.10% examples, 68470 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:33,292, word2vec, INFO, EPOCH 4 - PROGRESS: at 9.22% examples, 71062 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:34,383, word2vec, INFO, EPOCH 4 - PROGRESS: at 11.03% examples, 70529 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:35,422, word2vec, INFO, EPOCH 4 - PROGRESS: at 13.19% examples, 71913 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:36,525, word2vec, INFO, EPOCH 4 - PROGRESS: at 15.40% examples, 72433 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:37,607, word2vec, INFO, EPOCH 4 - PROGRESS: at 17.71% examples, 73939 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:39,010, word2vec, INFO, EPOCH 4 - PROGRESS: at 19.82% examples, 72072 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:40,017, word2vec, INFO, EPOCH 4 - PROGRESS: at 21.64% examples, 72244 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:41,033, word2vec, INFO, EPOCH 4 - PROGRESS: at 23.47% examples, 72289 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:42,111, word2vec, INFO, EPOCH 4 - PROGRESS: at 25.57% examples, 72687 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:43,120, word2vec, INFO, EPOCH 4 - PROGRESS: at 27.60% examples, 73318 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:44,124, word2vec, INFO, EPOCH 4 - PROGRESS: at 29.95% examples, 74494 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:45,265, word2vec, INFO, EPOCH 4 - PROGRESS: at 32.05% examples, 74401 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:46,572, word2vec, INFO, EPOCH 4 - PROGRESS: at 34.33% examples, 74155 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:47,604, word2vec, INFO, EPOCH 4 - PROGRESS: at 36.17% examples, 74026 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:48,723, word2vec, INFO, EPOCH 4 - PROGRESS: at 38.51% examples, 74512 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:49,745, word2vec, INFO, EPOCH 4 - PROGRESS: at 40.50% examples, 74852 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:50,762, word2vec, INFO, EPOCH 4 - PROGRESS: at 42.59% examples, 75178 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:51,811, word2vec, INFO, EPOCH 4 - PROGRESS: at 44.73% examples, 75365 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:53,139, word2vec, INFO, EPOCH 4 - PROGRESS: at 47.16% examples, 75060 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:54,164, word2vec, INFO, EPOCH 4 - PROGRESS: at 49.05% examples, 74961 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:55,451, word2vec, INFO, EPOCH 4 - PROGRESS: at 51.16% examples, 74482 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:56,722, word2vec, INFO, EPOCH 4 - PROGRESS: at 53.29% examples, 74091 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:57,800, word2vec, INFO, EPOCH 4 - PROGRESS: at 55.56% examples, 74527 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:23:58,924, word2vec, INFO, EPOCH 4 - PROGRESS: at 57.64% examples, 74515 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:23:59,997, word2vec, INFO, EPOCH 4 - PROGRESS: at 60.03% examples, 74913 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:01,131, word2vec, INFO, EPOCH 4 - PROGRESS: at 62.35% examples, 75148 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:02,211, word2vec, INFO, EPOCH 4 - PROGRESS: at 64.30% examples, 74937 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:03,301, word2vec, INFO, EPOCH 4 - PROGRESS: at 66.12% examples, 74727 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:04,309, word2vec, INFO, EPOCH 4 - PROGRESS: at 68.19% examples, 74956 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:05,310, word2vec, INFO, EPOCH 4 - PROGRESS: at 70.02% examples, 74938 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:06,403, word2vec, INFO, EPOCH 4 - PROGRESS: at 72.11% examples, 74977 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:07,463, word2vec, INFO, EPOCH 4 - PROGRESS: at 73.97% examples, 74850 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:08,470, word2vec, INFO, EPOCH 4 - PROGRESS: at 75.75% examples, 74830 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:09,722, word2vec, INFO, EPOCH 4 - PROGRESS: at 78.07% examples, 74795 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:10,726, word2vec, INFO, EPOCH 4 - PROGRESS: at 79.90% examples, 74781 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:24:11,746, word2vec, INFO, EPOCH 4 - PROGRESS: at 82.16% examples, 75159 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:12,958, word2vec, INFO, EPOCH 4 - PROGRESS: at 84.29% examples, 74990 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:14,074, word2vec, INFO, EPOCH 4 - PROGRESS: at 86.47% examples, 74984 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:15,359, word2vec, INFO, EPOCH 4 - PROGRESS: at 88.86% examples, 74907 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:16,386, word2vec, INFO, EPOCH 4 - PROGRESS: at 91.04% examples, 75035 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:17,411, word2vec, INFO, EPOCH 4 - PROGRESS: at 92.83% examples, 74972 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:18,436, word2vec, INFO, EPOCH 4 - PROGRESS: at 94.72% examples, 74922 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:19,493, word2vec, INFO, EPOCH 4 - PROGRESS: at 96.86% examples, 75007 words/s, in_qsize 13, out_qsize 0 ]
[2024-12-11 21:24:20,582, word2vec, INFO, EPOCH 4 - PROGRESS: at 99.03% examples, 75044 words/s, in_qsize 4, out_qsize 1 ]
[2024-12-11 21:24:20,767, word2vec, INFO, EPOCH 4: training on 4268692 raw words (3977347 effective words) took 52.7s, 75478 effective words/s ]
[2024-12-11 21:24:21,833, word2vec, INFO, EPOCH 5 - PROGRESS: at 1.39% examples, 52570 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 21:24:22,836, word2vec, INFO, EPOCH 5 - PROGRESS: at 3.43% examples, 67356 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:23,909, word2vec, INFO, EPOCH 5 - PROGRESS: at 5.52% examples, 70954 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:24,919, word2vec, INFO, EPOCH 5 - PROGRESS: at 7.13% examples, 69342 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:25,940, word2vec, INFO, EPOCH 5 - PROGRESS: at 8.75% examples, 68257 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:26,945, word2vec, INFO, EPOCH 5 - PROGRESS: at 10.56% examples, 69158 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:27,945, word2vec, INFO, EPOCH 5 - PROGRESS: at 12.97% examples, 72452 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:29,549, word2vec, INFO, EPOCH 5 - PROGRESS: at 15.16% examples, 68717 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:30,764, word2vec, INFO, EPOCH 5 - PROGRESS: at 17.45% examples, 69659 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:31,777, word2vec, INFO, EPOCH 5 - PROGRESS: at 19.37% examples, 69987 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:32,788, word2vec, INFO, EPOCH 5 - PROGRESS: at 21.19% examples, 70285 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:33,805, word2vec, INFO, EPOCH 5 - PROGRESS: at 23.49% examples, 71935 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:24:34,844, word2vec, INFO, EPOCH 5 - PROGRESS: at 25.53% examples, 72546 words/s, in_qsize 15, out_qsize 1 ]
[2024-12-11 21:24:35,881, word2vec, INFO, EPOCH 5 - PROGRESS: at 27.37% examples, 72430 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:36,945, word2vec, INFO, EPOCH 5 - PROGRESS: at 29.47% examples, 72825 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:37,977, word2vec, INFO, EPOCH 5 - PROGRESS: at 31.32% examples, 72788 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:38,988, word2vec, INFO, EPOCH 5 - PROGRESS: at 33.18% examples, 72794 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:40,039, word2vec, INFO, EPOCH 5 - PROGRESS: at 34.99% examples, 72684 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:41,143, word2vec, INFO, EPOCH 5 - PROGRESS: at 37.09% examples, 72841 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:42,228, word2vec, INFO, EPOCH 5 - PROGRESS: at 39.67% examples, 73914 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:43,246, word2vec, INFO, EPOCH 5 - PROGRESS: at 41.42% examples, 73866 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:44,280, word2vec, INFO, EPOCH 5 - PROGRESS: at 43.28% examples, 73788 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:45,581, word2vec, INFO, EPOCH 5 - PROGRESS: at 45.45% examples, 73260 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:46,635, word2vec, INFO, EPOCH 5 - PROGRESS: at 47.86% examples, 73859 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:24:47,647, word2vec, INFO, EPOCH 5 - PROGRESS: at 50.00% examples, 74185 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:48,911, word2vec, INFO, EPOCH 5 - PROGRESS: at 52.10% examples, 73819 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:49,985, word2vec, INFO, EPOCH 5 - PROGRESS: at 53.95% examples, 73649 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:51,044, word2vec, INFO, EPOCH 5 - PROGRESS: at 55.82% examples, 73519 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:24:52,064, word2vec, INFO, EPOCH 5 - PROGRESS: at 58.35% examples, 74383 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:53,127, word2vec, INFO, EPOCH 5 - PROGRESS: at 60.73% examples, 74805 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:54,394, word2vec, INFO, EPOCH 5 - PROGRESS: at 62.82% examples, 74464 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:24:55,412, word2vec, INFO, EPOCH 5 - PROGRESS: at 65.01% examples, 74676 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:56,426, word2vec, INFO, EPOCH 5 - PROGRESS: at 66.84% examples, 74643 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:57,445, word2vec, INFO, EPOCH 5 - PROGRESS: at 68.44% examples, 74341 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:58,503, word2vec, INFO, EPOCH 5 - PROGRESS: at 70.26% examples, 74224 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:24:59,591, word2vec, INFO, EPOCH 5 - PROGRESS: at 72.58% examples, 74541 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:00,650, word2vec, INFO, EPOCH 5 - PROGRESS: at 75.08% examples, 75125 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:01,714, word2vec, INFO, EPOCH 5 - PROGRESS: at 77.13% examples, 75204 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:02,779, word2vec, INFO, EPOCH 5 - PROGRESS: at 79.00% examples, 75068 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:03,890, word2vec, INFO, EPOCH 5 - PROGRESS: at 80.80% examples, 74857 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:04,971, word2vec, INFO, EPOCH 5 - PROGRESS: at 82.67% examples, 74703 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:06,246, word2vec, INFO, EPOCH 5 - PROGRESS: at 84.77% examples, 74442 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:07,302, word2vec, INFO, EPOCH 5 - PROGRESS: at 86.94% examples, 74548 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:08,359, word2vec, INFO, EPOCH 5 - PROGRESS: at 89.09% examples, 74646 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:09,386, word2vec, INFO, EPOCH 5 - PROGRESS: at 91.24% examples, 74780 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:10,451, word2vec, INFO, EPOCH 5 - PROGRESS: at 93.59% examples, 75036 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:11,639, word2vec, INFO, EPOCH 5 - PROGRESS: at 95.46% examples, 74748 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:25:12,639, word2vec, INFO, EPOCH 5 - PROGRESS: at 97.34% examples, 74738 words/s, in_qsize 11, out_qsize 0 ]
[2024-12-11 21:25:13,493, word2vec, INFO, EPOCH 5: training on 4268692 raw words (3977879 effective words) took 52.7s, 75462 effective words/s ]
[2024-12-11 21:25:14,650, word2vec, INFO, EPOCH 6 - PROGRESS: at 0.90% examples, 32158 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:15,724, word2vec, INFO, EPOCH 6 - PROGRESS: at 3.43% examples, 62448 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:16,758, word2vec, INFO, EPOCH 6 - PROGRESS: at 5.52% examples, 68284 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:17,768, word2vec, INFO, EPOCH 6 - PROGRESS: at 7.30% examples, 69541 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:19,092, word2vec, INFO, EPOCH 6 - PROGRESS: at 9.69% examples, 69711 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:20,341, word2vec, INFO, EPOCH 6 - PROGRESS: at 12.00% examples, 70544 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:21,421, word2vec, INFO, EPOCH 6 - PROGRESS: at 14.43% examples, 72636 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:22,604, word2vec, INFO, EPOCH 6 - PROGRESS: at 16.54% examples, 72368 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:23,823, word2vec, INFO, EPOCH 6 - PROGRESS: at 18.62% examples, 71928 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:24,832, word2vec, INFO, EPOCH 6 - PROGRESS: at 20.75% examples, 72868 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:25,857, word2vec, INFO, EPOCH 6 - PROGRESS: at 22.80% examples, 73621 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:26,874, word2vec, INFO, EPOCH 6 - PROGRESS: at 24.64% examples, 73544 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:27,955, word2vec, INFO, EPOCH 6 - PROGRESS: at 26.93% examples, 74410 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:29,093, word2vec, INFO, EPOCH 6 - PROGRESS: at 28.99% examples, 74344 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:30,202, word2vec, INFO, EPOCH 6 - PROGRESS: at 31.03% examples, 74401 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:31,243, word2vec, INFO, EPOCH 6 - PROGRESS: at 33.18% examples, 74725 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:32,364, word2vec, INFO, EPOCH 6 - PROGRESS: at 35.25% examples, 74719 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:33,388, word2vec, INFO, EPOCH 6 - PROGRESS: at 37.32% examples, 75069 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:34,395, word2vec, INFO, EPOCH 6 - PROGRESS: at 39.21% examples, 75014 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:35,431, word2vec, INFO, EPOCH 6 - PROGRESS: at 41.18% examples, 75263 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:36,516, word2vec, INFO, EPOCH 6 - PROGRESS: at 43.05% examples, 74953 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:37,571, word2vec, INFO, EPOCH 6 - PROGRESS: at 44.97% examples, 74744 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:38,632, word2vec, INFO, EPOCH 6 - PROGRESS: at 46.94% examples, 74526 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:39,673, word2vec, INFO, EPOCH 6 - PROGRESS: at 48.81% examples, 74401 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:40,688, word2vec, INFO, EPOCH 6 - PROGRESS: at 50.70% examples, 74348 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:41,693, word2vec, INFO, EPOCH 6 - PROGRESS: at 52.56% examples, 74335 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:42,759, word2vec, INFO, EPOCH 6 - PROGRESS: at 54.42% examples, 74158 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:43,766, word2vec, INFO, EPOCH 6 - PROGRESS: at 56.26% examples, 74149 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:44,767, word2vec, INFO, EPOCH 6 - PROGRESS: at 58.12% examples, 74140 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:45,853, word2vec, INFO, EPOCH 6 - PROGRESS: at 60.03% examples, 73945 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:46,939, word2vec, INFO, EPOCH 6 - PROGRESS: at 61.88% examples, 73768 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:48,003, word2vec, INFO, EPOCH 6 - PROGRESS: at 63.83% examples, 73636 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:49,034, word2vec, INFO, EPOCH 6 - PROGRESS: at 65.92% examples, 73847 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:50,082, word2vec, INFO, EPOCH 6 - PROGRESS: at 68.00% examples, 74010 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:51,106, word2vec, INFO, EPOCH 6 - PROGRESS: at 69.80% examples, 73974 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:52,126, word2vec, INFO, EPOCH 6 - PROGRESS: at 71.62% examples, 73946 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:25:53,174, word2vec, INFO, EPOCH 6 - PROGRESS: at 73.97% examples, 74331 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:54,229, word2vec, INFO, EPOCH 6 - PROGRESS: at 75.75% examples, 74230 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:55,324, word2vec, INFO, EPOCH 6 - PROGRESS: at 77.61% examples, 74060 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:56,338, word2vec, INFO, EPOCH 6 - PROGRESS: at 79.20% examples, 73827 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:25:57,389, word2vec, INFO, EPOCH 6 - PROGRESS: at 81.26% examples, 73960 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:58,428, word2vec, INFO, EPOCH 6 - PROGRESS: at 83.10% examples, 73903 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:25:59,563, word2vec, INFO, EPOCH 6 - PROGRESS: at 85.50% examples, 74091 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:00,644, word2vec, INFO, EPOCH 6 - PROGRESS: at 87.64% examples, 74163 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:01,802, word2vec, INFO, EPOCH 6 - PROGRESS: at 89.82% examples, 74112 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:02,860, word2vec, INFO, EPOCH 6 - PROGRESS: at 91.88% examples, 74200 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:04,090, word2vec, INFO, EPOCH 6 - PROGRESS: at 94.03% examples, 74048 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:05,157, word2vec, INFO, EPOCH 6 - PROGRESS: at 95.97% examples, 73958 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:06,220, word2vec, INFO, EPOCH 6 - PROGRESS: at 98.09% examples, 74054 words/s, in_qsize 8, out_qsize 0 ]
[2024-12-11 21:26:06,875, word2vec, INFO, EPOCH 6: training on 4268692 raw words (3977884 effective words) took 53.4s, 74531 effective words/s ]
[2024-12-11 21:26:07,915, word2vec, INFO, EPOCH 7 - PROGRESS: at 1.61% examples, 62987 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 21:26:08,936, word2vec, INFO, EPOCH 7 - PROGRESS: at 3.19% examples, 63105 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:26:10,003, word2vec, INFO, EPOCH 7 - PROGRESS: at 5.09% examples, 65313 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:11,018, word2vec, INFO, EPOCH 7 - PROGRESS: at 7.10% examples, 69454 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:12,086, word2vec, INFO, EPOCH 7 - PROGRESS: at 8.97% examples, 69501 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:13,173, word2vec, INFO, EPOCH 7 - PROGRESS: at 11.03% examples, 70773 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:14,395, word2vec, INFO, EPOCH 7 - PROGRESS: at 13.19% examples, 70372 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:15,418, word2vec, INFO, EPOCH 7 - PROGRESS: at 15.39% examples, 71746 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:16,877, word2vec, INFO, EPOCH 7 - PROGRESS: at 17.71% examples, 70545 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:17,912, word2vec, INFO, EPOCH 7 - PROGRESS: at 19.81% examples, 71503 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:18,918, word2vec, INFO, EPOCH 7 - PROGRESS: at 21.64% examples, 71705 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:19,928, word2vec, INFO, EPOCH 7 - PROGRESS: at 23.49% examples, 71833 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:20,994, word2vec, INFO, EPOCH 7 - PROGRESS: at 25.75% examples, 72948 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:22,018, word2vec, INFO, EPOCH 7 - PROGRESS: at 27.61% examples, 72898 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:23,124, word2vec, INFO, EPOCH 7 - PROGRESS: at 29.69% examples, 73078 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:24,243, word2vec, INFO, EPOCH 7 - PROGRESS: at 31.78% examples, 73158 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:25,346, word2vec, INFO, EPOCH 7 - PROGRESS: at 33.63% examples, 72811 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:26,385, word2vec, INFO, EPOCH 7 - PROGRESS: at 35.50% examples, 72742 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:27,443, word2vec, INFO, EPOCH 7 - PROGRESS: at 38.01% examples, 73958 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:28,481, word2vec, INFO, EPOCH 7 - PROGRESS: at 39.89% examples, 73840 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:29,581, word2vec, INFO, EPOCH 7 - PROGRESS: at 41.90% examples, 73946 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:30,602, word2vec, INFO, EPOCH 7 - PROGRESS: at 44.02% examples, 74289 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:31,681, word2vec, INFO, EPOCH 7 - PROGRESS: at 45.97% examples, 74036 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:32,772, word2vec, INFO, EPOCH 7 - PROGRESS: at 47.86% examples, 73782 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:33,891, word2vec, INFO, EPOCH 7 - PROGRESS: at 50.00% examples, 73817 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:35,095, word2vec, INFO, EPOCH 7 - PROGRESS: at 52.33% examples, 73953 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:36,150, word2vec, INFO, EPOCH 7 - PROGRESS: at 54.43% examples, 74141 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:37,211, word2vec, INFO, EPOCH 7 - PROGRESS: at 56.52% examples, 74301 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:38,304, word2vec, INFO, EPOCH 7 - PROGRESS: at 58.60% examples, 74368 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:39,560, word2vec, INFO, EPOCH 7 - PROGRESS: at 60.73% examples, 74067 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:40,732, word2vec, INFO, EPOCH 7 - PROGRESS: at 62.82% examples, 73963 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:41,800, word2vec, INFO, EPOCH 7 - PROGRESS: at 65.23% examples, 74350 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:42,912, word2vec, INFO, EPOCH 7 - PROGRESS: at 67.31% examples, 74378 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:43,916, word2vec, INFO, EPOCH 7 - PROGRESS: at 69.11% examples, 74368 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:45,266, word2vec, INFO, EPOCH 7 - PROGRESS: at 71.16% examples, 73933 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:46,287, word2vec, INFO, EPOCH 7 - PROGRESS: at 73.07% examples, 73900 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:47,289, word2vec, INFO, EPOCH 7 - PROGRESS: at 74.89% examples, 73906 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:48,366, word2vec, INFO, EPOCH 7 - PROGRESS: at 76.90% examples, 73993 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:49,493, word2vec, INFO, EPOCH 7 - PROGRESS: at 78.99% examples, 74000 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:50,504, word2vec, INFO, EPOCH 7 - PROGRESS: at 81.03% examples, 74201 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:51,528, word2vec, INFO, EPOCH 7 - PROGRESS: at 82.90% examples, 74161 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:52,654, word2vec, INFO, EPOCH 7 - PROGRESS: at 85.01% examples, 74163 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:26:53,738, word2vec, INFO, EPOCH 7 - PROGRESS: at 86.92% examples, 74029 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:54,812, word2vec, INFO, EPOCH 7 - PROGRESS: at 88.84% examples, 73918 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:55,850, word2vec, INFO, EPOCH 7 - PROGRESS: at 91.03% examples, 74046 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:57,113, word2vec, INFO, EPOCH 7 - PROGRESS: at 93.59% examples, 74208 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:26:58,159, word2vec, INFO, EPOCH 7 - PROGRESS: at 95.47% examples, 74149 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-11 21:26:59,204, word2vec, INFO, EPOCH 7 - PROGRESS: at 97.61% examples, 74267 words/s, in_qsize 10, out_qsize 0 ]
[2024-12-11 21:27:00,055, word2vec, INFO, EPOCH 7: training on 4268692 raw words (3978166 effective words) took 53.2s, 74819 effective words/s ]
[2024-12-11 21:27:01,088, word2vec, INFO, EPOCH 8 - PROGRESS: at 1.16% examples, 45209 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:02,161, word2vec, INFO, EPOCH 8 - PROGRESS: at 3.20% examples, 61682 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:03,172, word2vec, INFO, EPOCH 8 - PROGRESS: at 5.07% examples, 65527 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:04,275, word2vec, INFO, EPOCH 8 - PROGRESS: at 7.10% examples, 68196 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:05,464, word2vec, INFO, EPOCH 8 - PROGRESS: at 9.22% examples, 68683 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:06,491, word2vec, INFO, EPOCH 8 - PROGRESS: at 11.03% examples, 69279 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:07,548, word2vec, INFO, EPOCH 8 - PROGRESS: at 12.97% examples, 69408 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:08,616, word2vec, INFO, EPOCH 8 - PROGRESS: at 14.95% examples, 69433 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:09,746, word2vec, INFO, EPOCH 8 - PROGRESS: at 17.48% examples, 71874 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:10,855, word2vec, INFO, EPOCH 8 - PROGRESS: at 19.33% examples, 71341 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:11,977, word2vec, INFO, EPOCH 8 - PROGRESS: at 21.19% examples, 70859 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:13,030, word2vec, INFO, EPOCH 8 - PROGRESS: at 23.04% examples, 70841 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:14,131, word2vec, INFO, EPOCH 8 - PROGRESS: at 25.13% examples, 71233 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:15,162, word2vec, INFO, EPOCH 8 - PROGRESS: at 26.93% examples, 71226 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:16,163, word2vec, INFO, EPOCH 8 - PROGRESS: at 29.23% examples, 72561 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:17,331, word2vec, INFO, EPOCH 8 - PROGRESS: at 31.32% examples, 72501 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:18,370, word2vec, INFO, EPOCH 8 - PROGRESS: at 33.18% examples, 72419 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:19,477, word2vec, INFO, EPOCH 8 - PROGRESS: at 35.00% examples, 72122 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:20,578, word2vec, INFO, EPOCH 8 - PROGRESS: at 36.86% examples, 71859 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:21,664, word2vec, INFO, EPOCH 8 - PROGRESS: at 38.75% examples, 71684 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:22,674, word2vec, INFO, EPOCH 8 - PROGRESS: at 40.94% examples, 72575 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:23,680, word2vec, INFO, EPOCH 8 - PROGRESS: at 43.05% examples, 73038 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:24,850, word2vec, INFO, EPOCH 8 - PROGRESS: at 45.19% examples, 72947 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:25,911, word2vec, INFO, EPOCH 8 - PROGRESS: at 47.18% examples, 72813 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:27,045, word2vec, INFO, EPOCH 8 - PROGRESS: at 49.27% examples, 72852 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 21:27:28,073, word2vec, INFO, EPOCH 8 - PROGRESS: at 51.40% examples, 73154 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:29,076, word2vec, INFO, EPOCH 8 - PROGRESS: at 53.27% examples, 73184 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:30,077, word2vec, INFO, EPOCH 8 - PROGRESS: at 54.91% examples, 72901 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:31,113, word2vec, INFO, EPOCH 8 - PROGRESS: at 57.38% examples, 73759 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:32,129, word2vec, INFO, EPOCH 8 - PROGRESS: at 59.30% examples, 73727 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:33,143, word2vec, INFO, EPOCH 8 - PROGRESS: at 60.98% examples, 73430 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:34,189, word2vec, INFO, EPOCH 8 - PROGRESS: at 63.06% examples, 73627 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:35,253, word2vec, INFO, EPOCH 8 - PROGRESS: at 65.47% examples, 74030 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:36,361, word2vec, INFO, EPOCH 8 - PROGRESS: at 67.31% examples, 73819 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:37,545, word2vec, INFO, EPOCH 8 - PROGRESS: at 69.35% examples, 73719 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:38,610, word2vec, INFO, EPOCH 8 - PROGRESS: at 71.16% examples, 73614 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:39,634, word2vec, INFO, EPOCH 8 - PROGRESS: at 73.07% examples, 73584 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:40,720, word2vec, INFO, EPOCH 8 - PROGRESS: at 75.07% examples, 73674 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:41,911, word2vec, INFO, EPOCH 8 - PROGRESS: at 77.39% examples, 73787 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:42,926, word2vec, INFO, EPOCH 8 - PROGRESS: at 79.44% examples, 73993 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:43,926, word2vec, INFO, EPOCH 8 - PROGRESS: at 81.49% examples, 74209 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:44,981, word2vec, INFO, EPOCH 8 - PROGRESS: at 83.36% examples, 74120 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:46,503, word2vec, INFO, EPOCH 8 - PROGRESS: at 85.50% examples, 73486 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:47,524, word2vec, INFO, EPOCH 8 - PROGRESS: at 87.64% examples, 73664 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:48,539, word2vec, INFO, EPOCH 8 - PROGRESS: at 89.87% examples, 73846 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:49,710, word2vec, INFO, EPOCH 8 - PROGRESS: at 92.14% examples, 73956 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:50,747, word2vec, INFO, EPOCH 8 - PROGRESS: at 94.05% examples, 73909 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:51,775, word2vec, INFO, EPOCH 8 - PROGRESS: at 95.95% examples, 73881 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:52,788, word2vec, INFO, EPOCH 8 - PROGRESS: at 97.83% examples, 73872 words/s, in_qsize 9, out_qsize 0 ]
[2024-12-11 21:27:53,483, word2vec, INFO, EPOCH 8: training on 4268692 raw words (3978021 effective words) took 53.4s, 74468 effective words/s ]
[2024-12-11 21:27:54,511, word2vec, INFO, EPOCH 9 - PROGRESS: at 0.48% examples, 18584 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-11 21:27:55,514, word2vec, INFO, EPOCH 9 - PROGRESS: at 2.96% examples, 59966 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:56,517, word2vec, INFO, EPOCH 9 - PROGRESS: at 4.85% examples, 64664 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:27:57,645, word2vec, INFO, EPOCH 9 - PROGRESS: at 6.70% examples, 64946 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:58,806, word2vec, INFO, EPOCH 9 - PROGRESS: at 8.75% examples, 66562 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:27:59,844, word2vec, INFO, EPOCH 9 - PROGRESS: at 10.56% examples, 67368 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:00,912, word2vec, INFO, EPOCH 9 - PROGRESS: at 12.47% examples, 67676 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:02,009, word2vec, INFO, EPOCH 9 - PROGRESS: at 14.71% examples, 68785 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:03,072, word2vec, INFO, EPOCH 9 - PROGRESS: at 16.78% examples, 69861 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:04,080, word2vec, INFO, EPOCH 9 - PROGRESS: at 18.86% examples, 71095 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:05,095, word2vec, INFO, EPOCH 9 - PROGRESS: at 20.97% examples, 72082 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:06,203, word2vec, INFO, EPOCH 9 - PROGRESS: at 23.04% examples, 72382 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:07,333, word2vec, INFO, EPOCH 9 - PROGRESS: at 25.13% examples, 72497 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:08,516, word2vec, INFO, EPOCH 9 - PROGRESS: at 26.93% examples, 71672 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:09,640, word2vec, INFO, EPOCH 9 - PROGRESS: at 28.76% examples, 71285 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:10,682, word2vec, INFO, EPOCH 9 - PROGRESS: at 31.03% examples, 72363 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:11,740, word2vec, INFO, EPOCH 9 - PROGRESS: at 33.18% examples, 72725 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:13,017, word2vec, INFO, EPOCH 9 - PROGRESS: at 35.25% examples, 72255 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:14,074, word2vec, INFO, EPOCH 9 - PROGRESS: at 37.09% examples, 72146 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:15,144, word2vec, INFO, EPOCH 9 - PROGRESS: at 38.98% examples, 72015 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:16,187, word2vec, INFO, EPOCH 9 - PROGRESS: at 40.96% examples, 72378 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:17,191, word2vec, INFO, EPOCH 9 - PROGRESS: at 43.06% examples, 72843 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:18,206, word2vec, INFO, EPOCH 9 - PROGRESS: at 45.21% examples, 73230 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:19,317, word2vec, INFO, EPOCH 9 - PROGRESS: at 47.40% examples, 73294 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:20,389, word2vec, INFO, EPOCH 9 - PROGRESS: at 49.30% examples, 73136 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:21,484, word2vec, INFO, EPOCH 9 - PROGRESS: at 51.16% examples, 72923 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:22,588, word2vec, INFO, EPOCH 9 - PROGRESS: at 53.27% examples, 73032 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:23,678, word2vec, INFO, EPOCH 9 - PROGRESS: at 55.11% examples, 72847 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:24,760, word2vec, INFO, EPOCH 9 - PROGRESS: at 57.38% examples, 73295 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:25,787, word2vec, INFO, EPOCH 9 - PROGRESS: at 59.55% examples, 73541 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:26,901, word2vec, INFO, EPOCH 9 - PROGRESS: at 61.66% examples, 73589 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:27,926, word2vec, INFO, EPOCH 9 - PROGRESS: at 63.60% examples, 73548 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:29,029, word2vec, INFO, EPOCH 9 - PROGRESS: at 65.23% examples, 73088 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:30,089, word2vec, INFO, EPOCH 9 - PROGRESS: at 67.09% examples, 73007 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:31,118, word2vec, INFO, EPOCH 9 - PROGRESS: at 69.13% examples, 73232 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:32,225, word2vec, INFO, EPOCH 9 - PROGRESS: at 71.16% examples, 73301 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:33,246, word2vec, INFO, EPOCH 9 - PROGRESS: at 73.31% examples, 73519 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-11 21:28:34,358, word2vec, INFO, EPOCH 9 - PROGRESS: at 75.30% examples, 73565 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:35,401, word2vec, INFO, EPOCH 9 - PROGRESS: at 77.13% examples, 73497 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:36,505, word2vec, INFO, EPOCH 9 - PROGRESS: at 79.00% examples, 73340 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:37,627, word2vec, INFO, EPOCH 9 - PROGRESS: at 80.80% examples, 73163 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:38,658, word2vec, INFO, EPOCH 9 - PROGRESS: at 83.13% examples, 73544 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:39,735, word2vec, INFO, EPOCH 9 - PROGRESS: at 85.01% examples, 73435 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:40,785, word2vec, INFO, EPOCH 9 - PROGRESS: at 86.95% examples, 73374 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:41,825, word2vec, INFO, EPOCH 9 - PROGRESS: at 89.12% examples, 73523 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:42,861, word2vec, INFO, EPOCH 9 - PROGRESS: at 91.25% examples, 73663 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:43,891, word2vec, INFO, EPOCH 9 - PROGRESS: at 93.34% examples, 73806 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:44,891, word2vec, INFO, EPOCH 9 - PROGRESS: at 94.94% examples, 73639 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-11 21:28:45,936, word2vec, INFO, EPOCH 9 - PROGRESS: at 97.09% examples, 73764 words/s, in_qsize 12, out_qsize 0 ]
[2024-12-11 21:28:46,948, word2vec, INFO, EPOCH 9 - PROGRESS: at 99.27% examples, 73935 words/s, in_qsize 3, out_qsize 1 ]
[2024-12-11 21:28:47,204, word2vec, INFO, EPOCH 9: training on 4268692 raw words (3978260 effective words) took 53.7s, 74092 effective words/s ]
[2024-12-11 21:28:47,205, utils, INFO, FastText lifecycle event {'msg': 'training on 42686920 raw words (39779722 effective words) took 526.8s, 75513 effective words/s', 'datetime': '2024-12-11T21:28:47.205950', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-11 21:28:56,352, utils, INFO, FastText lifecycle event {'params': 'FastText<vocab=160129, vector_size=500, alpha=0.025>', 'datetime': '2024-12-11T21:28:56.352297', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-11 21:28:57,137, utils, INFO, FastText lifecycle event {'fname_or_handle': 'fasttext_reviews.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-12-11T21:28:57.137405', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'} ]
[2024-12-11 21:28:57,141, utils, INFO, not storing attribute vectors ]
[2024-12-11 21:28:57,142, utils, INFO, storing np array 'vectors_vocab' to fasttext_reviews.model.wv.vectors_vocab.npy ]
[2024-12-11 21:28:59,174, utils, INFO, storing np array 'vectors_ngrams' to fasttext_reviews.model.wv.vectors_ngrams.npy ]
[2024-12-11 21:29:25,343, utils, INFO, not storing attribute buckets_word ]
[2024-12-11 21:29:25,350, utils, INFO, storing np array 'syn1neg' to fasttext_reviews.model.syn1neg.npy ]
[2024-12-11 21:29:27,389, utils, INFO, not storing attribute cum_table ]
[2024-12-11 21:29:27,562, utils, INFO, saved fasttext_reviews.model ]
[2024-12-11 21:29:35,454, utils, INFO, loading FastText object from fasttext_reviews.model ]
[2024-12-11 21:29:35,848, utils, INFO, loading wv recursively from fasttext_reviews.model.wv.* with mmap=None ]
[2024-12-11 21:29:35,849, utils, INFO, loading vectors_vocab from fasttext_reviews.model.wv.vectors_vocab.npy with mmap=None ]
[2024-12-11 21:29:36,006, utils, INFO, loading vectors_ngrams from fasttext_reviews.model.wv.vectors_ngrams.npy with mmap=None ]
[2024-12-11 21:29:46,632, utils, INFO, setting ignored attribute vectors to None ]
[2024-12-11 21:29:46,635, utils, INFO, setting ignored attribute buckets_word to None ]
[2024-12-11 21:29:58,177, utils, INFO, loading syn1neg from fasttext_reviews.model.syn1neg.npy with mmap=None ]
[2024-12-11 21:29:59,054, utils, INFO, setting ignored attribute cum_table to None ]
[2024-12-11 21:30:00,686, utils, INFO, FastText lifecycle event {'fname': 'fasttext_reviews.model', 'datetime': '2024-12-11T21:30:00.686556', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-11 22:14:35,804, utils, INFO, FastTextKeyedVectors lifecycle event {'fname_or_handle': 'fasttext_keyedvectors.kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-12-11T22:14:35.804502', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'} ]
[2024-12-11 22:14:35,806, utils, INFO, storing np array 'vectors_vocab' to fasttext_keyedvectors.kv.vectors_vocab.npy ]
[2024-12-11 22:14:36,716, utils, INFO, storing np array 'vectors_ngrams' to fasttext_keyedvectors.kv.vectors_ngrams.npy ]
[2024-12-11 22:15:01,057, utils, INFO, not storing attribute vectors ]
[2024-12-11 22:15:01,060, utils, INFO, not storing attribute buckets_word ]
[2024-12-11 22:15:01,148, utils, INFO, saved fasttext_keyedvectors.kv ]
[2024-12-11 22:17:59,739, utils, INFO, loading KeyedVectors object from fasttext_keyedvectors.kv ]
[2024-12-11 22:17:59,910, utils, INFO, loading vectors_vocab from fasttext_keyedvectors.kv.vectors_vocab.npy with mmap=None ]
[2024-12-11 22:18:00,066, utils, INFO, loading vectors_ngrams from fasttext_keyedvectors.kv.vectors_ngrams.npy with mmap=None ]
[2024-12-11 22:18:11,561, utils, INFO, setting ignored attribute vectors to None ]
[2024-12-11 22:18:11,563, utils, INFO, setting ignored attribute buckets_word to None ]
[2024-12-11 22:18:23,548, utils, INFO, FastTextKeyedVectors lifecycle event {'fname': 'fasttext_keyedvectors.kv', 'datetime': '2024-12-11T22:18:23.548832', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-11 22:20:07,560, utils, INFO, FastTextKeyedVectors lifecycle event {'fname_or_handle': 'fasttext_keyedvectors.kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-12-11T22:20:07.560566', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'} ]
[2024-12-11 22:20:07,563, utils, INFO, storing np array 'vectors_vocab' to fasttext_keyedvectors.kv.vectors_vocab.npy ]
[2024-12-11 22:20:10,573, utils, INFO, storing np array 'vectors_ngrams' to fasttext_keyedvectors.kv.vectors_ngrams.npy ]
[2024-12-11 22:21:02,012, utils, INFO, not storing attribute vectors ]
[2024-12-11 22:21:02,014, utils, INFO, not storing attribute buckets_word ]
[2024-12-11 22:21:02,151, utils, INFO, saved fasttext_keyedvectors.kv ]
[2024-12-11 22:21:37,984, utils, INFO, loading KeyedVectors object from fasttext_keyedvectors.kv ]
[2024-12-11 22:21:48,379, utils, INFO, loading vectors_vocab from fasttext_keyedvectors.kv.vectors_vocab.npy with mmap=None ]
[2024-12-11 22:22:37,194, utils, INFO, loading KeyedVectors object from fasttext_keyedvectors_binary.kv ]
[2024-12-11 22:22:56,926, utils, INFO, loading KeyedVectors object from fasttext_keyedvectors_binary.kv ]
[2024-12-11 22:23:08,371, utils, INFO, loading KeyedVectors object from fasttext_keyedvectors.kv ]
[2024-12-11 22:23:25,055, utils, INFO, loading vectors_vocab from fasttext_keyedvectors.kv.vectors_vocab.npy with mmap=None ]
[2024-12-12 18:38:22,851, keyedvectors, INFO, loading projection weights from skipgram_model.bin ]
[2024-12-12 19:04:57,161, keyedvectors, WARNING, attribute count not present in KeyedVectors<vector_size=300, 30489 keys>; will store in internal index_to_key order ]
[2024-12-12 19:04:57,162, keyedvectors, INFO, storing 30489x300 projection weights into gensim_skipgram_model.bin ]
[2024-12-12 19:04:57,456, utils, INFO, KeyedVectors lifecycle event {'fname_or_handle': 'quantized_model.kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-12-12T19:04:57.456015', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'} ]
[2024-12-12 19:04:57,540, utils, INFO, saved quantized_model.kv ]
[2024-12-12 19:22:37,898, utils, INFO, loading KeyedVectors object from quantized_model.kv ]
[2024-12-12 19:22:37,944, utils, INFO, KeyedVectors lifecycle event {'fname': 'quantized_model.kv', 'datetime': '2024-12-12T19:22:37.944668', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-12 19:23:23,894, utils, INFO, loading KeyedVectors object from quantized_model.kv ]
[2024-12-12 19:23:23,934, utils, INFO, KeyedVectors lifecycle event {'fname': 'quantized_model.kv', 'datetime': '2024-12-12T19:23:23.934388', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-12 19:53:11,651, keyedvectors, INFO, loading projection weights from skipgram_model.bin ]
[2024-12-12 19:54:30,364, keyedvectors, INFO, loading projection weights from skipgram_model.bin ]
[2024-12-12 19:55:33,018, keyedvectors, INFO, loading projection weights from skipgram_model.bin ]
[2024-12-12 19:57:56,722, keyedvectors, INFO, loading projection weights from skipgram_model.bin ]
[2024-12-12 19:58:08,253, keyedvectors, INFO, loading projection weights from skipgram_model.bin ]
[2024-12-12 20:47:56,823, keyedvectors, WARNING, attribute count not present in KeyedVectors<vector_size=300, 115342 keys>; will store in internal index_to_key order ]
[2024-12-12 20:47:56,826, keyedvectors, INFO, storing 115342x300 projection weights into gensim_my_fasttext_model.bin ]
[2024-12-12 20:47:57,997, keyedvectors, WARNING, destructive init_sims(replace=True) deprecated & no longer required for space-efficiency ]
[2024-12-12 20:47:58,021, utils, INFO, KeyedVectors lifecycle event {'fname_or_handle': 'quantized_model.kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-12-12T20:47:58.021785', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'} ]
[2024-12-12 20:47:58,026, utils, INFO, storing np array 'vectors' to quantized_model.kv.vectors.npy ]
[2024-12-12 20:47:58,419, utils, INFO, saved quantized_model.kv ]
[2024-12-12 20:49:58,661, utils, INFO, loading KeyedVectors object from quantized_model.kv ]
[2024-12-12 20:49:59,028, utils, INFO, loading vectors from quantized_model.kv.vectors.npy with mmap=r ]
[2024-12-12 20:49:59,055, utils, INFO, KeyedVectors lifecycle event {'fname': 'quantized_model.kv', 'datetime': '2024-12-12T20:49:59.055133', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-13 06:26:58,826, utils, INFO, loading KeyedVectors object from quantized_model.kv ]
[2024-12-13 06:26:58,881, utils, INFO, loading vectors from quantized_model.kv.vectors.npy with mmap=r ]
[2024-12-13 06:26:58,892, utils, INFO, KeyedVectors lifecycle event {'fname': 'quantized_model.kv', 'datetime': '2024-12-13T06:26:58.892142', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-13 06:30:00,441, utils, INFO, loading KeyedVectors object from quantized_model.kv ]
[2024-12-13 06:30:00,491, utils, INFO, loading vectors from quantized_model.kv.vectors.npy with mmap=None ]
[2024-12-13 06:30:00,537, utils, INFO, KeyedVectors lifecycle event {'fname': 'quantized_model.kv', 'datetime': '2024-12-13T06:30:00.537891', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-13 06:54:37,875, keyedvectors, INFO, loading projection weights from gensim_my_fasttext_model.bin ]
[2024-12-13 06:54:39,344, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (115342, 300) matrix of type float32 from gensim_my_fasttext_model.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T06:54:39.344131', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 07:03:55,951, word2vec, INFO, collecting all words and their counts ]
[2024-12-13 07:03:55,953, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-13 07:03:56,093, word2vec, INFO, PROGRESS: at sentence #10000, processed 417356 words, keeping 38269 word types ]
[2024-12-13 07:03:56,277, word2vec, INFO, PROGRESS: at sentence #20000, processed 827291 words, keeping 57463 word types ]
[2024-12-13 07:03:56,435, word2vec, INFO, PROGRESS: at sentence #30000, processed 1245251 words, keeping 73816 word types ]
[2024-12-13 07:03:56,610, word2vec, INFO, PROGRESS: at sentence #40000, processed 1659818 words, keeping 88317 word types ]
[2024-12-13 07:03:56,845, word2vec, INFO, PROGRESS: at sentence #50000, processed 2073395 words, keeping 101105 word types ]
[2024-12-13 07:03:57,065, word2vec, INFO, PROGRESS: at sentence #60000, processed 2485740 words, keeping 113234 word types ]
[2024-12-13 07:03:57,233, word2vec, INFO, PROGRESS: at sentence #70000, processed 2895180 words, keeping 124533 word types ]
[2024-12-13 07:03:57,435, word2vec, INFO, PROGRESS: at sentence #80000, processed 3316945 words, keeping 135881 word types ]
[2024-12-13 07:03:57,612, word2vec, INFO, PROGRESS: at sentence #90000, processed 3730627 words, keeping 146822 word types ]
[2024-12-13 07:03:57,796, word2vec, INFO, PROGRESS: at sentence #100000, processed 4136518 words, keeping 156920 word types ]
[2024-12-13 07:03:57,860, word2vec, INFO, collected 160129 word types from a corpus of 4268692 raw words and 103304 sentences ]
[2024-12-13 07:03:57,862, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-13 07:03:58,748, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 160129 unique words (100.00% of original 160129, drops 0)', 'datetime': '2024-12-13T07:03:58.748080', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 07:03:58,749, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4268692 word corpus (100.00% of original 4268692, drops 0)', 'datetime': '2024-12-13T07:03:58.749114', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 07:04:00,531, word2vec, INFO, deleting the raw counts dictionary of 160129 items ]
[2024-12-13 07:04:00,538, word2vec, INFO, sample=0.001 downsamples 31 most-common words ]
[2024-12-13 07:04:00,540, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 3978110.9017322585 word corpus (93.2%% of prior 4268692)', 'datetime': '2024-12-13T07:04:00.540454', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 07:04:06,133, fasttext, INFO, estimated required memory for 160129 words, 2000000 buckets and 300 dimensions: 2899895040 bytes ]
[2024-12-13 07:04:06,134, word2vec, INFO, resetting layer weights ]
[2024-12-13 07:04:23,536, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-13T07:04:23.536569', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]
[2024-12-13 07:04:23,538, utils, INFO, FastText lifecycle event {'msg': 'training model with 8 workers on 160129 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-12-13T07:04:23.538522', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-13 07:04:24,562, word2vec, INFO, EPOCH 0 - PROGRESS: at 3.20% examples, 128248 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:25,578, word2vec, INFO, EPOCH 0 - PROGRESS: at 6.46% examples, 127985 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:26,588, word2vec, INFO, EPOCH 0 - PROGRESS: at 10.60% examples, 140501 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:27,757, word2vec, INFO, EPOCH 0 - PROGRESS: at 15.16% examples, 143435 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:28,789, word2vec, INFO, EPOCH 0 - PROGRESS: at 19.12% examples, 145265 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:29,842, word2vec, INFO, EPOCH 0 - PROGRESS: at 23.71% examples, 150465 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:30,920, word2vec, INFO, EPOCH 0 - PROGRESS: at 27.85% examples, 151044 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:32,016, word2vec, INFO, EPOCH 0 - PROGRESS: at 32.00% examples, 151167 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:33,029, word2vec, INFO, EPOCH 0 - PROGRESS: at 36.61% examples, 154577 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:34,165, word2vec, INFO, EPOCH 0 - PROGRESS: at 40.97% examples, 154631 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:35,240, word2vec, INFO, EPOCH 0 - PROGRESS: at 45.44% examples, 155499 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:36,258, word2vec, INFO, EPOCH 0 - PROGRESS: at 50.00% examples, 156897 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:37,297, word2vec, INFO, EPOCH 0 - PROGRESS: at 54.18% examples, 157177 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:38,319, word2vec, INFO, EPOCH 0 - PROGRESS: at 57.63% examples, 155710 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:39,337, word2vec, INFO, EPOCH 0 - PROGRESS: at 61.88% examples, 156250 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:40,380, word2vec, INFO, EPOCH 0 - PROGRESS: at 66.38% examples, 157024 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:41,487, word2vec, INFO, EPOCH 0 - PROGRESS: at 70.69% examples, 157163 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:42,488, word2vec, INFO, EPOCH 0 - PROGRESS: at 74.67% examples, 157187 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:43,560, word2vec, INFO, EPOCH 0 - PROGRESS: at 79.43% examples, 158504 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:44,620, word2vec, INFO, EPOCH 0 - PROGRESS: at 83.36% examples, 158023 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:04:45,694, word2vec, INFO, EPOCH 0 - PROGRESS: at 87.88% examples, 158314 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:46,743, word2vec, INFO, EPOCH 0 - PROGRESS: at 92.14% examples, 158310 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:47,793, word2vec, INFO, EPOCH 0 - PROGRESS: at 95.47% examples, 156828 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:48,798, word2vec, INFO, EPOCH 0 - PROGRESS: at 99.03% examples, 156105 words/s, in_qsize 4, out_qsize 1 ]
[2024-12-13 07:04:48,962, word2vec, INFO, EPOCH 0: training on 4268692 raw words (3977480 effective words) took 25.4s, 156542 effective words/s ]
[2024-12-13 07:04:49,993, word2vec, INFO, EPOCH 1 - PROGRESS: at 2.52% examples, 100279 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:51,007, word2vec, INFO, EPOCH 1 - PROGRESS: at 6.66% examples, 132439 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:04:52,184, word2vec, INFO, EPOCH 1 - PROGRESS: at 11.30% examples, 141739 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:53,217, word2vec, INFO, EPOCH 1 - PROGRESS: at 15.86% examples, 148807 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:54,242, word2vec, INFO, EPOCH 1 - PROGRESS: at 19.58% examples, 147988 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:55,277, word2vec, INFO, EPOCH 1 - PROGRESS: at 24.17% examples, 153208 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:56,463, word2vec, INFO, EPOCH 1 - PROGRESS: at 28.31% examples, 151114 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:04:57,519, word2vec, INFO, EPOCH 1 - PROGRESS: at 32.48% examples, 151956 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:58,526, word2vec, INFO, EPOCH 1 - PROGRESS: at 36.40% examples, 152451 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:04:59,603, word2vec, INFO, EPOCH 1 - PROGRESS: at 40.74% examples, 153593 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:00,812, word2vec, INFO, EPOCH 1 - PROGRESS: at 44.97% examples, 152031 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:01,821, word2vec, INFO, EPOCH 1 - PROGRESS: at 48.81% examples, 151607 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:02,861, word2vec, INFO, EPOCH 1 - PROGRESS: at 53.51% examples, 153617 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:03,897, word2vec, INFO, EPOCH 1 - PROGRESS: at 57.16% examples, 152912 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:04,936, word2vec, INFO, EPOCH 1 - PROGRESS: at 60.03% examples, 149907 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:05,940, word2vec, INFO, EPOCH 1 - PROGRESS: at 63.83% examples, 149774 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:07,027, word2vec, INFO, EPOCH 1 - PROGRESS: at 67.52% examples, 148981 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:08,119, word2vec, INFO, EPOCH 1 - PROGRESS: at 71.16% examples, 148243 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:09,242, word2vec, INFO, EPOCH 1 - PROGRESS: at 74.43% examples, 146443 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-13 07:05:10,319, word2vec, INFO, EPOCH 1 - PROGRESS: at 78.07% examples, 146001 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:11,373, word2vec, INFO, EPOCH 1 - PROGRESS: at 80.80% examples, 144125 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:12,514, word2vec, INFO, EPOCH 1 - PROGRESS: at 84.78% examples, 143827 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:13,582, word2vec, INFO, EPOCH 1 - PROGRESS: at 88.62% examples, 143621 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:14,683, word2vec, INFO, EPOCH 1 - PROGRESS: at 92.14% examples, 142856 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:15,692, word2vec, INFO, EPOCH 1 - PROGRESS: at 94.94% examples, 141641 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:16,768, word2vec, INFO, EPOCH 1 - PROGRESS: at 98.81% examples, 141510 words/s, in_qsize 5, out_qsize 1 ]
[2024-12-13 07:05:17,069, word2vec, INFO, EPOCH 1: training on 4268692 raw words (3978483 effective words) took 28.1s, 141634 effective words/s ]
[2024-12-13 07:05:18,213, word2vec, INFO, EPOCH 2 - PROGRESS: at 2.07% examples, 75267 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:19,352, word2vec, INFO, EPOCH 2 - PROGRESS: at 5.77% examples, 102978 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:05:20,363, word2vec, INFO, EPOCH 2 - PROGRESS: at 8.96% examples, 110956 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:21,392, word2vec, INFO, EPOCH 2 - PROGRESS: at 11.77% examples, 110319 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:05:22,464, word2vec, INFO, EPOCH 2 - PROGRESS: at 15.16% examples, 112463 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:23,548, word2vec, INFO, EPOCH 2 - PROGRESS: at 19.12% examples, 118036 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:24,566, word2vec, INFO, EPOCH 2 - PROGRESS: at 23.49% examples, 125574 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:25,582, word2vec, INFO, EPOCH 2 - PROGRESS: at 27.35% examples, 129018 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:26,598, word2vec, INFO, EPOCH 2 - PROGRESS: at 32.00% examples, 134718 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:27,642, word2vec, INFO, EPOCH 2 - PROGRESS: at 36.40% examples, 138097 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:28,724, word2vec, INFO, EPOCH 2 - PROGRESS: at 40.74% examples, 140403 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:29,725, word2vec, INFO, EPOCH 2 - PROGRESS: at 44.71% examples, 141800 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:05:30,757, word2vec, INFO, EPOCH 2 - PROGRESS: at 49.54% examples, 144626 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:31,850, word2vec, INFO, EPOCH 2 - PROGRESS: at 53.95% examples, 145863 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:32,982, word2vec, INFO, EPOCH 2 - PROGRESS: at 58.85% examples, 147716 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:34,015, word2vec, INFO, EPOCH 2 - PROGRESS: at 63.33% examples, 149109 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:35,027, word2vec, INFO, EPOCH 2 - PROGRESS: at 67.77% examples, 150527 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:36,124, word2vec, INFO, EPOCH 2 - PROGRESS: at 72.11% examples, 151125 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:37,137, word2vec, INFO, EPOCH 2 - PROGRESS: at 76.45% examples, 152288 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:38,254, word2vec, INFO, EPOCH 2 - PROGRESS: at 81.03% examples, 153033 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:39,261, word2vec, INFO, EPOCH 2 - PROGRESS: at 85.74% examples, 154450 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:05:40,490, word2vec, INFO, EPOCH 2 - PROGRESS: at 90.59% examples, 154244 words/s, in_qsize 15, out_qsize 2 ]
[2024-12-13 07:05:41,528, word2vec, INFO, EPOCH 2 - PROGRESS: at 95.72% examples, 156036 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:42,537, word2vec, INFO, EPOCH 2 - PROGRESS: at 99.51% examples, 155700 words/s, in_qsize 2, out_qsize 1 ]
[2024-12-13 07:05:42,730, word2vec, INFO, EPOCH 2: training on 4268692 raw words (3978071 effective words) took 25.6s, 155237 effective words/s ]
[2024-12-13 07:05:43,791, word2vec, INFO, EPOCH 3 - PROGRESS: at 2.06% examples, 80102 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:44,846, word2vec, INFO, EPOCH 3 - PROGRESS: at 5.78% examples, 110539 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:45,870, word2vec, INFO, EPOCH 3 - PROGRESS: at 9.67% examples, 124954 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:46,936, word2vec, INFO, EPOCH 3 - PROGRESS: at 14.43% examples, 137491 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:47,958, word2vec, INFO, EPOCH 3 - PROGRESS: at 19.12% examples, 146159 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:49,018, word2vec, INFO, EPOCH 3 - PROGRESS: at 22.80% examples, 145157 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:50,026, word2vec, INFO, EPOCH 3 - PROGRESS: at 27.85% examples, 152942 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:51,056, word2vec, INFO, EPOCH 3 - PROGRESS: at 33.18% examples, 159638 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:52,075, word2vec, INFO, EPOCH 3 - PROGRESS: at 37.55% examples, 161138 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:53,078, word2vec, INFO, EPOCH 3 - PROGRESS: at 41.63% examples, 161653 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:54,108, word2vec, INFO, EPOCH 3 - PROGRESS: at 46.20% examples, 162485 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:55,118, word2vec, INFO, EPOCH 3 - PROGRESS: at 50.70% examples, 163444 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:05:56,159, word2vec, INFO, EPOCH 3 - PROGRESS: at 56.05% examples, 166670 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:57,218, word2vec, INFO, EPOCH 3 - PROGRESS: at 61.20% examples, 168571 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:05:58,221, word2vec, INFO, EPOCH 3 - PROGRESS: at 66.84% examples, 172032 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:05:59,230, word2vec, INFO, EPOCH 3 - PROGRESS: at 71.88% examples, 173902 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:00,232, word2vec, INFO, EPOCH 3 - PROGRESS: at 76.90% examples, 175606 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:01,251, word2vec, INFO, EPOCH 3 - PROGRESS: at 81.92% examples, 176990 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:02,344, word2vec, INFO, EPOCH 3 - PROGRESS: at 87.15% examples, 177535 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:03,440, word2vec, INFO, EPOCH 3 - PROGRESS: at 92.83% examples, 178852 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:04,471, word2vec, INFO, EPOCH 3 - PROGRESS: at 98.32% examples, 180200 words/s, in_qsize 7, out_qsize 1 ]
[2024-12-13 07:06:04,645, word2vec, INFO, EPOCH 3: training on 4268692 raw words (3978766 effective words) took 21.9s, 181721 effective words/s ]
[2024-12-13 07:06:05,735, word2vec, INFO, EPOCH 4 - PROGRESS: at 3.91% examples, 145400 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:06,867, word2vec, INFO, EPOCH 4 - PROGRESS: at 9.44% examples, 171881 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:07,879, word2vec, INFO, EPOCH 4 - PROGRESS: at 14.69% examples, 181223 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:08,894, word2vec, INFO, EPOCH 4 - PROGRESS: at 19.82% examples, 185974 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:09,990, word2vec, INFO, EPOCH 4 - PROGRESS: at 24.41% examples, 182567 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:11,034, word2vec, INFO, EPOCH 4 - PROGRESS: at 29.93% examples, 187479 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:12,044, word2vec, INFO, EPOCH 4 - PROGRESS: at 35.25% examples, 190708 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:13,113, word2vec, INFO, EPOCH 4 - PROGRESS: at 40.74% examples, 192899 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:14,160, word2vec, INFO, EPOCH 4 - PROGRESS: at 46.43% examples, 195056 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:15,179, word2vec, INFO, EPOCH 4 - PROGRESS: at 51.16% examples, 193794 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:16,220, word2vec, INFO, EPOCH 4 - PROGRESS: at 56.74% examples, 195602 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:17,267, word2vec, INFO, EPOCH 4 - PROGRESS: at 62.35% examples, 197002 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:18,318, word2vec, INFO, EPOCH 4 - PROGRESS: at 67.77% examples, 197457 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:19,342, word2vec, INFO, EPOCH 4 - PROGRESS: at 72.61% examples, 196976 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:20,409, word2vec, INFO, EPOCH 4 - PROGRESS: at 77.61% examples, 196580 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-13 07:06:21,469, word2vec, INFO, EPOCH 4 - PROGRESS: at 82.42% examples, 195786 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:22,490, word2vec, INFO, EPOCH 4 - PROGRESS: at 86.70% examples, 193949 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:23,538, word2vec, INFO, EPOCH 4 - PROGRESS: at 92.36% examples, 194933 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:24,555, word2vec, INFO, EPOCH 4 - PROGRESS: at 97.10% examples, 194319 words/s, in_qsize 12, out_qsize 0 ]
[2024-12-13 07:06:24,932, word2vec, INFO, EPOCH 4: training on 4268692 raw words (3978549 effective words) took 20.3s, 196194 effective words/s ]
[2024-12-13 07:06:25,991, word2vec, INFO, EPOCH 5 - PROGRESS: at 3.91% examples, 149757 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:27,103, word2vec, INFO, EPOCH 5 - PROGRESS: at 9.44% examples, 175867 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:28,186, word2vec, INFO, EPOCH 5 - PROGRESS: at 15.16% examples, 185727 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:29,247, word2vec, INFO, EPOCH 5 - PROGRESS: at 20.51% examples, 189546 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:30,300, word2vec, INFO, EPOCH 5 - PROGRESS: at 25.56% examples, 190435 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:31,388, word2vec, INFO, EPOCH 5 - PROGRESS: at 30.82% examples, 191292 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:32,437, word2vec, INFO, EPOCH 5 - PROGRESS: at 36.40% examples, 194187 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:33,480, word2vec, INFO, EPOCH 5 - PROGRESS: at 41.87% examples, 196547 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:34,513, word2vec, INFO, EPOCH 5 - PROGRESS: at 46.65% examples, 194688 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:06:35,540, word2vec, INFO, EPOCH 5 - PROGRESS: at 52.10% examples, 195967 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:06:36,553, word2vec, INFO, EPOCH 5 - PROGRESS: at 57.38% examples, 197236 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:37,567, word2vec, INFO, EPOCH 5 - PROGRESS: at 62.82% examples, 198275 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:38,596, word2vec, INFO, EPOCH 5 - PROGRESS: at 68.00% examples, 198281 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:39,619, word2vec, INFO, EPOCH 5 - PROGRESS: at 73.07% examples, 198401 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:40,721, word2vec, INFO, EPOCH 5 - PROGRESS: at 78.29% examples, 198055 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:41,851, word2vec, INFO, EPOCH 5 - PROGRESS: at 83.81% examples, 198004 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:42,852, word2vec, INFO, EPOCH 5 - PROGRESS: at 89.37% examples, 198829 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:43,887, word2vec, INFO, EPOCH 5 - PROGRESS: at 94.72% examples, 199193 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:44,766, word2vec, INFO, EPOCH 5: training on 4268692 raw words (3978289 effective words) took 19.8s, 200656 effective words/s ]
[2024-12-13 07:06:45,846, word2vec, INFO, EPOCH 6 - PROGRESS: at 4.17% examples, 155529 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:46,870, word2vec, INFO, EPOCH 6 - PROGRESS: at 9.44% examples, 181490 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:47,899, word2vec, INFO, EPOCH 6 - PROGRESS: at 14.65% examples, 187021 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:48,947, word2vec, INFO, EPOCH 6 - PROGRESS: at 19.82% examples, 188981 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:50,038, word2vec, INFO, EPOCH 6 - PROGRESS: at 24.87% examples, 188593 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:51,106, word2vec, INFO, EPOCH 6 - PROGRESS: at 29.47% examples, 185948 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:52,230, word2vec, INFO, EPOCH 6 - PROGRESS: at 34.54% examples, 185267 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:53,243, word2vec, INFO, EPOCH 6 - PROGRESS: at 39.67% examples, 187183 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:06:54,269, word2vec, INFO, EPOCH 6 - PROGRESS: at 44.73% examples, 188460 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:55,273, word2vec, INFO, EPOCH 6 - PROGRESS: at 49.51% examples, 188085 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:06:56,382, word2vec, INFO, EPOCH 6 - PROGRESS: at 55.11% examples, 189276 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:57,399, word2vec, INFO, EPOCH 6 - PROGRESS: at 60.73% examples, 191642 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:58,420, word2vec, INFO, EPOCH 6 - PROGRESS: at 66.16% examples, 192935 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:06:59,447, word2vec, INFO, EPOCH 6 - PROGRESS: at 70.69% examples, 192095 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:00,452, word2vec, INFO, EPOCH 6 - PROGRESS: at 76.20% examples, 193981 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:01,452, word2vec, INFO, EPOCH 6 - PROGRESS: at 81.03% examples, 194048 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:02,579, word2vec, INFO, EPOCH 6 - PROGRESS: at 86.23% examples, 193218 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:03,590, word2vec, INFO, EPOCH 6 - PROGRESS: at 91.45% examples, 193673 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:04,650, word2vec, INFO, EPOCH 6 - PROGRESS: at 96.86% examples, 194077 words/s, in_qsize 12, out_qsize 1 ]
[2024-12-13 07:07:05,126, word2vec, INFO, EPOCH 6: training on 4268692 raw words (3977816 effective words) took 20.4s, 195454 effective words/s ]
[2024-12-13 07:07:06,166, word2vec, INFO, EPOCH 7 - PROGRESS: at 3.91% examples, 152781 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:07,167, word2vec, INFO, EPOCH 7 - PROGRESS: at 9.22% examples, 182678 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:08,201, word2vec, INFO, EPOCH 7 - PROGRESS: at 14.43% examples, 187688 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:09,253, word2vec, INFO, EPOCH 7 - PROGRESS: at 19.12% examples, 184814 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:10,265, word2vec, INFO, EPOCH 7 - PROGRESS: at 24.39% examples, 189993 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:11,334, word2vec, INFO, EPOCH 7 - PROGRESS: at 29.93% examples, 192983 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:12,357, word2vec, INFO, EPOCH 7 - PROGRESS: at 34.54% examples, 191305 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:13,359, word2vec, INFO, EPOCH 7 - PROGRESS: at 39.21% examples, 190598 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:14,369, word2vec, INFO, EPOCH 7 - PROGRESS: at 44.24% examples, 191838 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:15,453, word2vec, INFO, EPOCH 7 - PROGRESS: at 49.74% examples, 192315 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:16,545, word2vec, INFO, EPOCH 7 - PROGRESS: at 54.67% examples, 190970 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:17,555, word2vec, INFO, EPOCH 7 - PROGRESS: at 59.80% examples, 191845 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:18,583, word2vec, INFO, EPOCH 7 - PROGRESS: at 64.77% examples, 191645 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:19,723, word2vec, INFO, EPOCH 7 - PROGRESS: at 70.25% examples, 191979 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:07:20,801, word2vec, INFO, EPOCH 7 - PROGRESS: at 75.75% examples, 193004 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:21,812, word2vec, INFO, EPOCH 7 - PROGRESS: at 80.80% examples, 193538 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:22,860, word2vec, INFO, EPOCH 7 - PROGRESS: at 85.97% examples, 193601 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:23,896, word2vec, INFO, EPOCH 7 - PROGRESS: at 91.24% examples, 193773 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:25,015, word2vec, INFO, EPOCH 7 - PROGRESS: at 96.86% examples, 194072 words/s, in_qsize 13, out_qsize 0 ]
[2024-12-13 07:07:25,453, word2vec, INFO, EPOCH 7: training on 4268692 raw words (3978477 effective words) took 20.3s, 195812 effective words/s ]
[2024-12-13 07:07:26,500, word2vec, INFO, EPOCH 8 - PROGRESS: at 3.91% examples, 151574 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:27,600, word2vec, INFO, EPOCH 8 - PROGRESS: at 9.22% examples, 173488 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:28,714, word2vec, INFO, EPOCH 8 - PROGRESS: at 14.92% examples, 182504 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:29,740, word2vec, INFO, EPOCH 8 - PROGRESS: at 20.29% examples, 188653 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:30,750, word2vec, INFO, EPOCH 8 - PROGRESS: at 25.13% examples, 189439 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:31,792, word2vec, INFO, EPOCH 8 - PROGRESS: at 30.17% examples, 190326 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:32,887, word2vec, INFO, EPOCH 8 - PROGRESS: at 35.48% examples, 190997 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:33,930, word2vec, INFO, EPOCH 8 - PROGRESS: at 40.29% examples, 190481 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:34,939, word2vec, INFO, EPOCH 8 - PROGRESS: at 45.22% examples, 190777 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:36,033, word2vec, INFO, EPOCH 8 - PROGRESS: at 50.93% examples, 192069 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:37,044, word2vec, INFO, EPOCH 8 - PROGRESS: at 56.04% examples, 192931 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:38,047, word2vec, INFO, EPOCH 8 - PROGRESS: at 60.98% examples, 193044 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:39,146, word2vec, INFO, EPOCH 8 - PROGRESS: at 66.36% examples, 193130 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:40,245, word2vec, INFO, EPOCH 8 - PROGRESS: at 71.65% examples, 193223 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:41,368, word2vec, INFO, EPOCH 8 - PROGRESS: at 77.13% examples, 193583 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:42,499, word2vec, INFO, EPOCH 8 - PROGRESS: at 82.67% examples, 193806 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:43,605, word2vec, INFO, EPOCH 8 - PROGRESS: at 88.39% examples, 194263 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:44,637, word2vec, INFO, EPOCH 8 - PROGRESS: at 93.59% examples, 194403 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:45,650, word2vec, INFO, EPOCH 8 - PROGRESS: at 98.32% examples, 193862 words/s, in_qsize 7, out_qsize 1 ]
[2024-12-13 07:07:45,884, word2vec, INFO, EPOCH 8: training on 4268692 raw words (3978453 effective words) took 20.4s, 194810 effective words/s ]
[2024-12-13 07:07:46,926, word2vec, INFO, EPOCH 9 - PROGRESS: at 3.91% examples, 151892 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:47,927, word2vec, INFO, EPOCH 9 - PROGRESS: at 8.97% examples, 177617 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 07:07:48,945, word2vec, INFO, EPOCH 9 - PROGRESS: at 13.94% examples, 182245 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:49,981, word2vec, INFO, EPOCH 9 - PROGRESS: at 19.33% examples, 188258 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:50,996, word2vec, INFO, EPOCH 9 - PROGRESS: at 23.49% examples, 183595 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:52,033, word2vec, INFO, EPOCH 9 - PROGRESS: at 28.99% examples, 188747 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:53,038, word2vec, INFO, EPOCH 9 - PROGRESS: at 33.87% examples, 189401 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:54,068, word2vec, INFO, EPOCH 9 - PROGRESS: at 38.74% examples, 189359 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:55,088, word2vec, INFO, EPOCH 9 - PROGRESS: at 44.02% examples, 191585 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:56,093, word2vec, INFO, EPOCH 9 - PROGRESS: at 49.06% examples, 191756 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:57,103, word2vec, INFO, EPOCH 9 - PROGRESS: at 53.95% examples, 191852 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:07:58,108, word2vec, INFO, EPOCH 9 - PROGRESS: at 59.08% examples, 192734 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:07:59,128, word2vec, INFO, EPOCH 9 - PROGRESS: at 64.54% examples, 193986 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:08:00,174, word2vec, INFO, EPOCH 9 - PROGRESS: at 69.80% examples, 194756 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:08:01,206, word2vec, INFO, EPOCH 9 - PROGRESS: at 74.43% examples, 193748 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 07:08:02,278, word2vec, INFO, EPOCH 9 - PROGRESS: at 79.67% examples, 194098 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:08:03,319, word2vec, INFO, EPOCH 9 - PROGRESS: at 84.78% examples, 194216 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:08:04,323, word2vec, INFO, EPOCH 9 - PROGRESS: at 90.34% examples, 195207 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:08:05,342, word2vec, INFO, EPOCH 9 - PROGRESS: at 94.70% examples, 194002 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 07:08:06,224, word2vec, INFO, EPOCH 9: training on 4268692 raw words (3977569 effective words) took 20.3s, 195631 effective words/s ]
[2024-12-13 07:08:06,226, utils, INFO, FastText lifecycle event {'msg': 'training on 42686920 raw words (39781953 effective words) took 222.7s, 178646 effective words/s', 'datetime': '2024-12-13T07:08:06.226253', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-13 07:08:14,086, utils, INFO, FastText lifecycle event {'params': 'FastText<vocab=160129, vector_size=300, alpha=0.025>', 'datetime': '2024-12-13T07:08:14.086302', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-13 07:08:14,179, keyedvectors, WARNING, destructive init_sims(replace=True) deprecated & no longer required for space-efficiency ]
[2024-12-13 07:08:14,371, keyedvectors, INFO, storing 160129x300 projection weights into ft_reviews_vectors.bin ]
[2024-12-13 07:09:35,086, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 07:12:12,752, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 08:48:11,623, keyedvectors, WARNING, attribute count not present in KeyedVectors<vector_size=300, 120619 keys>; will store in internal index_to_key order ]
[2024-12-13 08:48:11,632, keyedvectors, INFO, storing 120619x300 projection weights into gensim_my_fasttext_model.bin ]
[2024-12-13 08:48:12,895, keyedvectors, WARNING, destructive init_sims(replace=True) deprecated & no longer required for space-efficiency ]
[2024-12-13 08:48:12,931, utils, INFO, KeyedVectors lifecycle event {'fname_or_handle': 'quantized_model.kv', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-12-13T08:48:12.931768', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'} ]
[2024-12-13 08:48:12,940, utils, INFO, storing np array 'vectors' to quantized_model.kv.vectors.npy ]
[2024-12-13 08:48:13,345, utils, INFO, saved quantized_model.kv ]
[2024-12-13 08:49:25,874, utils, INFO, loading KeyedVectors object from quantized_model.kv ]
[2024-12-13 08:49:26,045, utils, INFO, loading vectors from quantized_model.kv.vectors.npy with mmap=r ]
[2024-12-13 08:49:26,068, utils, INFO, KeyedVectors lifecycle event {'fname': 'quantized_model.kv', 'datetime': '2024-12-13T08:49:26.068104', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'} ]
[2024-12-13 08:52:01,719, keyedvectors, INFO, loading projection weights from gensim_my_fasttext_model.bin ]
[2024-12-13 08:52:03,277, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (120619, 300) matrix of type float32 from gensim_my_fasttext_model.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T08:52:03.277696', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 18:03:07,296, word2vec, INFO, collecting all words and their counts ]
[2024-12-13 18:03:07,297, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-13 18:03:07,414, word2vec, INFO, PROGRESS: at sentence #10000, processed 414946 words, keeping 45101 word types ]
[2024-12-13 18:03:07,554, word2vec, INFO, PROGRESS: at sentence #20000, processed 822575 words, keeping 69280 word types ]
[2024-12-13 18:03:07,691, word2vec, INFO, PROGRESS: at sentence #30000, processed 1238200 words, keeping 90115 word types ]
[2024-12-13 18:03:07,814, word2vec, INFO, PROGRESS: at sentence #40000, processed 1650529 words, keeping 108615 word types ]
[2024-12-13 18:03:07,921, word2vec, INFO, PROGRESS: at sentence #50000, processed 2061831 words, keeping 125191 word types ]
[2024-12-13 18:03:08,026, word2vec, INFO, PROGRESS: at sentence #60000, processed 2471877 words, keeping 140808 word types ]
[2024-12-13 18:03:08,152, word2vec, INFO, PROGRESS: at sentence #70000, processed 2878895 words, keeping 155424 word types ]
[2024-12-13 18:03:08,282, word2vec, INFO, PROGRESS: at sentence #80000, processed 3298436 words, keeping 170237 word types ]
[2024-12-13 18:03:08,411, word2vec, INFO, PROGRESS: at sentence #90000, processed 3709814 words, keeping 184531 word types ]
[2024-12-13 18:03:08,532, word2vec, INFO, PROGRESS: at sentence #100000, processed 4113390 words, keeping 197645 word types ]
[2024-12-13 18:03:08,570, word2vec, INFO, collected 201881 word types from a corpus of 4244784 raw words and 103304 sentences ]
[2024-12-13 18:03:08,571, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-13 18:03:09,545, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 201881 unique words (100.00% of original 201881, drops 0)', 'datetime': '2024-12-13T18:03:09.545612', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 18:03:09,547, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4244784 word corpus (100.00% of original 4244784, drops 0)', 'datetime': '2024-12-13T18:03:09.547571', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 18:03:11,082, word2vec, INFO, deleting the raw counts dictionary of 201881 items ]
[2024-12-13 18:03:11,088, word2vec, INFO, sample=0.001 downsamples 33 most-common words ]
[2024-12-13 18:03:11,089, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 4003657.77288981 word corpus (94.3%% of prior 4244784)', 'datetime': '2024-12-13T18:03:11.089450', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 18:03:15,597, fasttext, INFO, estimated required memory for 201881 words, 2000000 buckets and 25 dimensions: 384871980 bytes ]
[2024-12-13 18:03:15,598, word2vec, INFO, resetting layer weights ]
[2024-12-13 18:03:28,881, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-13T18:03:28.881621', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]
[2024-12-13 18:03:28,883, utils, INFO, FastText lifecycle event {'msg': 'training model with 8 workers on 201881 vocabulary and 25 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-12-13T18:03:28.883581', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-13 18:03:29,909, word2vec, INFO, EPOCH 0 - PROGRESS: at 9.74% examples, 388075 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:30,913, word2vec, INFO, EPOCH 0 - PROGRESS: at 20.41% examples, 404171 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:31,952, word2vec, INFO, EPOCH 0 - PROGRESS: at 31.02% examples, 408219 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 18:03:32,990, word2vec, INFO, EPOCH 0 - PROGRESS: at 41.67% examples, 410349 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-13 18:03:33,992, word2vec, INFO, EPOCH 0 - PROGRESS: at 53.38% examples, 419886 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:34,994, word2vec, INFO, EPOCH 0 - PROGRESS: at 63.94% examples, 419896 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:36,029, word2vec, INFO, EPOCH 0 - PROGRESS: at 75.04% examples, 422225 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:37,039, word2vec, INFO, EPOCH 0 - PROGRESS: at 86.29% examples, 425277 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:38,041, word2vec, INFO, EPOCH 0 - PROGRESS: at 96.76% examples, 423841 words/s, in_qsize 14, out_qsize 0 ]
[2024-12-13 18:03:38,330, word2vec, INFO, EPOCH 0: training on 4244784 raw words (4003695 effective words) took 9.4s, 424282 effective words/s ]
[2024-12-13 18:03:39,347, word2vec, INFO, EPOCH 1 - PROGRESS: at 9.52% examples, 382334 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:40,360, word2vec, INFO, EPOCH 1 - PROGRESS: at 19.24% examples, 381037 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:41,362, word2vec, INFO, EPOCH 1 - PROGRESS: at 29.64% examples, 394598 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:42,383, word2vec, INFO, EPOCH 1 - PROGRESS: at 39.48% examples, 392498 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-13 18:03:43,383, word2vec, INFO, EPOCH 1 - PROGRESS: at 50.78% examples, 404021 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:44,383, word2vec, INFO, EPOCH 1 - PROGRESS: at 62.00% examples, 411442 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:45,394, word2vec, INFO, EPOCH 1 - PROGRESS: at 72.52% examples, 412466 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:46,403, word2vec, INFO, EPOCH 1 - PROGRESS: at 82.91% examples, 413349 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 18:03:47,408, word2vec, INFO, EPOCH 1 - PROGRESS: at 93.69% examples, 414086 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:47,876, word2vec, INFO, EPOCH 1: training on 4244784 raw words (4003130 effective words) took 9.5s, 419788 effective words/s ]
[2024-12-13 18:03:48,888, word2vec, INFO, EPOCH 2 - PROGRESS: at 9.95% examples, 402591 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:49,915, word2vec, INFO, EPOCH 2 - PROGRESS: at 21.52% examples, 425653 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:50,944, word2vec, INFO, EPOCH 2 - PROGRESS: at 32.25% examples, 423859 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:51,946, word2vec, INFO, EPOCH 2 - PROGRESS: at 43.56% examples, 432377 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 18:03:52,963, word2vec, INFO, EPOCH 2 - PROGRESS: at 54.99% examples, 434455 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:53,981, word2vec, INFO, EPOCH 2 - PROGRESS: at 66.77% examples, 438705 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:54,983, word2vec, INFO, EPOCH 2 - PROGRESS: at 77.33% examples, 437714 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:55,986, word2vec, INFO, EPOCH 2 - PROGRESS: at 88.92% examples, 440370 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:56,913, word2vec, INFO, EPOCH 2: training on 4244784 raw words (4003035 effective words) took 9.0s, 443453 effective words/s ]
[2024-12-13 18:03:57,932, word2vec, INFO, EPOCH 3 - PROGRESS: at 9.95% examples, 399345 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:58,952, word2vec, INFO, EPOCH 3 - PROGRESS: at 21.08% examples, 416084 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:03:59,953, word2vec, INFO, EPOCH 3 - PROGRESS: at 32.69% examples, 433798 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:00,958, word2vec, INFO, EPOCH 3 - PROGRESS: at 44.07% examples, 439657 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:01,962, word2vec, INFO, EPOCH 3 - PROGRESS: at 55.68% examples, 443374 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:02,965, word2vec, INFO, EPOCH 3 - PROGRESS: at 66.77% examples, 442563 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:03,989, word2vec, INFO, EPOCH 3 - PROGRESS: at 78.06% examples, 443596 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:05,019, word2vec, INFO, EPOCH 3 - PROGRESS: at 89.42% examples, 442879 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 18:04:05,921, word2vec, INFO, EPOCH 3: training on 4244784 raw words (4003700 effective words) took 9.0s, 444892 effective words/s ]
[2024-12-13 18:04:06,956, word2vec, INFO, EPOCH 4 - PROGRESS: at 9.74% examples, 383809 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:07,964, word2vec, INFO, EPOCH 4 - PROGRESS: at 21.08% examples, 415123 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:08,975, word2vec, INFO, EPOCH 4 - PROGRESS: at 32.69% examples, 431687 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:09,993, word2vec, INFO, EPOCH 4 - PROGRESS: at 44.07% examples, 436810 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:11,015, word2vec, INFO, EPOCH 4 - PROGRESS: at 55.90% examples, 441384 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:12,019, word2vec, INFO, EPOCH 4 - PROGRESS: at 66.77% examples, 439298 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:13,040, word2vec, INFO, EPOCH 4 - PROGRESS: at 78.28% examples, 442401 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:14,047, word2vec, INFO, EPOCH 4 - PROGRESS: at 89.42% examples, 441933 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:14,891, word2vec, INFO, EPOCH 4: training on 4244784 raw words (4004438 effective words) took 9.0s, 446829 effective words/s ]
[2024-12-13 18:04:15,909, word2vec, INFO, EPOCH 5 - PROGRESS: at 10.16% examples, 409539 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:16,919, word2vec, INFO, EPOCH 5 - PROGRESS: at 21.08% examples, 418324 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:17,922, word2vec, INFO, EPOCH 5 - PROGRESS: at 32.69% examples, 434998 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:18,945, word2vec, INFO, EPOCH 5 - PROGRESS: at 43.56% examples, 434016 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:19,963, word2vec, INFO, EPOCH 5 - PROGRESS: at 55.20% examples, 437591 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:20,966, word2vec, INFO, EPOCH 5 - PROGRESS: at 66.03% examples, 436227 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 18:04:21,969, word2vec, INFO, EPOCH 5 - PROGRESS: at 77.33% examples, 439511 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:22,982, word2vec, INFO, EPOCH 5 - PROGRESS: at 88.20% examples, 437966 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:23,936, word2vec, INFO, EPOCH 5: training on 4244784 raw words (4003329 effective words) took 9.0s, 443021 effective words/s ]
[2024-12-13 18:04:24,953, word2vec, INFO, EPOCH 6 - PROGRESS: at 9.95% examples, 400846 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:25,973, word2vec, INFO, EPOCH 6 - PROGRESS: at 21.08% examples, 416755 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:26,981, word2vec, INFO, EPOCH 6 - PROGRESS: at 32.69% examples, 433136 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:28,001, word2vec, INFO, EPOCH 6 - PROGRESS: at 43.07% examples, 428324 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:29,011, word2vec, INFO, EPOCH 6 - PROGRESS: at 54.99% examples, 435642 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:30,031, word2vec, INFO, EPOCH 6 - PROGRESS: at 66.27% examples, 436397 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:31,034, word2vec, INFO, EPOCH 6 - PROGRESS: at 77.59% examples, 439661 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 18:04:32,051, word2vec, INFO, EPOCH 6 - PROGRESS: at 89.42% examples, 442520 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:32,980, word2vec, INFO, EPOCH 6: training on 4244784 raw words (4003758 effective words) took 9.0s, 443150 effective words/s ]
[2024-12-13 18:04:34,005, word2vec, INFO, EPOCH 7 - PROGRESS: at 9.95% examples, 397127 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:35,014, word2vec, INFO, EPOCH 7 - PROGRESS: at 21.30% examples, 421853 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:36,024, word2vec, INFO, EPOCH 7 - PROGRESS: at 32.90% examples, 436158 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:37,030, word2vec, INFO, EPOCH 7 - PROGRESS: at 43.56% examples, 434499 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 18:04:38,036, word2vec, INFO, EPOCH 7 - PROGRESS: at 54.99% examples, 437278 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:39,052, word2vec, INFO, EPOCH 7 - PROGRESS: at 66.52% examples, 439561 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 18:04:40,066, word2vec, INFO, EPOCH 7 - PROGRESS: at 77.11% examples, 437745 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:41,080, word2vec, INFO, EPOCH 7 - PROGRESS: at 89.18% examples, 442082 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:41,987, word2vec, INFO, EPOCH 7: training on 4244784 raw words (4003606 effective words) took 9.0s, 444883 effective words/s ]
[2024-12-13 18:04:43,010, word2vec, INFO, EPOCH 8 - PROGRESS: at 9.74% examples, 389048 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 18:04:44,013, word2vec, INFO, EPOCH 8 - PROGRESS: at 21.08% examples, 419069 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:45,014, word2vec, INFO, EPOCH 8 - PROGRESS: at 32.47% examples, 432655 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:46,037, word2vec, INFO, EPOCH 8 - PROGRESS: at 44.07% examples, 439313 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:47,045, word2vec, INFO, EPOCH 8 - PROGRESS: at 54.99% examples, 437221 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:48,074, word2vec, INFO, EPOCH 8 - PROGRESS: at 66.77% examples, 440153 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:49,077, word2vec, INFO, EPOCH 8 - PROGRESS: at 77.11% examples, 437580 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:50,077, word2vec, INFO, EPOCH 8 - PROGRESS: at 88.92% examples, 441552 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:51,003, word2vec, INFO, EPOCH 8: training on 4244784 raw words (4004168 effective words) took 9.0s, 444584 effective words/s ]
[2024-12-13 18:04:52,029, word2vec, INFO, EPOCH 9 - PROGRESS: at 10.16% examples, 406404 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:53,038, word2vec, INFO, EPOCH 9 - PROGRESS: at 21.30% examples, 421883 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:54,049, word2vec, INFO, EPOCH 9 - PROGRESS: at 32.25% examples, 426826 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:55,071, word2vec, INFO, EPOCH 9 - PROGRESS: at 43.81% examples, 434881 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:56,102, word2vec, INFO, EPOCH 9 - PROGRESS: at 54.99% examples, 433525 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:57,140, word2vec, INFO, EPOCH 9 - PROGRESS: at 66.77% examples, 436502 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:58,150, word2vec, INFO, EPOCH 9 - PROGRESS: at 77.59% examples, 436604 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 18:04:59,158, word2vec, INFO, EPOCH 9 - PROGRESS: at 88.92% examples, 437953 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-13 18:05:00,143, word2vec, INFO, EPOCH 9: training on 4244784 raw words (4003757 effective words) took 9.1s, 438486 effective words/s ]
[2024-12-13 18:05:00,144, utils, INFO, FastText lifecycle event {'msg': 'training on 42447840 raw words (40036616 effective words) took 91.3s, 438710 effective words/s', 'datetime': '2024-12-13T18:05:00.144110', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-13 18:05:10,033, utils, INFO, FastText lifecycle event {'params': 'FastText<vocab=201881, vector_size=25, alpha=0.025>', 'datetime': '2024-12-13T18:05:10.033708', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-13 18:05:10,048, keyedvectors, WARNING, destructive init_sims(replace=True) deprecated & no longer required for space-efficiency ]
[2024-12-13 18:05:10,250, keyedvectors, INFO, storing 201881x25 projection weights into ft_reviews_vectors.bin ]
[2024-12-13 18:07:35,667, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 18:07:38,241, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T18:07:38.241870', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 20:11:44,671, word2vec, INFO, collecting all words and their counts ]
[2024-12-13 20:11:44,673, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-13 20:11:44,800, word2vec, INFO, PROGRESS: at sentence #10000, processed 414946 words, keeping 45101 word types ]
[2024-12-13 20:11:44,886, word2vec, INFO, PROGRESS: at sentence #20000, processed 822575 words, keeping 69280 word types ]
[2024-12-13 20:11:44,987, word2vec, INFO, PROGRESS: at sentence #30000, processed 1238200 words, keeping 90115 word types ]
[2024-12-13 20:11:45,079, word2vec, INFO, PROGRESS: at sentence #40000, processed 1650529 words, keeping 108615 word types ]
[2024-12-13 20:11:45,167, word2vec, INFO, PROGRESS: at sentence #50000, processed 2061831 words, keeping 125191 word types ]
[2024-12-13 20:11:45,264, word2vec, INFO, PROGRESS: at sentence #60000, processed 2471877 words, keeping 140808 word types ]
[2024-12-13 20:11:45,375, word2vec, INFO, PROGRESS: at sentence #70000, processed 2878895 words, keeping 155424 word types ]
[2024-12-13 20:11:45,476, word2vec, INFO, PROGRESS: at sentence #80000, processed 3298436 words, keeping 170237 word types ]
[2024-12-13 20:11:45,576, word2vec, INFO, PROGRESS: at sentence #90000, processed 3709814 words, keeping 184531 word types ]
[2024-12-13 20:11:45,687, word2vec, INFO, PROGRESS: at sentence #100000, processed 4113390 words, keeping 197645 word types ]
[2024-12-13 20:11:45,789, word2vec, INFO, collected 201881 word types from a corpus of 4244784 raw words and 103304 sentences ]
[2024-12-13 20:11:45,790, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-13 20:11:46,666, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 201881 unique words (100.00% of original 201881, drops 0)', 'datetime': '2024-12-13T20:11:46.666338', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 20:11:46,668, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4244784 word corpus (100.00% of original 4244784, drops 0)', 'datetime': '2024-12-13T20:11:46.668332', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 20:11:47,948, word2vec, INFO, deleting the raw counts dictionary of 201881 items ]
[2024-12-13 20:11:47,952, word2vec, INFO, sample=0.001 downsamples 33 most-common words ]
[2024-12-13 20:11:47,954, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 4003657.77288981 word corpus (94.3%% of prior 4244784)', 'datetime': '2024-12-13T20:11:47.954891', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-13 20:11:51,831, fasttext, INFO, estimated required memory for 201881 words, 2000000 buckets and 50 dimensions: 625248180 bytes ]
[2024-12-13 20:11:51,832, word2vec, INFO, resetting layer weights ]
[2024-12-13 20:12:03,285, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-13T20:12:03.285585', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]
[2024-12-13 20:12:03,287, utils, INFO, FastText lifecycle event {'msg': 'training model with 8 workers on 201881 vocabulary and 50 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-13T20:12:03.287552', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-13 20:12:04,317, word2vec, INFO, EPOCH 0 - PROGRESS: at 4.42% examples, 176204 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:05,368, word2vec, INFO, EPOCH 0 - PROGRESS: at 9.74% examples, 191293 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:06,378, word2vec, INFO, EPOCH 0 - PROGRESS: at 16.20% examples, 210967 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:07,407, word2vec, INFO, EPOCH 0 - PROGRESS: at 21.08% examples, 206036 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:08,412, word2vec, INFO, EPOCH 0 - PROGRESS: at 26.60% examples, 209695 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:09,418, word2vec, INFO, EPOCH 0 - PROGRESS: at 32.25% examples, 212050 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:10,458, word2vec, INFO, EPOCH 0 - PROGRESS: at 38.75% examples, 217947 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:11,480, word2vec, INFO, EPOCH 0 - PROGRESS: at 44.28% examples, 218266 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:12,501, word2vec, INFO, EPOCH 0 - PROGRESS: at 50.56% examples, 220607 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:13,501, word2vec, INFO, EPOCH 0 - PROGRESS: at 56.16% examples, 221033 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:14,641, word2vec, INFO, EPOCH 0 - PROGRESS: at 62.00% examples, 219398 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:15,653, word2vec, INFO, EPOCH 0 - PROGRESS: at 67.91% examples, 220435 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:16,679, word2vec, INFO, EPOCH 0 - PROGRESS: at 73.25% examples, 219682 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:12:17,740, word2vec, INFO, EPOCH 0 - PROGRESS: at 79.43% examples, 221150 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:18,746, word2vec, INFO, EPOCH 0 - PROGRESS: at 85.04% examples, 221356 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:19,753, word2vec, INFO, EPOCH 0 - PROGRESS: at 90.87% examples, 221495 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:20,776, word2vec, INFO, EPOCH 0 - PROGRESS: at 95.82% examples, 219817 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:21,448, word2vec, INFO, EPOCH 0: training on 4244784 raw words (4003558 effective words) took 18.1s, 220688 effective words/s ]
[2024-12-13 20:12:22,458, word2vec, INFO, EPOCH 1 - PROGRESS: at 4.42% examples, 177746 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:23,465, word2vec, INFO, EPOCH 1 - PROGRESS: at 9.95% examples, 201066 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:24,488, word2vec, INFO, EPOCH 1 - PROGRESS: at 16.62% examples, 220015 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:25,523, word2vec, INFO, EPOCH 1 - PROGRESS: at 22.46% examples, 221609 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:26,675, word2vec, INFO, EPOCH 1 - PROGRESS: at 28.25% examples, 217645 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:27,682, word2vec, INFO, EPOCH 1 - PROGRESS: at 34.53% examples, 223183 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:28,691, word2vec, INFO, EPOCH 1 - PROGRESS: at 40.54% examples, 225875 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:29,722, word2vec, INFO, EPOCH 1 - PROGRESS: at 46.77% examples, 227199 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:30,791, word2vec, INFO, EPOCH 1 - PROGRESS: at 52.69% examples, 226379 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:31,824, word2vec, INFO, EPOCH 1 - PROGRESS: at 58.47% examples, 226308 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:32,849, word2vec, INFO, EPOCH 1 - PROGRESS: at 64.19% examples, 225677 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:33,869, word2vec, INFO, EPOCH 1 - PROGRESS: at 70.67% examples, 228360 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:34,885, word2vec, INFO, EPOCH 1 - PROGRESS: at 76.18% examples, 227892 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:12:35,936, word2vec, INFO, EPOCH 1 - PROGRESS: at 82.19% examples, 228243 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:36,997, word2vec, INFO, EPOCH 1 - PROGRESS: at 88.44% examples, 228384 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:38,006, word2vec, INFO, EPOCH 1 - PROGRESS: at 94.13% examples, 228064 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:38,834, word2vec, INFO, EPOCH 1: training on 4244784 raw words (4003644 effective words) took 17.4s, 230395 effective words/s ]
[2024-12-13 20:12:39,844, word2vec, INFO, EPOCH 2 - PROGRESS: at 5.56% examples, 225038 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:41,047, word2vec, INFO, EPOCH 2 - PROGRESS: at 11.39% examples, 208769 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:12:42,056, word2vec, INFO, EPOCH 2 - PROGRESS: at 17.59% examples, 219291 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:43,072, word2vec, INFO, EPOCH 2 - PROGRESS: at 23.62% examples, 224248 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:44,079, word2vec, INFO, EPOCH 2 - PROGRESS: at 29.42% examples, 225921 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:45,108, word2vec, INFO, EPOCH 2 - PROGRESS: at 35.25% examples, 226349 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:46,181, word2vec, INFO, EPOCH 2 - PROGRESS: at 41.22% examples, 226564 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:47,193, word2vec, INFO, EPOCH 2 - PROGRESS: at 46.99% examples, 226067 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:48,255, word2vec, INFO, EPOCH 2 - PROGRESS: at 53.63% examples, 228533 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:49,259, word2vec, INFO, EPOCH 2 - PROGRESS: at 59.20% examples, 227979 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:50,351, word2vec, INFO, EPOCH 2 - PROGRESS: at 65.36% examples, 227499 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:12:51,411, word2vec, INFO, EPOCH 2 - PROGRESS: at 70.87% examples, 226260 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:52,433, word2vec, INFO, EPOCH 2 - PROGRESS: at 76.67% examples, 226553 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-13 20:12:53,458, word2vec, INFO, EPOCH 2 - PROGRESS: at 82.68% examples, 227416 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:54,465, word2vec, INFO, EPOCH 2 - PROGRESS: at 88.68% examples, 227777 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:12:55,500, word2vec, INFO, EPOCH 2 - PROGRESS: at 94.57% examples, 227712 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:12:56,299, word2vec, INFO, EPOCH 2: training on 4244784 raw words (4003444 effective words) took 17.5s, 229345 effective words/s ]
[2024-12-13 20:12:57,347, word2vec, INFO, EPOCH 3 - PROGRESS: at 5.35% examples, 207545 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:58,370, word2vec, INFO, EPOCH 3 - PROGRESS: at 10.86% examples, 214059 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:12:59,395, word2vec, INFO, EPOCH 3 - PROGRESS: at 16.62% examples, 216032 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:00,456, word2vec, INFO, EPOCH 3 - PROGRESS: at 22.22% examples, 214997 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:01,485, word2vec, INFO, EPOCH 3 - PROGRESS: at 27.79% examples, 215781 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:02,498, word2vec, INFO, EPOCH 3 - PROGRESS: at 33.62% examples, 218459 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:03,593, word2vec, INFO, EPOCH 3 - PROGRESS: at 39.26% examples, 216577 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:04,619, word2vec, INFO, EPOCH 3 - PROGRESS: at 44.77% examples, 216966 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:05,667, word2vec, INFO, EPOCH 3 - PROGRESS: at 50.81% examples, 217723 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 20:13:06,674, word2vec, INFO, EPOCH 3 - PROGRESS: at 56.39% examples, 218291 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:07,773, word2vec, INFO, EPOCH 3 - PROGRESS: at 62.50% examples, 218521 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:08,803, word2vec, INFO, EPOCH 3 - PROGRESS: at 67.91% examples, 217807 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:09,806, word2vec, INFO, EPOCH 3 - PROGRESS: at 73.45% examples, 218361 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:10,832, word2vec, INFO, EPOCH 3 - PROGRESS: at 79.43% examples, 219787 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:11,839, word2vec, INFO, EPOCH 3 - PROGRESS: at 85.04% examples, 220072 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:12,895, word2vec, INFO, EPOCH 3 - PROGRESS: at 91.30% examples, 220760 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:13,910, word2vec, INFO, EPOCH 3 - PROGRESS: at 96.56% examples, 219772 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:14,446, word2vec, INFO, EPOCH 3: training on 4244784 raw words (4003791 effective words) took 18.1s, 220747 effective words/s ]
[2024-12-13 20:13:15,457, word2vec, INFO, EPOCH 4 - PROGRESS: at 4.87% examples, 196226 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:16,467, word2vec, INFO, EPOCH 4 - PROGRESS: at 9.95% examples, 200662 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:17,490, word2vec, INFO, EPOCH 4 - PROGRESS: at 16.44% examples, 216551 words/s, in_qsize 13, out_qsize 2 ]
[2024-12-13 20:13:18,509, word2vec, INFO, EPOCH 4 - PROGRESS: at 22.46% examples, 222208 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:19,561, word2vec, INFO, EPOCH 4 - PROGRESS: at 28.03% examples, 220580 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:20,610, word2vec, INFO, EPOCH 4 - PROGRESS: at 33.62% examples, 219666 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:21,622, word2vec, INFO, EPOCH 4 - PROGRESS: at 39.91% examples, 224027 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:13:22,637, word2vec, INFO, EPOCH 4 - PROGRESS: at 45.76% examples, 224899 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:23,638, word2vec, INFO, EPOCH 4 - PROGRESS: at 51.48% examples, 224921 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:24,672, word2vec, INFO, EPOCH 4 - PROGRESS: at 57.26% examples, 225088 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:13:25,700, word2vec, INFO, EPOCH 4 - PROGRESS: at 62.50% examples, 222775 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:26,721, word2vec, INFO, EPOCH 4 - PROGRESS: at 67.91% examples, 221856 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:27,725, word2vec, INFO, EPOCH 4 - PROGRESS: at 73.92% examples, 223513 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:13:28,735, word2vec, INFO, EPOCH 4 - PROGRESS: at 79.89% examples, 224827 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:29,782, word2vec, INFO, EPOCH 4 - PROGRESS: at 85.79% examples, 224803 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:30,815, word2vec, INFO, EPOCH 4 - PROGRESS: at 91.56% examples, 224360 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:31,835, word2vec, INFO, EPOCH 4 - PROGRESS: at 97.48% examples, 224729 words/s, in_qsize 11, out_qsize 0 ]
[2024-12-13 20:13:32,228, word2vec, INFO, EPOCH 4: training on 4244784 raw words (4003421 effective words) took 17.8s, 225264 effective words/s ]
[2024-12-13 20:13:33,239, word2vec, INFO, EPOCH 5 - PROGRESS: at 4.65% examples, 187234 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 20:13:34,276, word2vec, INFO, EPOCH 5 - PROGRESS: at 10.16% examples, 202732 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:35,283, word2vec, INFO, EPOCH 5 - PROGRESS: at 16.44% examples, 215949 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:36,292, word2vec, INFO, EPOCH 5 - PROGRESS: at 21.98% examples, 217660 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:37,315, word2vec, INFO, EPOCH 5 - PROGRESS: at 27.55% examples, 218189 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:38,344, word2vec, INFO, EPOCH 5 - PROGRESS: at 33.16% examples, 218357 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:39,352, word2vec, INFO, EPOCH 5 - PROGRESS: at 39.26% examples, 221738 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:40,365, word2vec, INFO, EPOCH 5 - PROGRESS: at 45.02% examples, 222981 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:13:41,367, word2vec, INFO, EPOCH 5 - PROGRESS: at 50.30% examples, 221119 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:42,377, word2vec, INFO, EPOCH 5 - PROGRESS: at 56.16% examples, 222259 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:43,399, word2vec, INFO, EPOCH 5 - PROGRESS: at 62.00% examples, 222788 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:44,438, word2vec, INFO, EPOCH 5 - PROGRESS: at 67.68% examples, 222291 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:13:45,480, word2vec, INFO, EPOCH 5 - PROGRESS: at 73.70% examples, 223265 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:46,563, word2vec, INFO, EPOCH 5 - PROGRESS: at 79.23% examples, 222156 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:47,578, word2vec, INFO, EPOCH 5 - PROGRESS: at 85.30% examples, 223401 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:48,652, word2vec, INFO, EPOCH 5 - PROGRESS: at 90.63% examples, 221346 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:49,689, word2vec, INFO, EPOCH 5 - PROGRESS: at 96.06% examples, 220574 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:50,262, word2vec, INFO, EPOCH 5: training on 4244784 raw words (4003332 effective words) took 18.0s, 222112 effective words/s ]
[2024-12-13 20:13:51,288, word2vec, INFO, EPOCH 6 - PROGRESS: at 4.65% examples, 184651 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:52,301, word2vec, INFO, EPOCH 6 - PROGRESS: at 9.95% examples, 199092 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:53,336, word2vec, INFO, EPOCH 6 - PROGRESS: at 16.20% examples, 211537 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:54,363, word2vec, INFO, EPOCH 6 - PROGRESS: at 21.52% examples, 211118 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:55,422, word2vec, INFO, EPOCH 6 - PROGRESS: at 27.55% examples, 215119 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:56,462, word2vec, INFO, EPOCH 6 - PROGRESS: at 33.16% examples, 215414 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:57,552, word2vec, INFO, EPOCH 6 - PROGRESS: at 38.75% examples, 214105 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:13:58,601, word2vec, INFO, EPOCH 6 - PROGRESS: at 44.28% examples, 214217 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:13:59,646, word2vec, INFO, EPOCH 6 - PROGRESS: at 50.35% examples, 215384 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:00,723, word2vec, INFO, EPOCH 6 - PROGRESS: at 55.93% examples, 214714 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:01,743, word2vec, INFO, EPOCH 6 - PROGRESS: at 61.55% examples, 215143 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:02,781, word2vec, INFO, EPOCH 6 - PROGRESS: at 67.68% examples, 216817 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:14:03,852, word2vec, INFO, EPOCH 6 - PROGRESS: at 73.25% examples, 216347 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:04,900, word2vec, INFO, EPOCH 6 - PROGRESS: at 78.53% examples, 215642 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:05,952, word2vec, INFO, EPOCH 6 - PROGRESS: at 84.32% examples, 216150 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:07,009, word2vec, INFO, EPOCH 6 - PROGRESS: at 89.89% examples, 215407 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:08,023, word2vec, INFO, EPOCH 6 - PROGRESS: at 95.82% examples, 216346 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:08,725, word2vec, INFO, EPOCH 6: training on 4244784 raw words (4003633 effective words) took 18.5s, 216984 effective words/s ]
[2024-12-13 20:14:09,737, word2vec, INFO, EPOCH 7 - PROGRESS: at 4.62% examples, 187037 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-13 20:14:10,751, word2vec, INFO, EPOCH 7 - PROGRESS: at 9.73% examples, 195620 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:11,755, word2vec, INFO, EPOCH 7 - PROGRESS: at 15.94% examples, 211511 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:12,787, word2vec, INFO, EPOCH 7 - PROGRESS: at 21.31% examples, 210819 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:13,788, word2vec, INFO, EPOCH 7 - PROGRESS: at 26.81% examples, 213702 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:14,797, word2vec, INFO, EPOCH 7 - PROGRESS: at 32.00% examples, 212273 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:15,799, word2vec, INFO, EPOCH 7 - PROGRESS: at 37.32% examples, 212679 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:16,828, word2vec, INFO, EPOCH 7 - PROGRESS: at 43.31% examples, 215837 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:14:17,884, word2vec, INFO, EPOCH 7 - PROGRESS: at 48.88% examples, 214477 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:18,920, word2vec, INFO, EPOCH 7 - PROGRESS: at 54.31% examples, 213870 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:14:19,971, word2vec, INFO, EPOCH 7 - PROGRESS: at 60.35% examples, 215487 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:20,986, word2vec, INFO, EPOCH 7 - PROGRESS: at 66.03% examples, 216014 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:21,990, word2vec, INFO, EPOCH 7 - PROGRESS: at 71.84% examples, 217362 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:22,997, word2vec, INFO, EPOCH 7 - PROGRESS: at 77.11% examples, 217212 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:24,009, word2vec, INFO, EPOCH 7 - PROGRESS: at 82.43% examples, 216979 words/s, in_qsize 13, out_qsize 2 ]
[2024-12-13 20:14:25,010, word2vec, INFO, EPOCH 7 - PROGRESS: at 87.94% examples, 216907 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:26,017, word2vec, INFO, EPOCH 7 - PROGRESS: at 93.90% examples, 217845 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:26,958, word2vec, INFO, EPOCH 7: training on 4244784 raw words (4003443 effective words) took 18.2s, 219683 effective words/s ]
[2024-12-13 20:14:27,975, word2vec, INFO, EPOCH 8 - PROGRESS: at 4.40% examples, 176770 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:29,053, word2vec, INFO, EPOCH 8 - PROGRESS: at 9.95% examples, 193550 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:30,117, word2vec, INFO, EPOCH 8 - PROGRESS: at 15.71% examples, 199697 words/s, in_qsize 15, out_qsize 1 ]
[2024-12-13 20:14:31,140, word2vec, INFO, EPOCH 8 - PROGRESS: at 21.30% examples, 204647 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:32,185, word2vec, INFO, EPOCH 8 - PROGRESS: at 27.55% examples, 212250 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:33,187, word2vec, INFO, EPOCH 8 - PROGRESS: at 32.90% examples, 212807 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:34,227, word2vec, INFO, EPOCH 8 - PROGRESS: at 39.00% examples, 215946 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:35,247, word2vec, INFO, EPOCH 8 - PROGRESS: at 43.56% examples, 212035 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:36,248, word2vec, INFO, EPOCH 8 - PROGRESS: at 49.37% examples, 213450 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:37,317, word2vec, INFO, EPOCH 8 - PROGRESS: at 55.68% examples, 215892 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:38,327, word2vec, INFO, EPOCH 8 - PROGRESS: at 61.07% examples, 215586 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:39,364, word2vec, INFO, EPOCH 8 - PROGRESS: at 67.23% examples, 217222 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:40,368, word2vec, INFO, EPOCH 8 - PROGRESS: at 72.06% examples, 215687 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:41,389, word2vec, INFO, EPOCH 8 - PROGRESS: at 77.81% examples, 216724 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:42,439, word2vec, INFO, EPOCH 8 - PROGRESS: at 83.37% examples, 216605 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:43,469, word2vec, INFO, EPOCH 8 - PROGRESS: at 89.64% examples, 217882 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:44,487, word2vec, INFO, EPOCH 8 - PROGRESS: at 95.06% examples, 217559 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:45,271, word2vec, INFO, EPOCH 8: training on 4244784 raw words (4003219 effective words) took 18.3s, 218709 effective words/s ]
[2024-12-13 20:14:46,310, word2vec, INFO, EPOCH 9 - PROGRESS: at 3.94% examples, 154729 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:47,337, word2vec, INFO, EPOCH 9 - PROGRESS: at 9.49% examples, 187215 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:48,363, word2vec, INFO, EPOCH 9 - PROGRESS: at 15.26% examples, 197970 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:49,415, word2vec, INFO, EPOCH 9 - PROGRESS: at 20.88% examples, 201987 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:14:50,468, word2vec, INFO, EPOCH 9 - PROGRESS: at 26.36% examples, 204457 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:51,571, word2vec, INFO, EPOCH 9 - PROGRESS: at 32.00% examples, 204502 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:52,579, word2vec, INFO, EPOCH 9 - PROGRESS: at 37.54% examples, 207095 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:53,588, word2vec, INFO, EPOCH 9 - PROGRESS: at 43.07% examples, 209087 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:54,644, word2vec, INFO, EPOCH 9 - PROGRESS: at 48.89% examples, 209546 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:55,653, word2vec, INFO, EPOCH 9 - PROGRESS: at 54.53% examples, 210924 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-13 20:14:56,676, word2vec, INFO, EPOCH 9 - PROGRESS: at 59.90% examples, 210814 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-13 20:14:57,721, word2vec, INFO, EPOCH 9 - PROGRESS: at 65.82% examples, 211922 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:58,727, word2vec, INFO, EPOCH 9 - PROGRESS: at 71.10% examples, 212181 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:14:59,768, word2vec, INFO, EPOCH 9 - PROGRESS: at 76.89% examples, 213153 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:15:00,786, word2vec, INFO, EPOCH 9 - PROGRESS: at 82.18% examples, 213111 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:15:01,833, word2vec, INFO, EPOCH 9 - PROGRESS: at 88.20% examples, 213814 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:15:02,874, word2vec, INFO, EPOCH 9 - PROGRESS: at 93.90% examples, 213965 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-13 20:15:03,798, word2vec, INFO, EPOCH 9: training on 4244784 raw words (4003256 effective words) took 18.5s, 216185 effective words/s ]
[2024-12-13 20:15:03,799, utils, INFO, FastText lifecycle event {'msg': 'training on 42447840 raw words (40034741 effective words) took 180.5s, 221786 effective words/s', 'datetime': '2024-12-13T20:15:03.799478', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-13 20:15:13,039, utils, INFO, FastText lifecycle event {'params': 'FastText<vocab=201881, vector_size=50, alpha=0.025>', 'datetime': '2024-12-13T20:15:13.039736', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-13 20:15:13,071, keyedvectors, WARNING, destructive init_sims(replace=True) deprecated & no longer required for space-efficiency ]
[2024-12-13 20:15:13,273, keyedvectors, INFO, storing 201881x50 projection weights into ft_reviews_vectors.bin ]
[2024-12-13 20:16:41,492, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 20:16:43,943, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T20:16:43.943035', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 21:34:37,365, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 21:34:37,367, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 21:34:37,371, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 21:34:37,372, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 21:34:38,631, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 21:36:12,166, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 21:36:12,166, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 21:36:12,169, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 21:36:12,170, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 21:36:12,601, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 21:36:51,274, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 21:36:51,276, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 21:36:51,278, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 21:36:51,280, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 21:36:51,614, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 21:36:53,122, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 21:36:53,124, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 21:36:54,603, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 21:36:54,635, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-13 21:57:31,608, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 21:57:31,610, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 21:57:31,613, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 21:57:31,615, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 21:57:31,945, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 21:57:33,416, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 21:57:33,418, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 21:57:35,127, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 21:59:16,121, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 21:59:16,123, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 21:59:16,124, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 21:59:16,125, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 21:59:16,518, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 21:59:18,086, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 21:59:18,087, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 21:59:19,814, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 21:59:19,846, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-13 21:59:19,847, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-13 21:59:21,214, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 21:59:21,218, data_transformation, INFO, Error in initiating the data transformation pipeline __init__() should return None, not 'remove_stop_words' ]
[2024-12-13 22:01:57,000, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:01:57,002, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:01:57,004, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:01:57,006, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:01:57,347, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:01:58,915, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:01:58,916, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:02:00,664, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:02:36,126, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:02:36,127, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:02:36,130, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:02:36,132, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:02:36,450, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:02:37,886, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:02:37,887, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:02:39,654, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:04:29,583, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:04:29,585, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:04:29,586, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:04:29,588, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:04:29,972, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:04:31,756, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:04:31,757, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:04:33,422, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:04:33,454, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-13 22:12:28,530, 882292963, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:12:28,532, 882292963, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:12:28,534, 882292963, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:12:28,883, 882292963, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:13:02,110, 1772344484, INFO, Initiating data ingestion ]
[2024-12-13 22:13:02,111, 882292963, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:13:02,113, 882292963, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:13:02,115, 882292963, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:13:02,461, 882292963, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:13:18,097, 3806098344, INFO, Initiating data ingestion ]
[2024-12-13 22:13:18,099, 882292963, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:13:18,101, 882292963, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:13:18,102, 882292963, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:13:18,439, 882292963, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:13:20,028, 3806098344, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:13:20,029, 3806098344, INFO, Initiating train test split ]
[2024-12-13 22:13:21,763, 3806098344, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:14:07,876, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:14:07,877, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:14:07,880, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:14:07,881, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:14:08,255, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:14:09,961, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:14:09,962, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:14:11,611, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:14:11,644, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-13 22:14:59,118, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:14:59,120, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:14:59,121, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:14:59,123, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:14:59,445, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:15:00,889, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:15:00,889, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:15:02,493, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:15:02,526, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-13 22:16:26,607, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:16:26,609, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:16:26,610, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:16:26,611, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:16:27,004, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:16:28,528, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:16:28,529, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:16:30,240, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:16:30,272, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-13 22:16:30,273, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:16:31,794, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:16:31,798, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:16:31,799, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:16:31,800, utils, INFO, loading KeyedVectors object from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2024-12-13 22:16:31,814, data_transformation, INFO, Error loading Gensim Word2Vec model: unpickling stack underflow ]
[2024-12-13 22:16:31,816, data_transformation, INFO, Error in creating the preprocessing pipeline : unpickling stack underflow ]
[2024-12-13 22:16:31,817, data_transformation, INFO, Error in initiating the data transformation pipeline unpickling stack underflow ]
[2024-12-13 22:17:47,939, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:17:47,940, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:17:47,942, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:17:47,944, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:17:48,275, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:17:49,954, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:17:49,956, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:17:51,687, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:17:51,716, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-13 22:17:51,717, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:17:53,195, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:17:53,196, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:17:53,198, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:17:53,199, utils, INFO, loading KeyedVectors object from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2024-12-13 22:17:53,200, data_transformation, INFO, Error loading Gensim Word2Vec model: unpickling stack underflow ]
[2024-12-13 22:17:53,201, data_transformation, INFO, Error in creating the preprocessing pipeline : unpickling stack underflow ]
[2024-12-13 22:17:53,203, data_transformation, INFO, Error in initiating the data transformation pipeline unpickling stack underflow ]
[2024-12-13 22:19:55,247, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:19:55,249, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:19:55,252, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:19:55,252, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:19:55,660, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:19:57,458, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:19:57,459, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:19:59,236, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:19:59,260, 4064193279, INFO, Initiating the DataTransformation ]
[2024-12-13 22:20:18,089, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:20:18,090, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:20:18,092, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:20:18,093, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:20:18,447, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:20:19,924, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:20:19,926, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:20:21,646, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:20:21,672, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:20:21,674, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:20:23,176, 308188128, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:20:23,178, 2999778935, INFO, Error in initiating the data transformation pipeline __init__() should return None, not 'remove_stop_words' ]
[2024-12-13 22:21:19,331, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:21:19,333, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:21:19,334, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:21:19,335, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:21:19,737, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:21:21,375, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:21:21,376, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:21:22,825, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:21:22,854, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:21:22,856, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:21:24,378, 4224321577, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:21:24,380, 4224321577, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:21:24,381, 4224321577, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:21:24,381, utils, INFO, loading KeyedVectors object from ft_reviews_vectors.model ]
[2024-12-13 22:21:24,383, 4224321577, INFO, Error loading Gensim Word2Vec model: [Errno 2] No such file or directory: 'ft_reviews_vectors.model' ]
[2024-12-13 22:21:24,383, 2999778935, INFO, Error in initiating the data transformation pipeline [Errno 2] No such file or directory: 'ft_reviews_vectors.model' ]
[2024-12-13 22:21:58,290, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:21:58,292, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:21:58,293, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:21:58,295, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:21:58,640, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:22:00,200, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:22:00,201, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:22:01,697, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:22:01,725, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:22:01,726, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:22:03,271, 2030179346, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:22:03,273, 2030179346, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:22:03,274, 2030179346, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:22:03,275, utils, INFO, loading KeyedVectors object from ft_reviews_vectors.bin ]
[2024-12-13 22:22:03,277, 2030179346, INFO, Error loading Gensim Word2Vec model: unpickling stack underflow ]
[2024-12-13 22:22:03,277, 2999778935, INFO, Error in initiating the data transformation pipeline unpickling stack underflow ]
[2024-12-13 22:23:04,230, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:23:04,232, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:23:04,233, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:23:04,235, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:23:04,567, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:23:06,150, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:23:06,151, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:23:07,908, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:23:07,932, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:23:07,934, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:23:09,415, 3260265656, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:23:09,416, 3260265656, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:23:09,417, 3260265656, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:23:09,419, 3260265656, INFO, Error loading Gensim Word2Vec model: load() got an unexpected keyword argument 'binary' ]
[2024-12-13 22:23:09,420, 2999778935, INFO, Error in initiating the data transformation pipeline load() got an unexpected keyword argument 'binary' ]
[2024-12-13 22:24:14,717, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:24:14,718, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:24:14,720, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:24:14,721, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:24:15,138, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:24:16,830, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:24:16,831, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:24:18,311, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:24:18,335, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:24:18,337, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:24:19,614, 1131251720, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:24:19,615, 1131251720, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:24:19,616, 1131251720, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:24:19,617, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:24:21,957, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:24:21.957954', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:24:21,958, 2999778935, INFO, Numerical and text pipelines created ]
[2024-12-13 22:24:21,968, 1131251720, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:24:21,971, 1131251720, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:24:21,973, 1131251720, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:24:21,974, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:24:24,007, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:24:24.007432', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:24:24,032, 1131251720, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:24:24,039, 1131251720, INFO, Error in pre-processing the text: 'Series' object has no attribute 'lower' ]
[2024-12-13 22:24:24,040, 2999778935, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'lower' ]
[2024-12-13 22:26:36,712, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:26:36,713, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:26:36,716, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:26:36,717, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:26:37,055, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:26:38,690, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:26:38,691, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:26:40,169, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:26:40,192, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:26:40,192, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:26:41,675, 4064944957, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:26:41,677, 4064944957, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:26:41,678, 4064944957, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:26:41,678, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:26:44,227, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:26:44.227965', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:26:44,229, 2999778935, INFO, Numerical and text pipelines created ]
[2024-12-13 22:26:44,233, 4064944957, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:26:44,235, 4064944957, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:26:44,236, 4064944957, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:26:44,237, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:26:46,508, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:26:46.508872', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:26:46,526, 4064944957, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:26:46,527, 4064944957, INFO, Error in pre-processing the text: 'DataFrame' object has no attribute 'str' ]
[2024-12-13 22:26:46,528, 2999778935, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'str' ]
[2024-12-13 22:27:08,736, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:27:08,738, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:27:08,740, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:27:08,741, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:27:09,162, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:27:10,755, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:27:10,756, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:27:14,416, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:27:14,435, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:27:14,436, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:27:15,731, 110027380, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:27:15,732, 110027380, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:27:15,733, 110027380, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:27:15,734, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:27:19,490, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:27:19.490049', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:27:19,491, 2999778935, INFO, Numerical and text pipelines created ]
[2024-12-13 22:27:19,495, 110027380, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:27:19,497, 110027380, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:27:19,498, 110027380, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:27:19,499, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:27:21,539, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:27:21.539103', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:27:21,556, 110027380, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:27:21,667, 110027380, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:27:21,669, 110027380, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:27:21,670, 110027380, INFO, Error in pre-processing the text: 'Series' object has no attribute 'split' ]
[2024-12-13 22:27:21,671, 2999778935, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'split' ]
[2024-12-13 22:27:49,751, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:27:49,753, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:27:49,755, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:27:49,756, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:27:50,107, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:27:52,628, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:27:52,631, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:27:54,704, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:27:54,725, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:27:54,726, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:27:56,034, 3199597980, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:27:56,036, 3199597980, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:27:56,036, 3199597980, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:27:56,037, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:27:58,191, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:27:58.191380', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:27:58,192, 2999778935, INFO, Numerical and text pipelines created ]
[2024-12-13 22:27:58,194, 3199597980, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:27:58,196, 3199597980, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:27:58,197, 3199597980, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:27:58,199, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:28:00,322, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:28:00.322681', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:28:00,344, 3199597980, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:28:00,481, 3199597980, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:28:00,482, 3199597980, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:28:01,822, 3199597980, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:28:01,824, 3199597980, INFO, Removing punctuations from text ]
[2024-12-13 22:28:01,825, 3199597980, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-13 22:28:01,825, 2999778935, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-13 22:32:07,308, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:32:07,311, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:32:07,312, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:32:07,313, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:32:07,628, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:32:09,141, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:32:09,142, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:32:10,664, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:32:10,686, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:32:10,688, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:32:12,217, 2935546167, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:32:12,218, 2935546167, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:32:12,219, 2935546167, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:32:12,220, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:32:14,618, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:32:14.618451', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:32:14,619, 2999778935, INFO, Numerical and text pipelines created ]
[2024-12-13 22:32:14,623, 2935546167, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:32:14,624, 2935546167, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:32:14,626, 2935546167, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:32:14,627, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:32:16,825, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:32:16.825572', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:32:16,841, 2935546167, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:32:16,843, 2935546167, INFO, Error in pre-processing the text: 'DataFrame' object has no attribute 'str' ]
[2024-12-13 22:32:16,844, 2999778935, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'str' ]
[2024-12-13 22:35:08,652, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:35:08,654, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:35:08,656, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:35:08,657, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:35:08,998, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:35:10,587, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:35:10,589, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:35:12,209, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:35:12,232, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:35:12,233, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:35:13,797, 967265483, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:35:13,799, 967265483, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:35:13,800, 967265483, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:35:13,801, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:35:16,156, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:35:16.156866', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:35:16,157, 2999778935, INFO, Numerical and text pipelines created ]
[2024-12-13 22:35:16,161, 967265483, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:35:16,162, 967265483, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:35:16,163, 967265483, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:35:16,164, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:35:18,302, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:35:18.302976', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:35:18,317, 967265483, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:35:18,418, 967265483, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:35:18,419, 967265483, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:35:19,390, 967265483, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:35:19,391, 967265483, INFO, Removing punctuations from text ]
[2024-12-13 22:35:19,392, 967265483, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-13 22:35:19,394, 2999778935, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-13 22:36:38,288, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:36:38,290, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:36:38,292, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:36:38,294, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:36:38,608, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:36:40,167, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:36:40,168, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:36:41,758, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:36:41,775, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:36:41,776, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:36:43,295, 184367788, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:36:43,297, 184367788, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:36:43,298, 184367788, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:36:43,299, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:36:45,662, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:36:45.662913', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:36:45,663, 2999778935, INFO, Numerical and text pipelines created ]
[2024-12-13 22:36:45,666, 184367788, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:36:45,668, 184367788, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:36:45,669, 184367788, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:36:45,671, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:36:48,066, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:36:48.066273', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:36:48,085, 184367788, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:36:48,188, 184367788, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:36:48,189, 184367788, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:36:49,218, 184367788, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:36:49,220, 184367788, INFO, Removing punctuations from text ]
[2024-12-13 22:36:49,223, 184367788, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-13 22:36:49,224, 2999778935, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-13 22:38:05,246, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:38:05,248, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:38:05,250, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:38:05,251, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:38:05,647, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:38:07,194, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:38:07,196, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:38:08,666, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:38:08,687, 2999778935, INFO, Initiating the DataTransformation ]
[2024-12-13 22:38:08,688, 2999778935, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:38:10,234, 772459225, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:38:10,236, 772459225, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:38:10,237, 772459225, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:38:10,237, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:38:12,665, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:38:12.665305', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:38:12,666, 2999778935, INFO, Numerical and text pipelines created ]
[2024-12-13 22:38:12,669, 772459225, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:38:12,670, 772459225, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:38:12,671, 772459225, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:38:12,672, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:38:14,814, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:38:14.814557', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:38:14,832, 772459225, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:38:14,951, 772459225, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:38:14,952, 772459225, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:38:16,660, 772459225, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:38:16,661, 772459225, INFO, Removing punctuations from text ]
[2024-12-13 22:38:16,888, 772459225, INFO, Removing punctuations from text successful ]
[2024-12-13 22:38:16,889, 772459225, INFO, Removing the stopwords from the tokenized words ]
[2024-12-13 22:38:16,891, 772459225, INFO, Error in pre-processing the text: Length of values (0) does not match length of index (1) ]
[2024-12-13 22:38:16,893, 2999778935, INFO, Error in initiating the data transformation pipeline Length of values (0) does not match length of index (1) ]
[2024-12-13 22:39:57,573, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:39:57,574, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:39:57,576, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:39:57,577, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:39:57,902, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:39:59,632, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:39:59,634, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:40:01,422, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:40:01,444, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-13 22:40:01,445, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:40:02,939, 772459225, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:40:02,941, 772459225, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:40:02,942, 772459225, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:40:02,943, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:40:05,260, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:40:05.260278', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:40:05,261, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-13 22:40:05,264, 772459225, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:40:05,266, 772459225, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:40:05,267, 772459225, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:40:05,268, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:40:07,381, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:40:07.381597', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:40:07,396, 772459225, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:40:07,486, 772459225, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:40:07,487, 772459225, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:40:08,839, 772459225, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:40:08,841, 772459225, INFO, Removing punctuations from text ]
[2024-12-13 22:40:09,067, 772459225, INFO, Removing punctuations from text successful ]
[2024-12-13 22:40:09,068, 772459225, INFO, Removing the stopwords from the tokenized words ]
[2024-12-13 22:40:09,070, 772459225, INFO, Error in pre-processing the text: Length of values (0) does not match length of index (1) ]
[2024-12-13 22:40:09,070, 3413596454, INFO, Error in initiating the data transformation pipeline Length of values (0) does not match length of index (1) ]
[2024-12-13 22:42:29,477, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:42:29,479, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:42:29,481, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:42:29,482, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:42:29,801, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:42:31,526, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:42:31,528, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:42:33,287, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:42:33,303, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-13 22:42:33,304, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:42:34,687, 184367788, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:42:34,689, 184367788, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:42:34,689, 184367788, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:42:34,690, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:42:36,995, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:42:36.995705', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:42:36,996, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-13 22:42:36,999, 184367788, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:42:37,001, 184367788, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:42:37,002, 184367788, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:42:37,003, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:42:39,316, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:42:39.316486', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:42:39,338, 184367788, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:42:39,443, 184367788, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:42:39,444, 184367788, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:42:40,751, 184367788, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:42:40,752, 184367788, INFO, Removing punctuations from text ]
[2024-12-13 22:42:40,754, 184367788, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-13 22:42:40,755, 3413596454, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-13 22:43:04,234, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:43:04,236, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:43:04,238, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:43:04,240, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:43:04,577, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:43:06,652, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:43:06,660, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:43:10,113, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:43:10,138, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-13 22:43:10,140, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:43:11,695, 184367788, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:43:11,697, 184367788, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:43:11,698, 184367788, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:43:11,699, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:43:15,701, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:43:15.700440', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:43:15,702, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-13 22:43:15,706, 184367788, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:43:15,708, 184367788, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:43:15,708, 184367788, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:43:15,710, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:43:18,026, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:43:18.026182', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:43:18,043, 184367788, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:43:18,146, 184367788, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:43:18,147, 184367788, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:43:19,168, 184367788, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:43:19,170, 184367788, INFO, Removing punctuations from text ]
[2024-12-13 22:43:19,172, 184367788, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-13 22:43:19,174, 3413596454, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-13 22:43:30,025, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:43:30,026, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:43:30,028, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:43:30,030, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:43:30,377, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:43:32,014, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:43:32,015, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:43:33,721, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:43:33,742, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-13 22:43:33,744, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:43:35,129, 772459225, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:43:35,132, 772459225, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:43:35,133, 772459225, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:43:35,134, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:43:37,593, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:43:37.593240', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:43:37,595, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-13 22:43:37,598, 772459225, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:43:37,600, 772459225, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:43:37,602, 772459225, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:43:37,603, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:43:39,848, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:43:39.848682', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:43:39,865, 772459225, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:43:39,972, 772459225, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:43:39,973, 772459225, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:43:41,520, 772459225, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:43:41,522, 772459225, INFO, Removing punctuations from text ]
[2024-12-13 22:43:41,723, 772459225, INFO, Removing punctuations from text successful ]
[2024-12-13 22:43:41,724, 772459225, INFO, Removing the stopwords from the tokenized words ]
[2024-12-13 22:43:41,727, 772459225, INFO, Error in pre-processing the text: Length of values (0) does not match length of index (1) ]
[2024-12-13 22:43:41,728, 3413596454, INFO, Error in initiating the data transformation pipeline Length of values (0) does not match length of index (1) ]
[2024-12-13 22:45:11,016, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:45:11,018, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:45:11,020, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:45:11,020, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:45:11,329, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:45:12,925, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:45:12,926, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:45:14,661, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:45:14,678, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-13 22:45:14,679, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:45:16,209, 4093102912, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-13 22:45:16,211, 772459225, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:45:16,212, 772459225, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:45:16,212, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:45:18,491, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:45:18.491894', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:45:18,492, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-13 22:45:18,495, 4093102912, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-13 22:45:18,497, 772459225, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:45:18,498, 772459225, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:45:18,499, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:45:20,476, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:45:20.476589', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:45:20,495, 4093102912, INFO, Ensuring input is a pandas Series of strings ]
[2024-12-13 22:45:20,497, 4093102912, ERROR, Error in pre-processing the text: Input X must be a pandas Series ]
[2024-12-13 22:45:20,498, 3413596454, INFO, Error in initiating the data transformation pipeline Input X must be a pandas Series ]
[2024-12-13 22:47:23,481, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:47:23,483, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:47:23,485, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:47:23,487, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:47:23,855, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:47:27,424, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:47:27,425, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:47:29,064, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:47:29,087, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-13 22:47:29,088, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:47:30,649, 967265483, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:47:30,650, 967265483, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:47:30,651, 967265483, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:47:30,652, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:47:33,025, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:47:33.025997', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:47:33,026, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-13 22:47:33,030, 967265483, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:47:33,032, 967265483, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:47:33,033, 967265483, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:47:33,034, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:47:35,482, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:47:35.482468', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:47:35,501, 967265483, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:47:35,600, 967265483, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:47:35,602, 967265483, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:47:37,124, 967265483, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:47:37,125, 967265483, INFO, Removing punctuations from text ]
[2024-12-13 22:47:37,127, 967265483, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-13 22:47:37,128, 3413596454, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-13 22:48:20,059, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:48:20,061, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:48:20,063, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:48:20,065, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:48:20,490, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:48:22,376, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:48:22,377, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:48:23,827, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:48:23,848, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-13 22:48:23,850, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:48:25,409, 3416562058, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:48:25,411, 3416562058, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:48:25,412, 3416562058, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:48:25,413, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:48:27,878, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:48:27.878378', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:48:27,879, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-13 22:48:27,883, 3416562058, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:48:27,885, 3416562058, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:48:27,886, 3416562058, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:48:27,886, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:48:30,118, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:48:30.118691', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:48:30,134, 3416562058, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:48:30,227, 3416562058, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:48:30,228, 3416562058, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:48:31,137, 3416562058, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:48:31,139, 3416562058, INFO, Removing punctuations from text ]
[2024-12-13 22:48:31,140, 3416562058, INFO, Error in pre-processing the text: type object 'str' has no attribute 'x' ]
[2024-12-13 22:48:31,140, 3413596454, INFO, Error in initiating the data transformation pipeline type object 'str' has no attribute 'x' ]
[2024-12-13 22:49:04,487, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-13 22:49:04,489, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-13 22:49:04,491, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-13 22:49:04,493, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-13 22:49:04,842, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-13 22:49:06,428, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-13 22:49:06,429, data_ingestion, INFO, Initiating train test split ]
[2024-12-13 22:49:09,285, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-13 22:49:09,349, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-13 22:49:09,355, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-13 22:49:10,904, 1679785480, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:49:10,907, 1679785480, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:49:10,908, 1679785480, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:49:10,909, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:49:17,720, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:49:17.720339', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:49:17,721, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-13 22:49:17,726, 1679785480, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-13 22:49:17,729, 1679785480, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-13 22:49:17,731, 1679785480, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-13 22:49:17,732, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-13 22:49:19,697, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-13T22:49:19.697043', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-13 22:49:19,713, 1679785480, INFO, Transforming all the letters into lowercase ]
[2024-12-13 22:49:19,816, 1679785480, INFO, Transforming all letters into lowercase is successful ]
[2024-12-13 22:49:19,817, 1679785480, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-13 22:49:21,278, 1679785480, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-13 22:49:21,279, 1679785480, INFO, Removing punctuations from text ]
[2024-12-13 22:49:21,285, 1679785480, INFO, Error in pre-processing the text: 'StringMethods' object is not iterable ]
[2024-12-13 22:49:21,286, 3413596454, INFO, Error in initiating the data transformation pipeline 'StringMethods' object is not iterable ]
[2024-12-14 11:11:20,720, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:11:20,722, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:11:20,724, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:11:20,726, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:11:21,070, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:11:22,588, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:11:22,590, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:11:24,067, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:11:24,088, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:11:24,089, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:11:25,428, 3818907554, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:11:25,430, 3818907554, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:11:25,431, 3818907554, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:11:25,432, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:11:27,648, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:11:27.648857', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:11:27,650, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:11:27,653, 3818907554, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:11:27,654, 3818907554, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:11:27,655, 3818907554, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:11:27,656, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:11:29,641, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:11:29.641965', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:11:29,655, 3818907554, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:11:29,751, 3818907554, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:11:29,752, 3818907554, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:11:31,353, 3818907554, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:11:31,358, 3818907554, INFO, Removing punctuations from text ]
[2024-12-14 11:11:31,359, 3818907554, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 11:11:31,360, 3413596454, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 11:12:02,368, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:12:02,370, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:12:02,372, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:12:02,373, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:12:02,692, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:12:04,142, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:12:04,143, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:12:05,705, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:12:05,740, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:12:05,742, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:12:07,270, 480795491, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:12:07,273, 480795491, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:12:07,274, 480795491, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:12:07,275, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:12:10,495, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:12:10.495953', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:12:10,496, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:12:10,501, 480795491, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:12:10,504, 480795491, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:12:10,505, 480795491, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:12:10,505, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:12:13,123, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:12:13.123929', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:12:13,139, 480795491, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:12:13,236, 480795491, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:12:13,238, 480795491, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:12:14,388, 480795491, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:12:14,389, 480795491, INFO, Removing punctuations from text ]
[2024-12-14 11:12:14,390, 480795491, INFO, Error in pre-processing the text: 'Series' object has no attribute 'split' ]
[2024-12-14 11:12:14,391, 3413596454, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'split' ]
[2024-12-14 11:13:07,074, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:13:07,075, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:13:07,077, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:13:07,078, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:13:07,433, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:13:09,042, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:13:09,043, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:13:10,522, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:13:10,543, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:13:10,545, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:13:11,949, 1515147130, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:13:11,950, 1515147130, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:13:11,952, 1515147130, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:13:11,953, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:13:14,234, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:13:14.234566', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:13:14,235, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:13:14,239, 1515147130, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:13:14,240, 1515147130, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:13:14,241, 1515147130, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:13:14,242, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:13:16,307, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:13:16.307023', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:13:16,323, 1515147130, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:13:16,436, 1515147130, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:13:16,437, 1515147130, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:13:18,307, 1515147130, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:13:18,308, 1515147130, INFO, Removing punctuations from text ]
[2024-12-14 11:13:18,309, 1515147130, INFO, Error in pre-processing the text: 'DataFrame' object has no attribute 'str' ]
[2024-12-14 11:13:18,310, 3413596454, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'str' ]
[2024-12-14 11:14:24,339, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:14:24,341, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:14:24,342, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:14:24,343, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:14:24,667, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:14:26,215, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:14:26,216, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:14:27,716, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:14:27,738, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:14:27,739, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:14:29,170, 3818907554, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:14:29,172, 3818907554, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:14:29,174, 3818907554, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:14:29,175, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:14:31,443, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:14:31.443879', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:14:31,444, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:14:31,447, 3818907554, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:14:31,449, 3818907554, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:14:31,450, 3818907554, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:14:31,452, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:14:33,533, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:14:33.533691', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:14:33,552, 3818907554, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:14:33,646, 3818907554, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:14:33,648, 3818907554, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:14:35,292, 3818907554, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:14:35,293, 3818907554, INFO, Removing punctuations from text ]
[2024-12-14 11:14:35,294, 3818907554, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 11:14:35,296, 3413596454, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 11:16:08,186, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:16:08,188, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:16:08,189, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:16:08,190, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:16:08,509, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:16:10,140, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:16:10,141, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:16:11,726, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:16:11,748, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:16:11,748, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:16:13,185, 3851500527, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:16:13,187, 3851500527, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:16:13,187, 3851500527, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:16:13,189, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:16:15,595, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:16:15.595754', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:16:15,596, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:16:15,600, 3851500527, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:16:15,602, 3851500527, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:16:15,602, 3851500527, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:16:15,604, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:16:17,610, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:16:17.610522', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:16:17,626, 3851500527, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:16:17,713, 3851500527, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:16:17,714, 3851500527, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:16:18,541, 3851500527, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:16:18,543, 3851500527, INFO, Error in pre-processing the text: name 'X_' is not defined ]
[2024-12-14 11:16:18,543, 3413596454, INFO, Error in initiating the data transformation pipeline name 'X_' is not defined ]
[2024-12-14 11:16:38,298, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:16:38,300, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:16:38,301, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:16:38,302, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:16:38,626, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:16:40,081, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:16:40,082, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:16:41,641, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:16:41,662, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:16:41,663, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:16:43,092, 3356500986, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:16:43,094, 3356500986, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:16:43,095, 3356500986, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:16:43,096, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:16:45,371, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:16:45.371686', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:16:45,372, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:16:45,375, 3356500986, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:16:45,377, 3356500986, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:16:45,378, 3356500986, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:16:45,379, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:16:47,640, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:16:47.640617', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:16:47,655, 3356500986, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:16:47,763, 3356500986, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:16:47,764, 3356500986, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:16:49,826, 3356500986, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:16:49,836, 3356500986, INFO, Removing punctuations from text ]
[2024-12-14 11:16:49,837, 3356500986, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 11:16:49,838, 3413596454, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 11:17:36,336, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:17:36,338, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:17:36,341, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:17:36,343, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:17:36,692, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:17:38,286, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:17:38,287, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:17:39,928, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:17:39,947, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:17:39,948, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:17:41,234, 3200035017, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:17:41,246, 3200035017, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:17:41,247, 3200035017, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:17:41,248, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:17:43,509, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:17:43.509513', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:17:43,511, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:17:43,514, 3200035017, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:17:43,515, 3200035017, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:17:43,516, 3200035017, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:17:43,517, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:17:45,489, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:17:45.489555', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:17:45,504, 3200035017, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:17:45,594, 3200035017, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:17:45,595, 3200035017, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:17:46,429, 3200035017, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:17:46,434, 3200035017, INFO, Removing punctuations from text ]
[2024-12-14 11:17:46,435, 3200035017, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 11:17:46,435, 3413596454, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 11:18:21,127, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:18:21,128, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:18:21,130, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:18:21,131, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:18:21,484, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:18:23,146, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:18:23,147, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:18:24,757, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:18:24,777, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:18:24,779, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:18:26,160, 3601916054, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:18:26,162, 3601916054, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:18:26,163, 3601916054, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:18:26,164, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:18:28,509, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:18:28.509307', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:18:28,510, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:18:28,513, 3601916054, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:18:28,515, 3601916054, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:18:28,516, 3601916054, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:18:28,516, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:18:30,516, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:18:30.516964', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:18:30,532, 3601916054, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:18:30,627, 3601916054, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:18:30,628, 3601916054, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:18:32,617, 3601916054, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:18:32,624, 3601916054, INFO, Removing punctuations from text ]
[2024-12-14 11:18:32,625, 3601916054, INFO, Error in pre-processing the text: name 'word' is not defined ]
[2024-12-14 11:18:32,627, 3413596454, INFO, Error in initiating the data transformation pipeline name 'word' is not defined ]
[2024-12-14 11:19:53,457, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:19:53,458, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:19:53,460, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:19:53,461, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:19:53,789, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:19:55,411, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:19:55,412, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:19:56,867, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:19:56,888, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:19:56,890, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:19:58,254, 4106890872, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:19:58,256, 4106890872, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:19:58,257, 4106890872, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:19:58,257, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:20:00,535, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:20:00.535085', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:20:00,537, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:20:00,539, 4106890872, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:20:00,541, 4106890872, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:20:00,542, 4106890872, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:20:00,543, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:20:02,720, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:20:02.720719', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:20:02,736, 4106890872, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:20:02,737, 4106890872, INFO, Error in pre-processing the text: 'Series' object has no attribute 'lower' ]
[2024-12-14 11:20:02,739, 3413596454, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'lower' ]
[2024-12-14 11:22:20,731, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:22:20,733, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:22:20,734, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:22:20,735, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:22:21,121, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:22:22,756, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:22:22,757, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:22:24,196, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:22:24,218, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:22:24,219, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:22:25,535, 1953441227, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:22:25,537, 1953441227, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:22:25,537, 1953441227, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:22:25,538, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:22:27,821, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:22:27.821531', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:22:27,822, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:22:27,826, 1953441227, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:22:27,828, 1953441227, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:22:27,829, 1953441227, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:22:27,829, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:22:29,877, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:22:29.877036', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:22:29,893, 1953441227, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:22:29,894, 1953441227, INFO, Error in pre-processing the text: 'Series' object has no attribute 'lower' ]
[2024-12-14 11:22:29,895, 3413596454, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'lower' ]
[2024-12-14 11:22:56,279, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:22:56,280, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:22:56,282, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:22:56,283, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:22:56,649, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:22:58,371, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:22:58,372, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:22:59,840, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:22:59,860, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:22:59,862, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:23:01,227, 397639816, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:23:01,229, 397639816, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:23:01,230, 397639816, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:23:01,231, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:23:03,513, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:23:03.513239', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:23:03,514, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:23:03,517, 397639816, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:23:03,519, 397639816, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:23:03,520, 397639816, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:23:03,521, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:23:05,525, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:23:05.524876', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:23:05,540, 397639816, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:23:05,634, 397639816, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:23:05,636, 397639816, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:23:06,490, 397639816, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:23:06,495, 397639816, INFO, Removing punctuations from text ]
[2024-12-14 11:23:06,496, 397639816, INFO, Error in pre-processing the text: 'Series' object has no attribute 'translate' ]
[2024-12-14 11:23:06,497, 3413596454, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'translate' ]
[2024-12-14 11:24:15,425, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:24:15,427, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:24:15,428, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:24:15,429, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:24:15,803, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:24:17,559, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:24:17,561, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:24:19,022, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:24:19,044, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:24:19,046, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:24:20,437, 397639816, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:24:20,439, 397639816, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:24:20,440, 397639816, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:24:20,441, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:24:22,677, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:24:22.677899', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:24:22,679, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:24:22,681, 397639816, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:24:22,683, 397639816, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:24:22,684, 397639816, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:24:22,685, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:24:24,653, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:24:24.653200', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:24:24,669, 397639816, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:24:24,771, 397639816, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:24:24,773, 397639816, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:24:26,901, 397639816, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:24:26,905, 397639816, INFO, Removing punctuations from text ]
[2024-12-14 11:24:26,907, 397639816, INFO, Error in pre-processing the text: 'Series' object has no attribute 'translate' ]
[2024-12-14 11:24:26,907, 3413596454, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'translate' ]
[2024-12-14 11:25:06,711, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:25:06,713, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:25:06,714, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:25:06,715, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:25:07,042, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:25:08,778, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:25:08,780, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:25:10,298, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:25:10,318, 3413596454, INFO, Initiating the DataTransformation ]
[2024-12-14 11:25:10,321, 3413596454, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:25:11,647, 2398587094, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:25:11,649, 2398587094, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:25:11,649, 2398587094, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:25:11,651, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:25:13,999, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:25:13.999896', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:25:14,000, 3413596454, INFO, Numerical and text pipelines created ]
[2024-12-14 11:25:14,004, 2398587094, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:25:14,005, 2398587094, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:25:14,006, 2398587094, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:25:14,007, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:25:16,160, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:25:16.160916', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:25:16,176, 2398587094, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:25:16,267, 2398587094, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:25:16,268, 2398587094, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:25:17,193, 2398587094, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:25:17,198, 2398587094, INFO, Removing punctuations from text ]
[2024-12-14 11:25:17,327, 2398587094, INFO, Removing punctuations from text successful ]
[2024-12-14 11:25:17,328, 2398587094, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 11:25:17,362, 2398587094, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 11:25:17,363, 2398587094, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 11:25:22,654, 2398587094, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 11:25:22,655, 3413596454, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 11:28:54,532, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:28:54,534, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:28:54,535, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:28:54,537, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:28:54,871, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:28:56,496, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:28:56,497, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:28:58,257, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:28:58,279, 1674846216, INFO, Initiating the DataTransformation ]
[2024-12-14 11:28:58,281, 1674846216, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:28:59,831, 2398587094, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:28:59,833, 2398587094, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:28:59,834, 2398587094, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:28:59,835, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:29:02,165, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:29:02.165467', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:29:02,167, 1674846216, INFO, Numerical and text pipelines created ]
[2024-12-14 11:29:02,170, 2398587094, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:29:02,171, 2398587094, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:29:02,172, 2398587094, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:29:02,174, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:29:04,373, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:29:04.373556', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:29:04,389, 2398587094, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:29:04,481, 2398587094, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:29:04,482, 2398587094, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:29:05,345, 2398587094, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:29:05,350, 2398587094, INFO, Removing punctuations from text ]
[2024-12-14 11:29:05,470, 2398587094, INFO, Removing punctuations from text successful ]
[2024-12-14 11:29:05,471, 2398587094, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 11:29:05,502, 2398587094, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 11:29:05,504, 2398587094, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 11:29:05,505, 2398587094, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 11:29:05,508, 1674846216, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 11:32:51,821, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:32:51,823, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:32:51,825, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:32:51,826, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:32:52,156, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:32:53,897, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:32:53,898, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:32:55,441, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:32:55,463, 1674846216, INFO, Initiating the DataTransformation ]
[2024-12-14 11:32:55,464, 1674846216, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:32:56,836, 2398587094, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:32:56,837, 2398587094, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:32:56,838, 2398587094, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:32:56,839, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:32:59,117, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:32:59.117292', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:32:59,118, 1674846216, INFO, Numerical and text pipelines created ]
[2024-12-14 11:32:59,122, 2398587094, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:32:59,123, 2398587094, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:32:59,124, 2398587094, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:32:59,124, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:33:01,171, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:33:01.171435', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:33:01,187, 2398587094, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:33:01,283, 2398587094, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:33:01,284, 2398587094, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:33:02,153, 2398587094, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:33:02,159, 2398587094, INFO, Removing punctuations from text ]
[2024-12-14 11:33:02,276, 2398587094, INFO, Removing punctuations from text successful ]
[2024-12-14 11:33:02,277, 2398587094, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 11:33:02,307, 2398587094, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 11:33:02,308, 2398587094, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 11:33:02,310, 2398587094, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 11:33:02,312, 1674846216, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 11:36:25,722, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:36:25,725, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:36:25,726, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:36:25,728, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:36:26,111, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:36:27,864, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:36:27,867, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:36:29,358, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:36:29,379, 1674846216, INFO, Initiating the DataTransformation ]
[2024-12-14 11:36:29,380, 1674846216, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:36:30,835, 4271229514, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:36:30,837, 4271229514, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:36:30,838, 4271229514, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:36:30,839, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:36:33,069, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:36:33.069750', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:36:33,070, 1674846216, INFO, Numerical and text pipelines created ]
[2024-12-14 11:36:33,073, 4271229514, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:36:33,075, 4271229514, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:36:33,076, 4271229514, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:36:33,076, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:36:35,170, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:36:35.170134', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:36:35,192, 4271229514, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:36:35,320, 4271229514, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:36:35,321, 4271229514, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:36:36,857, 4271229514, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:36:36,861, 4271229514, INFO, Removing punctuations from text ]
[2024-12-14 11:36:36,978, 4271229514, INFO, Removing punctuations from text successful ]
[2024-12-14 11:36:36,979, 4271229514, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 11:36:37,013, 4271229514, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 11:36:37,014, 4271229514, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 11:36:37,016, 4271229514, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 11:36:37,017, 1674846216, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 11:58:24,428, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 11:58:24,431, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 11:58:24,433, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 11:58:24,435, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 11:58:24,809, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 11:58:26,229, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 11:58:26,231, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 11:58:27,708, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 11:58:27,727, 1674846216, INFO, Initiating the DataTransformation ]
[2024-12-14 11:58:27,729, 1674846216, INFO, Initiatig data transformation pipeline ]
[2024-12-14 11:58:29,137, 2169316, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:58:29,139, 2169316, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:58:29,141, 2169316, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:58:29,141, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:58:31,155, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:58:31.155364', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:58:31,156, 1674846216, INFO, Numerical and text pipelines created ]
[2024-12-14 11:58:31,159, 2169316, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 11:58:31,161, 2169316, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 11:58:31,162, 2169316, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 11:58:31,164, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 11:58:33,165, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T11:58:33.165969', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 11:58:33,183, 2169316, INFO, Transforming all the letters into lowercase ]
[2024-12-14 11:58:33,269, 2169316, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 11:58:33,271, 2169316, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 11:58:34,083, 2169316, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 11:58:34,085, 2169316, INFO, Removing punctuations from text ]
[2024-12-14 11:58:34,203, 2169316, INFO, Removing punctuations from text successful ]
[2024-12-14 11:58:34,204, 2169316, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 11:58:34,233, 2169316, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 11:58:34,235, 2169316, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 11:58:34,237, 2169316, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 11:58:34,239, 1674846216, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 12:00:31,884, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 12:00:31,885, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 12:00:31,887, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 12:00:31,889, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 12:00:32,264, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 12:00:33,872, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 12:00:33,874, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 12:00:35,357, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 12:00:35,374, 1674846216, INFO, Initiating the DataTransformation ]
[2024-12-14 12:00:35,375, 1674846216, INFO, Initiatig data transformation pipeline ]
[2024-12-14 12:00:36,674, 1339007772, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:00:36,676, 1339007772, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:00:36,677, 1339007772, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:00:36,678, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:00:38,739, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:00:38.739253', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:00:38,740, 1674846216, INFO, Numerical and text pipelines created ]
[2024-12-14 12:00:38,743, 1339007772, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:00:38,745, 1339007772, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:00:38,746, 1339007772, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:00:38,748, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:00:40,814, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:00:40.813754', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:00:40,830, 1339007772, INFO, Transforming all the letters into lowercase ]
[2024-12-14 12:00:40,937, 1339007772, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 12:00:40,938, 1339007772, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 12:00:42,099, 1339007772, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 12:00:42,100, 1339007772, INFO, Removing punctuations from text ]
[2024-12-14 12:00:42,213, 1339007772, INFO, Removing punctuations from text successful ]
[2024-12-14 12:00:42,215, 1339007772, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 12:00:42,256, 1339007772, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 12:00:42,258, 1339007772, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 12:00:42,261, 1339007772, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 12:00:42,262, 1674846216, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 12:04:25,999, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 12:04:26,001, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 12:04:26,003, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 12:04:26,004, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 12:04:26,314, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 12:04:27,945, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 12:04:27,947, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 12:04:29,438, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 12:04:29,455, 1920069468, INFO, Initiating the DataTransformation ]
[2024-12-14 12:04:29,458, 1920069468, INFO, Initiatig data transformation pipeline ]
[2024-12-14 12:04:30,824, 1339007772, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:04:30,826, 1339007772, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:04:30,827, 1339007772, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:04:30,828, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:04:33,075, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:04:33.075223', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:04:33,077, 1920069468, INFO, Numerical and text pipelines created ]
[2024-12-14 12:04:33,094, 1339007772, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:04:33,096, 1339007772, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:04:33,097, 1339007772, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:04:33,099, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:04:35,206, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:04:35.206890', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:04:35,233, 1339007772, INFO, Transforming all the letters into lowercase ]
[2024-12-14 12:04:35,322, 1339007772, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 12:04:35,324, 1339007772, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 12:04:36,154, 1339007772, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 12:04:36,156, 1339007772, INFO, Removing punctuations from text ]
[2024-12-14 12:04:36,273, 1339007772, INFO, Removing punctuations from text successful ]
[2024-12-14 12:04:36,274, 1339007772, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 12:04:36,302, 1339007772, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 12:04:36,305, 1339007772, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 12:04:36,307, 1339007772, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 12:04:36,307, 1920069468, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 12:08:39,696, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 12:08:39,698, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 12:08:39,700, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 12:08:39,701, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 12:08:40,050, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 12:08:41,626, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 12:08:41,627, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 12:08:43,082, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 12:08:43,103, 1920069468, INFO, Initiating the DataTransformation ]
[2024-12-14 12:08:43,105, 1920069468, INFO, Initiatig data transformation pipeline ]
[2024-12-14 12:08:44,436, 3136698187, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:08:44,438, 3136698187, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:08:44,439, 3136698187, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:08:44,439, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:08:46,898, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:08:46.898135', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:08:46,899, 1920069468, INFO, Numerical and text pipelines created ]
[2024-12-14 12:08:46,912, 3136698187, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:08:46,913, 3136698187, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:08:46,915, 3136698187, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:08:46,916, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:08:48,918, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:08:48.918734', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:08:48,935, 3136698187, INFO, Transforming all the letters into lowercase ]
[2024-12-14 12:08:49,023, 3136698187, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 12:08:49,024, 3136698187, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 12:08:50,090, 3136698187, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 12:08:50,091, 3136698187, INFO, Removing punctuations from text ]
[2024-12-14 12:08:50,204, 3136698187, INFO, Removing punctuations from text successful ]
[2024-12-14 12:08:50,205, 3136698187, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 12:08:50,238, 3136698187, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 12:08:50,240, 3136698187, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 12:08:50,241, 3136698187, INFO, Lemmatization successful ]
[2024-12-14 12:08:50,243, 3136698187, INFO, Generating embeddings for the text data ]
[2024-12-14 12:08:50,244, 3136698187, INFO, Successfully generated embeddings ]
[2024-12-14 12:08:50,245, 1920069468, INFO, Error in initiating the data transformation pipeline Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead. ]
[2024-12-14 12:13:27,658, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 12:13:27,660, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 12:13:27,662, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 12:13:27,662, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 12:13:28,032, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 12:13:29,663, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 12:13:29,664, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 12:13:31,304, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 12:13:31,320, 1920069468, INFO, Initiating the DataTransformation ]
[2024-12-14 12:13:31,323, 1920069468, INFO, Initiatig data transformation pipeline ]
[2024-12-14 12:13:32,656, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:13:32,658, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:13:32,659, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:13:32,660, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:13:34,869, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:13:34.869067', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:13:34,870, 1920069468, INFO, Numerical and text pipelines created ]
[2024-12-14 12:13:34,881, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:13:34,882, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:13:34,886, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:13:34,886, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:13:37,006, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:13:37.006240', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:13:37,025, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 12:13:37,118, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 12:13:37,119, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 12:13:38,221, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 12:13:38,223, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 12:13:38,337, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 12:13:38,338, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 12:13:38,368, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 12:13:38,370, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 12:13:38,371, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 12:13:38,372, 1920069468, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 12:16:00,207, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 12:16:00,208, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 12:16:00,210, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 12:16:00,211, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 12:16:00,518, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 12:16:04,238, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 12:16:04,239, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 12:16:05,707, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 12:16:05,724, 2331505436, INFO, Initiating the DataTransformation ]
[2024-12-14 12:16:05,724, 2331505436, INFO, Initiatig data transformation pipeline ]
[2024-12-14 12:16:07,026, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:16:07,027, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:16:07,028, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:16:07,030, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:16:09,441, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:16:09.441396', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:16:09,442, 2331505436, INFO, Numerical and text pipelines created ]
[2024-12-14 12:16:09,446, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:16:09,448, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:16:09,451, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:16:09,453, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:16:11,613, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:16:11.613301', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:16:11,636, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 12:16:11,729, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 12:16:11,731, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 12:16:12,689, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 12:16:12,691, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 12:16:12,828, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 12:16:12,828, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 12:16:12,858, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 12:16:12,859, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 12:16:12,860, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 12:16:12,862, 2331505436, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 12:17:10,088, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 12:17:10,091, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 12:17:10,092, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 12:17:10,093, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 12:17:10,408, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 12:17:11,994, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 12:17:11,995, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 12:17:15,120, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 12:17:15,141, 1644471381, INFO, Initiating the DataTransformation ]
[2024-12-14 12:17:15,142, 1644471381, INFO, Initiatig data transformation pipeline ]
[2024-12-14 12:17:16,523, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:17:16,525, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:17:16,526, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:17:16,527, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:17:18,658, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:17:18.658821', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:17:18,660, 1644471381, INFO, Numerical and text pipelines created ]
[2024-12-14 12:17:18,662, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:17:18,664, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:17:18,666, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:17:18,667, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:17:20,757, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:17:20.757480', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:17:20,766, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 12:17:20,878, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 12:17:20,879, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 12:17:22,133, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 12:17:22,135, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 12:17:22,254, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 12:17:22,255, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 12:17:22,282, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 12:17:22,284, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 12:17:22,286, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 12:17:22,288, 1644471381, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 12:18:48,125, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 12:18:48,127, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 12:18:48,129, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 12:18:48,131, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 12:18:48,505, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 12:18:49,963, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 12:18:49,965, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 12:18:51,445, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 12:18:51,461, 2331505436, INFO, Initiating the DataTransformation ]
[2024-12-14 12:18:51,463, 2331505436, INFO, Initiatig data transformation pipeline ]
[2024-12-14 12:18:52,773, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:18:52,774, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:18:52,775, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:18:52,778, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:18:54,964, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:18:54.964203', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:18:54,965, 2331505436, INFO, Numerical and text pipelines created ]
[2024-12-14 12:18:54,969, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:18:54,971, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:18:54,972, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:18:54,972, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:18:57,029, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:18:57.029682', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:18:57,049, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 12:18:57,136, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 12:18:57,137, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 12:18:57,969, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 12:18:57,970, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 12:18:58,083, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 12:18:58,084, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 12:18:58,113, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 12:18:58,115, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 12:18:58,117, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 12:18:58,118, 2331505436, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 12:36:08,482, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 12:36:08,483, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 12:36:08,485, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 12:36:08,487, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 12:36:08,803, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 12:36:10,547, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 12:36:10,548, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 12:36:12,279, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 12:36:12,299, 3532593754, INFO, Initiating the DataTransformation ]
[2024-12-14 12:36:12,301, 3532593754, INFO, Initiatig data transformation pipeline ]
[2024-12-14 12:36:13,686, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:36:13,688, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:36:13,690, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:36:13,690, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:36:15,787, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:36:15.787064', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:36:15,788, 3532593754, INFO, Numerical and text pipelines created ]
[2024-12-14 12:36:15,792, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 12:36:15,793, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 12:36:15,794, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 12:36:15,795, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 12:36:18,033, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T12:36:18.033869', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 12:36:18,053, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 12:36:18,155, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 12:36:18,156, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 12:36:19,454, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 12:36:19,456, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 12:36:19,570, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 12:36:19,571, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 12:36:19,598, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 12:36:19,600, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 12:36:19,602, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 12:36:19,604, 3532593754, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 13:10:29,464, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:10:29,466, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:10:29,467, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:10:29,469, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:10:29,774, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:10:31,302, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:10:31,303, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:10:32,996, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:10:33,012, 347725411, INFO, Initiating the DataTransformation ]
[2024-12-14 13:10:33,013, 347725411, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:10:34,313, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:10:34,315, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:10:34,315, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:10:34,317, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:10:36,367, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:10:36.367759', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:10:36,368, 347725411, INFO, Numerical and text pipelines created ]
[2024-12-14 13:10:36,373, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:10:36,375, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:10:36,376, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:10:36,377, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:10:38,451, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:10:38.451228', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:10:38,470, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:10:38,553, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:10:38,555, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:10:39,394, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:10:39,395, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 13:10:39,524, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 13:10:39,525, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:10:39,555, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:10:39,557, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 13:10:39,558, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 13:10:39,559, 347725411, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 13:14:09,440, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:14:09,442, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:14:09,445, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:14:09,446, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:14:09,820, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:15:02,900, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:15:02,901, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:15:02,903, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:15:02,905, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:15:03,242, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:15:05,075, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:15:05,077, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:15:06,762, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:15:06,797, 347725411, INFO, Initiating the DataTransformation ]
[2024-12-14 13:15:06,798, 347725411, INFO, Error initiating the DataTransformation : name 'DataColumns' is not defined ]
[2024-12-14 13:15:28,601, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:15:28,603, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:15:28,604, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:15:28,607, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:15:28,947, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:15:30,536, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:15:30,538, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:15:32,217, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:15:32,252, 2919385610, INFO, Initiating the DataTransformation ]
[2024-12-14 13:15:32,253, 2919385610, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:15:33,798, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:15:33,803, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:15:33,805, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:15:33,806, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:15:36,241, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:15:36.241048', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:15:36,242, 2919385610, INFO, Numerical and text pipelines created ]
[2024-12-14 13:15:36,245, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:15:36,247, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:15:36,248, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:15:36,249, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:15:38,351, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:15:38.351409', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:15:38,372, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:15:38,475, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:15:38,476, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:15:39,417, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:15:39,419, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 13:15:39,537, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 13:15:39,538, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:15:39,564, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:15:39,566, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 13:15:43,904, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 13:15:43,905, 2919385610, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 13:16:21,857, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:16:21,859, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:16:21,861, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:16:21,863, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:16:22,246, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:16:23,872, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:16:23,873, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:16:25,575, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:16:25,600, 2424228137, INFO, Initiating the DataTransformation ]
[2024-12-14 13:16:25,602, 2424228137, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:16:26,906, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:16:26,908, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:16:26,910, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:16:26,911, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:16:29,138, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:16:29.138100', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:16:29,139, 2424228137, INFO, Numerical and text pipelines created ]
[2024-12-14 13:16:29,144, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:16:29,147, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:16:29,148, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:16:29,150, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:16:31,547, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:16:31.547284', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:16:31,566, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:16:31,667, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:16:31,669, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:16:32,776, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:16:32,777, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 13:16:32,921, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 13:16:32,922, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:16:32,957, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:16:32,959, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 13:16:32,961, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 13:16:32,962, 2424228137, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 13:28:14,293, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:28:14,295, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:28:14,297, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:28:14,299, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:28:14,642, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:28:16,224, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:28:16,226, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:28:17,698, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:28:17,728, 2424228137, INFO, Initiating the DataTransformation ]
[2024-12-14 13:28:17,729, 2424228137, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:28:19,026, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:28:19,028, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:28:19,028, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:28:19,029, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:28:21,390, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:28:21.390429', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:28:21,391, 2424228137, INFO, Numerical and text pipelines created ]
[2024-12-14 13:28:21,395, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:28:21,397, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:28:21,398, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:28:21,398, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:28:23,842, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:28:23.842251', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:28:23,859, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:28:23,979, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:28:23,981, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:28:25,321, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:28:25,322, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 13:28:25,465, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 13:28:25,466, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:28:25,502, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:28:25,504, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 13:28:25,505, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 13:28:25,506, 2424228137, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 13:29:27,870, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:29:27,871, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:29:27,873, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:29:27,876, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:29:28,187, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:29:29,709, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:29:29,711, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:29:31,265, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:29:31,284, 2031253932, INFO, Initiating the DataTransformation ]
[2024-12-14 13:29:31,285, 2031253932, INFO, Initiating data transformation pipeline. ]
[2024-12-14 13:29:32,618, 2031253932, ERROR, Error in get_transformer_object: name 'RemoveStopWords' is not defined ]
[2024-12-14 13:29:32,619, 2031253932, ERROR, Error in data transformation pipeline: name 'RemoveStopWords' is not defined ]
[2024-12-14 13:37:30,225, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:37:30,228, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:37:30,230, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:37:30,231, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:37:30,564, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:37:32,197, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:37:32,199, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:37:33,839, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:37:33,877, 3962780592, INFO, Initiating the DataTransformation ]
[2024-12-14 13:37:33,878, 3962780592, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:37:35,410, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:37:35,415, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:37:35,416, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:37:35,417, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:37:37,785, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:37:37.785825', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:37:37,786, 3962780592, INFO, Numerical and text pipelines created ]
[2024-12-14 13:37:37,787, 3962780592, INFO, Error in initiating the data transformation pipeline __init__() got an unexpected keyword argument 'steps' ]
[2024-12-14 13:38:04,086, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:38:04,088, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:38:04,090, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:38:04,092, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:38:04,432, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:38:05,998, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:38:05,999, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:38:07,614, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:38:07,647, 155414716, INFO, Initiating the DataTransformation ]
[2024-12-14 13:38:07,648, 155414716, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:38:09,176, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:38:09,178, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:38:09,179, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:38:09,181, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:38:11,639, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:38:11.639245', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:38:11,640, 155414716, INFO, Numerical and text pipelines created ]
[2024-12-14 13:38:11,641, 155414716, INFO, Error in initiating the data transformation pipeline __init__() got an unexpected keyword argument 'steps' ]
[2024-12-14 13:38:33,497, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:38:33,499, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:38:33,500, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:38:33,501, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:38:33,823, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:38:35,739, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:38:35,745, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:38:38,777, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:38:38,802, 2405749325, INFO, Initiating the DataTransformation ]
[2024-12-14 13:38:38,803, 2405749325, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:38:40,083, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:38:40,085, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:38:40,086, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:38:40,087, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:38:42,471, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:38:42.471922', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:38:42,474, 2405749325, INFO, Numerical and text pipelines created ]
[2024-12-14 13:38:42,477, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:38:42,479, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 13:38:42,481, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:38:42,482, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:38:44,857, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:38:44.857392', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:38:44,877, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:38:45,006, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:38:45,007, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:38:46,103, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:38:46,104, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 13:38:46,221, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 13:38:46,223, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:38:46,254, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:38:46,256, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 13:38:50,748, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 13:38:50,749, 2405749325, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 13:41:39,028, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:41:39,030, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:41:39,032, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:41:39,033, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:41:39,378, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:41:40,969, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:41:40,970, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:41:42,433, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:41:42,457, 1754977927, INFO, Initiating the DataTransformation ]
[2024-12-14 13:41:42,458, 1754977927, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:41:43,948, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:41:43,950, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:41:43,951, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:41:46,458, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:41:46.458158', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:41:46,459, 1754977927, INFO, Numerical and text pipelines created ]
[2024-12-14 13:41:46,463, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:41:46,465, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:41:46,466, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:41:48,701, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:41:48.701904', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:41:48,720, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:41:48,832, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:41:48,833, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:41:50,036, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:41:50,037, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 13:41:50,194, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 13:41:50,196, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:41:50,233, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:41:50,236, 2457220914, INFO, Generating embeddings for the text data ]
[2024-12-14 13:41:50,337, 2457220914, INFO, Successfully generated embeddings ]
[2024-12-14 13:41:50,339, 1754977927, INFO, Error in initiating the data transformation pipeline all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 103304 and the array at index 1 has size 50 ]
[2024-12-14 13:46:14,314, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:46:14,316, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:46:14,318, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:46:14,320, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:46:14,655, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:46:16,414, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:46:16,416, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:46:18,218, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:46:18,248, 1754977927, INFO, Initiating the DataTransformation ]
[2024-12-14 13:46:18,249, 1754977927, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:46:19,786, 2912022246, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:46:19,788, 2912022246, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:46:19,789, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:46:22,349, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:46:22.348263', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:46:22,349, 1754977927, INFO, Numerical and text pipelines created ]
[2024-12-14 13:46:22,353, 2912022246, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:46:22,355, 2912022246, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:46:22,356, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:46:24,512, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:46:24.512868', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:46:24,531, 2912022246, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:46:24,648, 2912022246, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:46:24,650, 2912022246, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:46:26,023, 2912022246, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:46:26,024, 2912022246, INFO, Removing punctuations from text ]
[2024-12-14 13:46:26,163, 2912022246, INFO, Removing punctuations from text successful ]
[2024-12-14 13:46:26,165, 2912022246, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:46:26,201, 2912022246, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:46:26,202, 2912022246, INFO, Generating embeddings for the text data ]
[2024-12-14 13:46:26,203, 2912022246, ERROR, Error during transformation: 'Series' object has no attribute 'split' ]
[2024-12-14 13:46:26,204, 1754977927, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'split' ]
[2024-12-14 13:47:20,827, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:47:20,829, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:47:20,831, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:47:20,832, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:47:21,151, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:47:22,921, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:47:22,922, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:47:24,676, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:47:24,698, 1754977927, INFO, Initiating the DataTransformation ]
[2024-12-14 13:47:24,699, 1754977927, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:47:26,070, 2093553626, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:47:26,072, 2093553626, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:47:26,073, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:47:28,166, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:47:28.166819', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:47:28,167, 1754977927, INFO, Numerical and text pipelines created ]
[2024-12-14 13:47:28,172, 2093553626, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:47:28,175, 2093553626, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:47:28,176, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:47:30,311, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:47:30.311628', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:47:30,330, 2093553626, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:47:30,440, 2093553626, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:47:30,441, 2093553626, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:47:31,460, 2093553626, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:47:31,461, 2093553626, INFO, Removing punctuations from text ]
[2024-12-14 13:47:31,595, 2093553626, INFO, Removing punctuations from text successful ]
[2024-12-14 13:47:31,596, 2093553626, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:47:31,629, 2093553626, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:47:31,630, 2093553626, INFO, Generating embeddings for the text data ]
[2024-12-14 13:47:31,631, 2093553626, ERROR, Error during transformation: 'Series' object has no attribute 'split' ]
[2024-12-14 13:47:31,632, 1754977927, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'split' ]
[2024-12-14 13:48:50,856, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 13:48:50,858, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 13:48:50,859, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 13:48:50,861, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 13:48:51,191, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 13:48:52,818, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 13:48:52,819, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 13:48:54,331, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 13:48:54,361, 1754977927, INFO, Initiating the DataTransformation ]
[2024-12-14 13:48:54,362, 1754977927, INFO, Initiatig data transformation pipeline ]
[2024-12-14 13:48:55,921, 1383614726, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:48:55,923, 1383614726, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:48:55,924, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:48:58,328, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:48:58.328508', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:48:58,329, 1754977927, INFO, Numerical and text pipelines created ]
[2024-12-14 13:48:58,333, 1383614726, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 13:48:58,335, 1383614726, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 13:48:58,335, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 13:49:00,528, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T13:49:00.528629', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 13:49:00,547, 1383614726, INFO, Transforming all the letters into lowercase ]
[2024-12-14 13:49:00,649, 1383614726, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 13:49:00,651, 1383614726, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 13:49:01,878, 1383614726, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 13:49:01,879, 1383614726, INFO, Removing punctuations from text ]
[2024-12-14 13:49:02,001, 1383614726, INFO, Removing punctuations from text successful ]
[2024-12-14 13:49:02,002, 1383614726, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 13:49:02,032, 1383614726, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 13:49:02,034, 1383614726, INFO, Generating embeddings for the text data ]
[2024-12-14 13:49:02,035, 1383614726, INFO, Successfully generated embeddings ]
[2024-12-14 13:49:02,036, 1383614726, ERROR, Error during transformation: 'DataFrame' object has no attribute 'tolist' ]
[2024-12-14 13:49:02,037, 1754977927, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'tolist' ]
[2024-12-14 14:14:03,621, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 14:14:03,623, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 14:14:03,625, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 14:14:03,627, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 14:14:03,952, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 14:14:05,494, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 14:14:05,495, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 14:14:07,030, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 14:14:07,048, 1754977927, INFO, Initiating the DataTransformation ]
[2024-12-14 14:14:07,049, 1754977927, INFO, Initiatig data transformation pipeline ]
[2024-12-14 14:14:08,353, 4217064066, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:14:08,355, 4217064066, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:14:08,355, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:14:10,654, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:14:10.654530', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:14:10,655, 1754977927, INFO, Numerical and text pipelines created ]
[2024-12-14 14:14:10,659, 4217064066, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:14:10,661, 4217064066, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:14:10,661, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:14:12,840, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:14:12.840291', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:14:12,858, 4217064066, INFO, Transforming all the letters into lowercase ]
[2024-12-14 14:14:12,948, 4217064066, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 14:14:12,949, 4217064066, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 14:14:13,894, 4217064066, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 14:14:13,896, 4217064066, INFO, Removing punctuations from text ]
[2024-12-14 14:14:14,038, 4217064066, INFO, Removing punctuations from text successful ]
[2024-12-14 14:14:14,039, 4217064066, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 14:14:14,077, 4217064066, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 14:14:14,079, 4217064066, INFO, Generating embeddings for the text data ]
[2024-12-14 14:14:14,081, 4217064066, INFO, Successfully generated embeddings ]
[2024-12-14 14:14:14,085, 1754977927, INFO, Error in initiating the data transformation pipeline The output of the 'Text_Pipeline' transformer should be 2D (numpy array, scipy sparse array, dataframe). ]
[2024-12-14 14:15:21,259, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 14:15:21,261, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 14:15:21,263, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 14:15:21,264, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 14:15:21,599, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 14:15:23,451, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 14:15:23,453, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 14:15:25,180, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 14:15:25,209, 1754977927, INFO, Initiating the DataTransformation ]
[2024-12-14 14:15:25,210, 1754977927, INFO, Initiatig data transformation pipeline ]
[2024-12-14 14:15:26,502, 3225407149, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:15:26,505, 3225407149, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:15:26,505, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:15:28,766, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:15:28.766068', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:15:28,767, 1754977927, INFO, Numerical and text pipelines created ]
[2024-12-14 14:15:28,771, 3225407149, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:15:28,773, 3225407149, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:15:28,774, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:15:32,102, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:15:32.102208', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:15:32,164, 3225407149, INFO, Transforming all the letters into lowercase ]
[2024-12-14 14:15:32,380, 3225407149, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 14:15:32,381, 3225407149, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 14:15:33,628, 3225407149, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 14:15:33,630, 3225407149, INFO, Removing punctuations from text ]
[2024-12-14 14:15:33,745, 3225407149, INFO, Removing punctuations from text successful ]
[2024-12-14 14:15:33,747, 3225407149, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 14:15:33,783, 3225407149, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 14:15:33,784, 3225407149, INFO, Generating embeddings for the text data ]
[2024-12-14 14:15:33,787, 3225407149, INFO, Successfully generated embeddings ]
[2024-12-14 14:15:33,791, 1754977927, INFO, Error in initiating the data transformation pipeline all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 103304 and the array at index 1 has size 1 ]
[2024-12-14 14:27:18,197, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 14:27:18,198, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 14:27:18,201, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 14:27:18,202, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 14:27:18,511, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 14:27:20,083, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 14:27:20,085, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 14:27:21,496, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 14:27:21,516, 2405749325, INFO, Initiating the DataTransformation ]
[2024-12-14 14:27:21,518, 2405749325, INFO, Initiatig data transformation pipeline ]
[2024-12-14 14:27:22,814, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:27:22,816, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:27:22,817, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:27:22,818, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:27:25,174, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:27:25.174068', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:27:25,175, 2405749325, INFO, Numerical and text pipelines created ]
[2024-12-14 14:27:25,179, 2457220914, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:27:25,181, 2457220914, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:27:25,183, 2457220914, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:27:25,184, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:27:27,312, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:27:27.312865', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:27:27,333, 2457220914, INFO, Transforming all the letters into lowercase ]
[2024-12-14 14:27:27,439, 2457220914, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 14:27:27,440, 2457220914, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 14:27:28,443, 2457220914, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 14:27:28,444, 2457220914, INFO, Removing punctuations from text ]
[2024-12-14 14:27:28,590, 2457220914, INFO, Removing punctuations from text successful ]
[2024-12-14 14:27:28,591, 2457220914, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 14:27:28,627, 2457220914, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 14:27:28,628, 2457220914, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 14:27:28,629, 2457220914, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 14:27:28,630, 2405749325, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 14:31:09,074, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 14:31:09,076, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 14:31:09,078, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 14:31:09,080, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 14:31:09,439, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 14:31:11,867, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 14:31:11,871, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 14:31:14,795, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 14:31:14,821, 2405749325, INFO, Initiating the DataTransformation ]
[2024-12-14 14:31:14,824, 2405749325, INFO, Initiatig data transformation pipeline ]
[2024-12-14 14:31:16,116, 956976421, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:31:16,118, 956976421, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:31:16,119, 956976421, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:31:16,119, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:31:18,427, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:31:18.427347', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:31:18,429, 2405749325, INFO, Numerical and text pipelines created ]
[2024-12-14 14:31:18,432, 956976421, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:31:18,434, 956976421, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:31:18,435, 956976421, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:31:18,436, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:31:20,730, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:31:20.730943', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:31:20,747, 956976421, INFO, Transforming all the letters into lowercase ]
[2024-12-14 14:31:20,851, 956976421, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 14:31:20,852, 956976421, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 14:31:22,146, 956976421, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 14:31:22,147, 956976421, INFO, Removing punctuations from text ]
[2024-12-14 14:31:22,280, 956976421, INFO, Removing punctuations from text successful ]
[2024-12-14 14:31:22,281, 956976421, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 14:31:22,317, 956976421, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 14:31:22,318, 956976421, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 14:31:22,319, 956976421, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 14:31:22,320, 2405749325, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 14:38:22,432, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 14:38:22,434, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 14:38:22,436, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 14:38:22,437, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 14:38:22,848, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 14:38:24,694, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 14:38:24,695, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 14:38:26,461, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 14:38:26,502, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 14:38:26,505, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 14:38:28,276, 1545973463, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:38:28,280, 1545973463, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:38:28,280, 1545973463, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:38:28,281, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:38:30,930, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:38:30.930083', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:38:30,932, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 14:38:30,935, 1545973463, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:38:30,937, 1545973463, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:38:30,938, 1545973463, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:38:30,939, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:38:33,435, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:38:33.435386', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:38:33,459, 1545973463, INFO, Transforming all the letters into lowercase ]
[2024-12-14 14:38:33,575, 1545973463, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 14:38:33,576, 1545973463, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 14:38:34,706, 1545973463, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 14:38:34,707, 1545973463, INFO, Removing punctuations from text ]
[2024-12-14 14:38:34,846, 1545973463, INFO, Removing punctuations from text successful ]
[2024-12-14 14:38:34,847, 1545973463, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 14:38:34,882, 1545973463, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 14:38:34,884, 1545973463, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 14:38:39,459, 1545973463, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 14:38:39,460, 3987913102, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 14:43:32,232, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 14:43:32,234, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 14:43:32,235, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 14:43:32,236, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 14:43:32,649, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 14:43:34,546, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 14:43:34,547, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 14:43:36,270, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 14:43:36,300, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 14:43:36,301, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 14:43:37,856, 3846616926, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:43:37,858, 3846616926, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:43:37,860, 3846616926, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:43:37,861, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:43:40,409, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:43:40.409047', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:43:40,410, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 14:43:40,414, 3846616926, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:43:40,414, 3846616926, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:43:40,415, 3846616926, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:43:40,416, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:43:43,029, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:43:43.029009', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:43:43,050, 3846616926, INFO, Transforming all the letters into lowercase ]
[2024-12-14 14:43:43,168, 3846616926, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 14:43:43,169, 3846616926, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 14:43:44,404, 3846616926, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 14:43:44,405, 3846616926, INFO, Removing punctuations from text ]
[2024-12-14 14:43:44,539, 3846616926, INFO, Removing punctuations from text successful ]
[2024-12-14 14:43:44,540, 3846616926, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 14:43:44,572, 3846616926, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 14:43:44,573, 3846616926, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 14:43:44,574, 3846616926, INFO, Error in lemmatization: 'Series' object has no attribute 'split' ]
[2024-12-14 14:43:44,575, 3987913102, INFO, Error in initiating the data transformation pipeline 'Series' object has no attribute 'split' ]
[2024-12-14 14:44:07,089, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 14:44:07,090, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 14:44:07,092, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 14:44:07,093, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 14:44:07,503, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 14:44:09,300, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 14:44:09,301, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 14:44:11,040, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 14:44:11,076, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 14:44:11,079, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 14:44:12,604, 601768826, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:44:12,606, 601768826, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:44:12,608, 601768826, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:44:12,609, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:44:15,060, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:44:15.060488', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:44:15,062, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 14:44:15,065, 601768826, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 14:44:15,067, 601768826, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 14:44:15,068, 601768826, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 14:44:15,068, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 14:44:17,427, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T14:44:17.427161', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 14:44:17,447, 601768826, INFO, Transforming all the letters into lowercase ]
[2024-12-14 14:44:17,566, 601768826, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 14:44:17,567, 601768826, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 14:44:18,901, 601768826, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 14:44:18,902, 601768826, INFO, Removing punctuations from text ]
[2024-12-14 14:44:19,034, 601768826, INFO, Removing punctuations from text successful ]
[2024-12-14 14:44:19,035, 601768826, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 14:44:19,071, 601768826, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 14:44:19,073, 601768826, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 14:44:19,074, 601768826, INFO, Error in lemmatization: 'float' object has no attribute 'endswith' ]
[2024-12-14 14:44:19,075, 3987913102, INFO, Error in initiating the data transformation pipeline 'float' object has no attribute 'endswith' ]
[2024-12-14 16:13:55,487, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:13:55,489, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:13:55,491, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:13:55,492, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:13:55,820, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:13:57,504, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:13:57,506, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:13:58,969, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:13:58,993, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:13:58,995, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:14:00,344, 3424631851, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:14:00,346, 3424631851, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:14:00,347, 3424631851, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:14:00,348, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:14:02,809, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:14:02.809202', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:14:02,810, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:14:02,813, 3424631851, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:14:02,815, 3424631851, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:14:02,816, 3424631851, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:14:02,817, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:14:04,823, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:14:04.823051', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:14:04,840, 3424631851, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:14:04,952, 3424631851, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:14:04,954, 3424631851, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:14:06,305, 3424631851, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:14:06,306, 3424631851, INFO, Removing punctuations from text ]
[2024-12-14 16:14:06,312, 3424631851, INFO, Error in pre-processing the text: 'StringMethods' object is not iterable ]
[2024-12-14 16:14:06,313, 3987913102, INFO, Error in initiating the data transformation pipeline 'StringMethods' object is not iterable ]
[2024-12-14 16:22:17,222, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:22:17,224, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:22:17,226, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:22:17,228, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:22:17,557, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:22:19,295, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:22:19,297, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:22:20,783, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:22:20,801, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:22:20,802, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:22:22,153, 4265192798, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:22:22,154, 4265192798, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:22:22,155, 4265192798, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:22:22,156, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:22:24,484, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:22:24.484520', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:22:24,485, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:22:24,488, 4265192798, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:22:24,490, 4265192798, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:22:24,490, 4265192798, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:22:24,492, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:22:26,617, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:22:26.617237', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:22:26,634, 4265192798, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:22:26,728, 4265192798, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:22:26,729, 4265192798, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:22:27,748, 4265192798, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:22:27,749, 4265192798, INFO, Removing punctuations from text ]
[2024-12-14 16:22:27,750, 4265192798, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 16:22:27,751, 3987913102, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 16:23:00,875, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:23:00,877, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:23:00,879, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:23:00,880, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:23:01,277, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:23:02,905, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:23:02,906, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:23:04,356, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:23:04,378, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:23:04,379, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:23:05,747, 2868195062, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:23:05,749, 2868195062, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:23:05,750, 2868195062, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:23:05,751, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:23:08,018, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:23:08.018486', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:23:08,020, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:23:08,023, 2868195062, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:23:08,025, 2868195062, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:23:08,026, 2868195062, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:23:08,027, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:23:10,101, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (201881, 50) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:23:10.101069', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:23:10,121, 2868195062, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:23:10,241, 2868195062, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:23:10,242, 2868195062, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:23:11,260, 2868195062, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:23:11,261, 2868195062, INFO, Removing punctuations from text ]
[2024-12-14 16:23:11,262, 2868195062, INFO, Error in pre-processing the text: 'DataFrame' object has no attribute 'str' ]
[2024-12-14 16:23:11,262, 3987913102, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'str' ]
[2024-12-14 16:24:26,944, word2vec, INFO, collecting all words and their counts ]
[2024-12-14 16:24:26,946, word2vec, INFO, PROGRESS: at sentence #0, processed 0 words, keeping 0 word types ]
[2024-12-14 16:24:27,061, word2vec, INFO, PROGRESS: at sentence #10000, processed 408439 words, keeping 26554 word types ]
[2024-12-14 16:24:27,163, word2vec, INFO, PROGRESS: at sentence #20000, processed 809383 words, keeping 39092 word types ]
[2024-12-14 16:24:27,264, word2vec, INFO, PROGRESS: at sentence #30000, processed 1218278 words, keeping 49917 word types ]
[2024-12-14 16:24:27,377, word2vec, INFO, PROGRESS: at sentence #40000, processed 1623949 words, keeping 59450 word types ]
[2024-12-14 16:24:27,462, word2vec, INFO, PROGRESS: at sentence #50000, processed 2028560 words, keeping 67824 word types ]
[2024-12-14 16:24:27,564, word2vec, INFO, PROGRESS: at sentence #60000, processed 2431931 words, keeping 75714 word types ]
[2024-12-14 16:24:27,655, word2vec, INFO, PROGRESS: at sentence #70000, processed 2832678 words, keeping 83130 word types ]
[2024-12-14 16:24:27,753, word2vec, INFO, PROGRESS: at sentence #80000, processed 3245426 words, keeping 90629 word types ]
[2024-12-14 16:24:27,841, word2vec, INFO, PROGRESS: at sentence #90000, processed 3650408 words, keeping 97905 word types ]
[2024-12-14 16:24:27,928, word2vec, INFO, PROGRESS: at sentence #100000, processed 4047571 words, keeping 104455 word types ]
[2024-12-14 16:24:27,971, word2vec, INFO, collected 106574 word types from a corpus of 4176922 raw words and 103304 sentences ]
[2024-12-14 16:24:27,972, word2vec, INFO, Creating a fresh vocabulary ]
[2024-12-14 16:24:28,489, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 retains 106574 unique words (100.00% of original 106574, drops 0)', 'datetime': '2024-12-14T16:24:28.489497', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-14 16:24:28,490, utils, INFO, FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4176922 word corpus (100.00% of original 4176922, drops 0)', 'datetime': '2024-12-14T16:24:28.490495', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-14 16:24:29,217, word2vec, INFO, deleting the raw counts dictionary of 106574 items ]
[2024-12-14 16:24:29,220, word2vec, INFO, sample=0.001 downsamples 35 most-common words ]
[2024-12-14 16:24:29,222, utils, INFO, FastText lifecycle event {'msg': 'downsampling leaves estimated 3726514.7489644596 word corpus (89.2%% of prior 4176922)', 'datetime': '2024-12-14T16:24:29.222538', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'} ]
[2024-12-14 16:24:31,349, fasttext, INFO, estimated required memory for 106574 words, 2000000 buckets and 25 dimensions: 297459020 bytes ]
[2024-12-14 16:24:31,352, word2vec, INFO, resetting layer weights ]
[2024-12-14 16:24:37,802, utils, INFO, FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-14T16:24:37.802674', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'} ]
[2024-12-14 16:24:37,804, utils, INFO, FastText lifecycle event {'msg': 'training model with 8 workers on 106574 vocabulary and 25 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-14T16:24:37.804669', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-14 16:24:38,815, word2vec, INFO, EPOCH 0 - PROGRESS: at 6.16% examples, 230760 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:24:39,826, word2vec, INFO, EPOCH 0 - PROGRESS: at 13.31% examples, 247417 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:40,857, word2vec, INFO, EPOCH 0 - PROGRESS: at 21.17% examples, 259742 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:41,884, word2vec, INFO, EPOCH 0 - PROGRESS: at 27.52% examples, 253301 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:42,910, word2vec, INFO, EPOCH 0 - PROGRESS: at 34.37% examples, 252809 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:43,910, word2vec, INFO, EPOCH 0 - PROGRESS: at 41.18% examples, 253561 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:44,911, word2vec, INFO, EPOCH 0 - PROGRESS: at 49.24% examples, 259115 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:45,964, word2vec, INFO, EPOCH 0 - PROGRESS: at 57.01% examples, 261460 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:46,965, word2vec, INFO, EPOCH 0 - PROGRESS: at 64.31% examples, 261971 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:47,971, word2vec, INFO, EPOCH 0 - PROGRESS: at 71.59% examples, 263159 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:49,028, word2vec, INFO, EPOCH 0 - PROGRESS: at 79.09% examples, 263736 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:50,069, word2vec, INFO, EPOCH 0 - PROGRESS: at 86.73% examples, 264543 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:51,137, word2vec, INFO, EPOCH 0 - PROGRESS: at 94.45% examples, 264619 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:51,755, word2vec, INFO, EPOCH 0: training on 4176922 raw words (3726695 effective words) took 13.9s, 267315 effective words/s ]
[2024-12-14 16:24:52,775, word2vec, INFO, EPOCH 1 - PROGRESS: at 5.44% examples, 202600 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:53,807, word2vec, INFO, EPOCH 1 - PROGRESS: at 11.04% examples, 204570 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:54,817, word2vec, INFO, EPOCH 1 - PROGRESS: at 16.89% examples, 206733 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:24:55,825, word2vec, INFO, EPOCH 1 - PROGRESS: at 22.82% examples, 210007 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:56,852, word2vec, INFO, EPOCH 1 - PROGRESS: at 28.44% examples, 209562 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:24:57,890, word2vec, INFO, EPOCH 1 - PROGRESS: at 33.93% examples, 207413 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:58,947, word2vec, INFO, EPOCH 1 - PROGRESS: at 39.63% examples, 206558 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:24:59,999, word2vec, INFO, EPOCH 1 - PROGRESS: at 45.27% examples, 206035 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:01,001, word2vec, INFO, EPOCH 1 - PROGRESS: at 51.13% examples, 206689 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:02,006, word2vec, INFO, EPOCH 1 - PROGRESS: at 57.28% examples, 208914 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:25:03,012, word2vec, INFO, EPOCH 1 - PROGRESS: at 63.31% examples, 209951 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-14 16:25:04,021, word2vec, INFO, EPOCH 1 - PROGRESS: at 68.99% examples, 210090 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:05,024, word2vec, INFO, EPOCH 1 - PROGRESS: at 74.85% examples, 210963 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:25:06,026, word2vec, INFO, EPOCH 1 - PROGRESS: at 80.70% examples, 211733 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:07,143, word2vec, INFO, EPOCH 1 - PROGRESS: at 86.73% examples, 210793 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:08,180, word2vec, INFO, EPOCH 1 - PROGRESS: at 92.55% examples, 210444 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:09,185, word2vec, INFO, EPOCH 1 - PROGRESS: at 98.36% examples, 210535 words/s, in_qsize 7, out_qsize 1 ]
[2024-12-14 16:25:09,310, word2vec, INFO, EPOCH 1: training on 4176922 raw words (3726671 effective words) took 17.5s, 212401 effective words/s ]
[2024-12-14 16:25:10,370, word2vec, INFO, EPOCH 2 - PROGRESS: at 5.18% examples, 186610 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:11,391, word2vec, INFO, EPOCH 2 - PROGRESS: at 11.06% examples, 201933 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:12,445, word2vec, INFO, EPOCH 2 - PROGRESS: at 17.39% examples, 207610 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:13,457, word2vec, INFO, EPOCH 2 - PROGRESS: at 23.28% examples, 210492 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:14,621, word2vec, INFO, EPOCH 2 - PROGRESS: at 29.42% examples, 207857 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:15,630, word2vec, INFO, EPOCH 2 - PROGRESS: at 35.57% examples, 211163 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:16,674, word2vec, INFO, EPOCH 2 - PROGRESS: at 41.41% examples, 211409 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:17,779, word2vec, INFO, EPOCH 2 - PROGRESS: at 47.78% examples, 211101 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:18,783, word2vec, INFO, EPOCH 2 - PROGRESS: at 53.27% examples, 210156 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:19,795, word2vec, INFO, EPOCH 2 - PROGRESS: at 59.23% examples, 211023 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:20,902, word2vec, INFO, EPOCH 2 - PROGRESS: at 65.29% examples, 210001 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:21,953, word2vec, INFO, EPOCH 2 - PROGRESS: at 71.59% examples, 211568 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:22,969, word2vec, INFO, EPOCH 2 - PROGRESS: at 77.42% examples, 212113 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:23,972, word2vec, INFO, EPOCH 2 - PROGRESS: at 82.84% examples, 211541 words/s, in_qsize 14, out_qsize 1 ]
[2024-12-14 16:25:25,002, word2vec, INFO, EPOCH 2 - PROGRESS: at 88.94% examples, 211814 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:26,016, word2vec, INFO, EPOCH 2 - PROGRESS: at 95.17% examples, 212763 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:26,699, word2vec, INFO, EPOCH 2: training on 4176922 raw words (3726576 effective words) took 17.4s, 214451 effective words/s ]
[2024-12-14 16:25:27,730, word2vec, INFO, EPOCH 3 - PROGRESS: at 4.94% examples, 183203 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:28,829, word2vec, INFO, EPOCH 3 - PROGRESS: at 10.78% examples, 193092 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:29,842, word2vec, INFO, EPOCH 3 - PROGRESS: at 16.70% examples, 198744 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:30,843, word2vec, INFO, EPOCH 3 - PROGRESS: at 22.57% examples, 204237 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:31,865, word2vec, INFO, EPOCH 3 - PROGRESS: at 28.69% examples, 208598 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:32,883, word2vec, INFO, EPOCH 3 - PROGRESS: at 34.61% examples, 210145 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:33,920, word2vec, INFO, EPOCH 3 - PROGRESS: at 39.86% examples, 207003 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:34,965, word2vec, INFO, EPOCH 3 - PROGRESS: at 46.06% examples, 208739 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:25:35,968, word2vec, INFO, EPOCH 3 - PROGRESS: at 51.60% examples, 208135 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:36,997, word2vec, INFO, EPOCH 3 - PROGRESS: at 57.50% examples, 208843 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:38,069, word2vec, INFO, EPOCH 3 - PROGRESS: at 63.83% examples, 209445 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:39,135, word2vec, INFO, EPOCH 3 - PROGRESS: at 69.68% examples, 209379 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:25:40,226, word2vec, INFO, EPOCH 3 - PROGRESS: at 75.78% examples, 209607 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:41,291, word2vec, INFO, EPOCH 3 - PROGRESS: at 81.63% examples, 209533 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:42,307, word2vec, INFO, EPOCH 3 - PROGRESS: at 87.71% examples, 210121 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:43,376, word2vec, INFO, EPOCH 3 - PROGRESS: at 94.01% examples, 210469 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:44,234, word2vec, INFO, EPOCH 3: training on 4176922 raw words (3726260 effective words) took 17.5s, 212642 effective words/s ]
[2024-12-14 16:25:45,257, word2vec, INFO, EPOCH 4 - PROGRESS: at 4.44% examples, 167431 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:46,268, word2vec, INFO, EPOCH 4 - PROGRESS: at 10.56% examples, 197998 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:47,338, word2vec, INFO, EPOCH 4 - PROGRESS: at 16.65% examples, 201253 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:48,363, word2vec, INFO, EPOCH 4 - PROGRESS: at 22.59% examples, 205058 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:49,382, word2vec, INFO, EPOCH 4 - PROGRESS: at 28.44% examples, 207601 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:50,391, word2vec, INFO, EPOCH 4 - PROGRESS: at 34.39% examples, 209622 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:51,457, word2vec, INFO, EPOCH 4 - PROGRESS: at 40.27% examples, 209467 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:52,478, word2vec, INFO, EPOCH 4 - PROGRESS: at 46.55% examples, 211522 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:53,498, word2vec, INFO, EPOCH 4 - PROGRESS: at 52.79% examples, 213066 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:54,498, word2vec, INFO, EPOCH 4 - PROGRESS: at 57.99% examples, 211307 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:55,527, word2vec, INFO, EPOCH 4 - PROGRESS: at 63.81% examples, 210938 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:56,590, word2vec, INFO, EPOCH 4 - PROGRESS: at 69.92% examples, 211486 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:25:57,627, word2vec, INFO, EPOCH 4 - PROGRESS: at 75.77% examples, 211719 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:58,650, word2vec, INFO, EPOCH 4 - PROGRESS: at 81.84% examples, 212741 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:25:59,657, word2vec, INFO, EPOCH 4 - PROGRESS: at 87.68% examples, 212672 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:00,659, word2vec, INFO, EPOCH 4 - PROGRESS: at 93.54% examples, 212668 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-14 16:26:01,640, word2vec, INFO, EPOCH 4: training on 4176922 raw words (3726535 effective words) took 17.4s, 214256 effective words/s ]
[2024-12-14 16:26:02,712, word2vec, INFO, EPOCH 5 - PROGRESS: at 4.51% examples, 159024 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:03,748, word2vec, INFO, EPOCH 5 - PROGRESS: at 10.56% examples, 190729 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:04,806, word2vec, INFO, EPOCH 5 - PROGRESS: at 16.65% examples, 197184 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:05,822, word2vec, INFO, EPOCH 5 - PROGRESS: at 22.34% examples, 200200 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:06,850, word2vec, INFO, EPOCH 5 - PROGRESS: at 28.21% examples, 203413 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:07,865, word2vec, INFO, EPOCH 5 - PROGRESS: at 34.17% examples, 205881 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-14 16:26:08,898, word2vec, INFO, EPOCH 5 - PROGRESS: at 40.08% examples, 207225 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:09,906, word2vec, INFO, EPOCH 5 - PROGRESS: at 46.06% examples, 208773 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:10,925, word2vec, INFO, EPOCH 5 - PROGRESS: at 51.86% examples, 208742 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:11,924, word2vec, INFO, EPOCH 5 - PROGRESS: at 57.74% examples, 209985 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:13,012, word2vec, INFO, EPOCH 5 - PROGRESS: at 63.83% examples, 209423 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:14,059, word2vec, INFO, EPOCH 5 - PROGRESS: at 69.92% examples, 210386 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:15,090, word2vec, INFO, EPOCH 5 - PROGRESS: at 75.99% examples, 211445 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:16,120, word2vec, INFO, EPOCH 5 - PROGRESS: at 81.84% examples, 211755 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:17,181, word2vec, INFO, EPOCH 5 - PROGRESS: at 87.94% examples, 211583 words/s, in_qsize 16, out_qsize 1 ]
[2024-12-14 16:26:18,241, word2vec, INFO, EPOCH 5 - PROGRESS: at 93.99% examples, 211444 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:19,199, word2vec, INFO, EPOCH 5: training on 4176922 raw words (3726571 effective words) took 17.5s, 212361 effective words/s ]
[2024-12-14 16:26:20,258, word2vec, INFO, EPOCH 6 - PROGRESS: at 5.21% examples, 186831 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:21,295, word2vec, INFO, EPOCH 6 - PROGRESS: at 11.04% examples, 200540 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:22,339, word2vec, INFO, EPOCH 6 - PROGRESS: at 17.15% examples, 204545 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:23,376, word2vec, INFO, EPOCH 6 - PROGRESS: at 22.82% examples, 204696 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:26:24,382, word2vec, INFO, EPOCH 6 - PROGRESS: at 28.44% examples, 206135 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:26:25,401, word2vec, INFO, EPOCH 6 - PROGRESS: at 34.37% examples, 208116 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:26,404, word2vec, INFO, EPOCH 6 - PROGRESS: at 40.29% examples, 209929 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:27,408, word2vec, INFO, EPOCH 6 - PROGRESS: at 45.79% examples, 209155 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:28,423, word2vec, INFO, EPOCH 6 - PROGRESS: at 51.12% examples, 207256 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:29,508, word2vec, INFO, EPOCH 6 - PROGRESS: at 56.58% examples, 205195 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:30,517, word2vec, INFO, EPOCH 6 - PROGRESS: at 62.81% examples, 207290 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:31,541, word2vec, INFO, EPOCH 6 - PROGRESS: at 68.75% examples, 208090 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:32,549, word2vec, INFO, EPOCH 6 - PROGRESS: at 74.67% examples, 209033 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:33,602, word2vec, INFO, EPOCH 6 - PROGRESS: at 80.24% examples, 208578 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:34,606, word2vec, INFO, EPOCH 6 - PROGRESS: at 85.74% examples, 208253 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:35,617, word2vec, INFO, EPOCH 6 - PROGRESS: at 91.35% examples, 207876 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:36,620, word2vec, INFO, EPOCH 6 - PROGRESS: at 97.61% examples, 209132 words/s, in_qsize 10, out_qsize 0 ]
[2024-12-14 16:26:36,904, word2vec, INFO, EPOCH 6: training on 4176922 raw words (3726546 effective words) took 17.7s, 210619 effective words/s ]
[2024-12-14 16:26:37,932, word2vec, INFO, EPOCH 7 - PROGRESS: at 4.94% examples, 184016 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:38,933, word2vec, INFO, EPOCH 7 - PROGRESS: at 10.10% examples, 189661 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:39,981, word2vec, INFO, EPOCH 7 - PROGRESS: at 15.53% examples, 188471 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:41,011, word2vec, INFO, EPOCH 7 - PROGRESS: at 21.41% examples, 195215 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:42,092, word2vec, INFO, EPOCH 7 - PROGRESS: at 27.76% examples, 200841 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:43,126, word2vec, INFO, EPOCH 7 - PROGRESS: at 33.45% examples, 201788 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:44,161, word2vec, INFO, EPOCH 7 - PROGRESS: at 38.91% examples, 201086 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:26:45,161, word2vec, INFO, EPOCH 7 - PROGRESS: at 44.77% examples, 203611 words/s, in_qsize 13, out_qsize 2 ]
[2024-12-14 16:26:46,190, word2vec, INFO, EPOCH 7 - PROGRESS: at 51.36% examples, 206851 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:47,238, word2vec, INFO, EPOCH 7 - PROGRESS: at 57.01% examples, 206451 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:48,254, word2vec, INFO, EPOCH 7 - PROGRESS: at 63.05% examples, 207490 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:49,286, word2vec, INFO, EPOCH 7 - PROGRESS: at 68.78% examples, 207421 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:50,287, word2vec, INFO, EPOCH 7 - PROGRESS: at 74.61% examples, 208515 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:51,312, word2vec, INFO, EPOCH 7 - PROGRESS: at 80.24% examples, 208506 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:52,377, word2vec, INFO, EPOCH 7 - PROGRESS: at 86.26% examples, 208525 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:53,381, word2vec, INFO, EPOCH 7 - PROGRESS: at 91.81% examples, 208192 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:54,399, word2vec, INFO, EPOCH 7 - PROGRESS: at 97.63% examples, 208241 words/s, in_qsize 10, out_qsize 0 ]
[2024-12-14 16:26:54,688, word2vec, INFO, EPOCH 7: training on 4176922 raw words (3726795 effective words) took 17.8s, 209697 effective words/s ]
[2024-12-14 16:26:55,704, word2vec, INFO, EPOCH 8 - PROGRESS: at 4.74% examples, 176825 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:26:56,772, word2vec, INFO, EPOCH 8 - PROGRESS: at 10.78% examples, 197380 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:57,895, word2vec, INFO, EPOCH 8 - PROGRESS: at 16.65% examples, 194801 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:58,921, word2vec, INFO, EPOCH 8 - PROGRESS: at 22.34% examples, 197859 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:26:59,957, word2vec, INFO, EPOCH 8 - PROGRESS: at 28.21% examples, 201184 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:00,969, word2vec, INFO, EPOCH 8 - PROGRESS: at 34.14% examples, 204164 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:02,167, word2vec, INFO, EPOCH 8 - PROGRESS: at 40.29% examples, 202290 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:03,222, word2vec, INFO, EPOCH 8 - PROGRESS: at 46.55% examples, 204349 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:04,231, word2vec, INFO, EPOCH 8 - PROGRESS: at 52.57% examples, 205872 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:05,267, word2vec, INFO, EPOCH 8 - PROGRESS: at 58.74% examples, 207539 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:06,313, word2vec, INFO, EPOCH 8 - PROGRESS: at 64.79% examples, 207973 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:07,320, word2vec, INFO, EPOCH 8 - PROGRESS: at 70.41% examples, 208302 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:08,336, word2vec, INFO, EPOCH 8 - PROGRESS: at 76.01% examples, 208426 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:09,415, word2vec, INFO, EPOCH 8 - PROGRESS: at 81.84% examples, 208246 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:10,444, word2vec, INFO, EPOCH 8 - PROGRESS: at 87.94% examples, 208743 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:11,458, word2vec, INFO, EPOCH 8 - PROGRESS: at 94.01% examples, 209324 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:12,369, word2vec, INFO, EPOCH 8: training on 4176922 raw words (3726910 effective words) took 17.7s, 210924 effective words/s ]
[2024-12-14 16:27:13,395, word2vec, INFO, EPOCH 9 - PROGRESS: at 4.94% examples, 183782 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:14,397, word2vec, INFO, EPOCH 9 - PROGRESS: at 10.56% examples, 198167 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:15,430, word2vec, INFO, EPOCH 9 - PROGRESS: at 16.65% examples, 203895 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:16,444, word2vec, INFO, EPOCH 9 - PROGRESS: at 22.34% examples, 205423 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:17,473, word2vec, INFO, EPOCH 9 - PROGRESS: at 28.23% examples, 207514 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:18,474, word2vec, INFO, EPOCH 9 - PROGRESS: at 33.93% examples, 208371 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:27:19,499, word2vec, INFO, EPOCH 9 - PROGRESS: at 39.63% examples, 208319 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:20,513, word2vec, INFO, EPOCH 9 - PROGRESS: at 45.27% examples, 208567 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:21,518, word2vec, INFO, EPOCH 9 - PROGRESS: at 51.13% examples, 208899 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:22,533, word2vec, INFO, EPOCH 9 - PROGRESS: at 57.01% examples, 209825 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:27:23,542, word2vec, INFO, EPOCH 9 - PROGRESS: at 63.05% examples, 210736 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:27:24,560, word2vec, INFO, EPOCH 9 - PROGRESS: at 68.53% examples, 209890 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:25,563, word2vec, INFO, EPOCH 9 - PROGRESS: at 74.17% examples, 210097 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:26,616, word2vec, INFO, EPOCH 9 - PROGRESS: at 79.79% examples, 209556 words/s, in_qsize 16, out_qsize 0 ]
[2024-12-14 16:27:27,627, word2vec, INFO, EPOCH 9 - PROGRESS: at 85.50% examples, 209640 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:28,629, word2vec, INFO, EPOCH 9 - PROGRESS: at 91.35% examples, 209822 words/s, in_qsize 15, out_qsize 0 ]
[2024-12-14 16:27:29,644, word2vec, INFO, EPOCH 9 - PROGRESS: at 97.11% examples, 209802 words/s, in_qsize 12, out_qsize 0 ]
[2024-12-14 16:27:29,986, word2vec, INFO, EPOCH 9: training on 4176922 raw words (3725855 effective words) took 17.6s, 211615 effective words/s ]
[2024-12-14 16:27:29,987, utils, INFO, FastText lifecycle event {'msg': 'training on 41769220 raw words (37265414 effective words) took 172.2s, 216430 effective words/s', 'datetime': '2024-12-14T16:27:29.987945', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'} ]
[2024-12-14 16:27:34,467, utils, INFO, FastText lifecycle event {'params': 'FastText<vocab=106574, vector_size=25, alpha=0.025>', 'datetime': '2024-12-14T16:27:34.467439', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-14 16:27:34,476, keyedvectors, WARNING, destructive init_sims(replace=True) deprecated & no longer required for space-efficiency ]
[2024-12-14 16:27:34,571, keyedvectors, INFO, storing 106574x25 projection weights into ft_reviews_vectors.bin ]
[2024-12-14 16:27:42,577, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:27:43,820, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:27:43.820247', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:27:56,715, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:27:56,717, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:27:56,719, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:27:56,720, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:27:57,065, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:27:58,693, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:27:58,694, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:28:00,338, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:28:00,363, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:28:00,364, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:28:01,764, 1291398518, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:28:01,766, 1291398518, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:28:01,768, 1291398518, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:28:01,770, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:28:03,044, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:28:03.044157', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:28:03,045, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:28:03,048, 1291398518, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:28:03,050, 1291398518, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:28:03,051, 1291398518, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:28:03,052, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:28:04,255, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:28:04.255544', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:28:04,275, 1291398518, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:28:04,378, 1291398518, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:28:04,380, 1291398518, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:28:07,035, 1291398518, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:28:07,036, 1291398518, INFO, Removing punctuations from text ]
[2024-12-14 16:28:07,037, 1291398518, INFO, Error in pre-processing the text: 'list' object has no attribute 'str' ]
[2024-12-14 16:28:07,038, 3987913102, INFO, Error in initiating the data transformation pipeline 'list' object has no attribute 'str' ]
[2024-12-14 16:31:55,933, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:31:55,936, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:31:55,937, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:31:55,939, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:31:56,260, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:31:57,863, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:31:57,864, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:31:59,334, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:31:59,354, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:31:59,356, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:32:00,682, 3548401477, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:32:00,684, 3548401477, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:32:00,685, 3548401477, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:32:00,685, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:32:01,988, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:32:01.988736', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:32:01,989, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:32:01,992, 3548401477, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:32:01,994, 3548401477, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:32:01,995, 3548401477, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:32:01,996, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:32:03,149, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:32:03.149634', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:32:03,165, 3548401477, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:32:03,259, 3548401477, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:32:03,260, 3548401477, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:32:04,168, 3548401477, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:32:04,169, 3548401477, INFO, Removing punctuations from text ]
[2024-12-14 16:32:04,170, 3548401477, INFO, Error in pre-processing the text: 'DataFrame' object has no attribute 'self' ]
[2024-12-14 16:32:04,172, 3987913102, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'self' ]
[2024-12-14 16:32:41,810, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:32:41,812, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:32:41,814, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:32:41,815, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:32:42,180, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:32:43,853, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:32:43,854, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:32:45,353, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:32:45,373, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:32:45,375, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:32:46,769, 764552505, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:32:46,770, 764552505, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:32:46,771, 764552505, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:32:46,772, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:32:48,109, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:32:48.109340', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:32:48,111, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:32:48,114, 764552505, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:32:48,116, 764552505, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:32:48,118, 764552505, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:32:48,119, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:32:49,385, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:32:49.385889', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:32:49,407, 764552505, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:32:49,525, 764552505, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:32:49,526, 764552505, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:32:50,479, 764552505, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:32:50,481, 764552505, INFO, Removing punctuations from text ]
[2024-12-14 16:32:50,483, 764552505, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 16:32:50,484, 3987913102, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 16:33:20,302, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:33:20,304, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:33:20,306, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:33:20,307, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:33:20,676, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:33:22,136, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:33:22,137, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:33:23,589, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:33:23,610, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:33:23,612, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:33:24,961, 3517424198, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:33:24,963, 3517424198, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:33:24,964, 3517424198, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:33:24,965, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:33:26,228, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:33:26.228902', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:33:26,230, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:33:26,233, 3517424198, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:33:26,235, 3517424198, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:33:26,236, 3517424198, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:33:26,237, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:33:27,353, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:33:27.353940', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:33:27,373, 3517424198, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:33:27,479, 3517424198, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:33:27,480, 3517424198, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:33:30,901, 3517424198, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:33:30,902, 3517424198, INFO, Removing punctuations from text ]
[2024-12-14 16:33:30,904, 3517424198, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 16:33:30,905, 3987913102, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 16:34:07,554, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:34:07,556, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:34:07,558, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:34:07,559, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:34:07,912, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:34:09,666, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:34:09,667, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:34:11,238, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:34:11,254, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:34:11,256, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:34:12,646, 610511674, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:34:12,648, 610511674, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:34:12,650, 610511674, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:34:12,651, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:34:13,931, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:34:13.931112', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:34:13,932, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:34:13,935, 610511674, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:34:13,937, 610511674, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:34:13,939, 610511674, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:34:13,940, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:34:15,072, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:34:15.072061', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:34:15,090, 610511674, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:34:15,186, 610511674, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:34:15,187, 610511674, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:34:16,265, 610511674, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:34:16,266, 610511674, INFO, Removing punctuations from text ]
[2024-12-14 16:34:16,267, 610511674, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 16:34:16,268, 3987913102, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 16:38:22,583, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:38:22,584, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:38:22,586, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:38:22,587, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:38:22,968, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:38:24,591, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:38:24,592, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:38:26,069, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:38:26,092, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:38:26,093, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:38:27,480, 3408856019, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:38:27,482, 3408856019, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:38:27,482, 3408856019, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:38:27,483, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:38:28,940, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:38:28.940518', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:38:28,941, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:38:28,944, 3408856019, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:38:28,946, 3408856019, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:38:28,947, 3408856019, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:38:28,948, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:38:30,126, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:38:30.125350', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:38:30,149, 3408856019, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:38:30,265, 3408856019, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:38:30,265, 3408856019, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:38:31,195, 3408856019, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:38:31,196, 3408856019, INFO, Removing punctuations from text ]
[2024-12-14 16:38:31,201, 3408856019, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 16:38:31,202, 3987913102, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 16:39:30,710, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:39:30,712, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:39:30,713, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:39:30,714, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:39:31,054, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:39:32,643, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:39:32,644, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:39:34,139, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:39:34,159, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:39:34,161, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:39:35,725, 31161083, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:39:35,727, 31161083, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:39:35,728, 31161083, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:39:35,729, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:39:37,075, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:39:37.075716', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:39:37,076, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:39:37,080, 31161083, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:39:37,082, 31161083, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:39:37,083, 31161083, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:39:37,083, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:39:38,241, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:39:38.241336', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:39:38,260, 31161083, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:39:38,368, 31161083, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:39:38,369, 31161083, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:39:39,311, 31161083, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:39:39,313, 31161083, INFO, Removing punctuations from text ]
[2024-12-14 16:39:39,314, 31161083, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 16:39:39,315, 3987913102, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 16:41:02,438, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:41:02,440, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:41:02,442, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:41:02,443, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:41:02,829, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:41:04,580, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:41:04,581, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:41:06,322, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:41:06,342, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:41:06,343, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:41:07,767, 31161083, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:41:07,769, 31161083, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:41:07,771, 31161083, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:41:07,772, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:41:09,091, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:41:09.091392', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:41:09,092, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:41:09,095, 31161083, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:41:09,097, 31161083, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:41:09,098, 31161083, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:41:09,099, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:41:10,170, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:41:10.170507', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:41:10,193, 31161083, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:41:10,299, 31161083, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:41:10,299, 31161083, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:41:11,259, 31161083, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:41:11,262, 31161083, INFO, Removing punctuations from text ]
[2024-12-14 16:41:11,263, 31161083, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-14 16:41:11,264, 3987913102, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-14 16:42:03,757, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:42:03,758, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:42:03,760, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:42:03,762, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:42:07,206, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:42:08,831, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:42:08,832, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:42:10,467, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:42:10,487, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:42:10,488, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:42:11,794, 2685266526, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:42:11,796, 2685266526, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:42:11,797, 2685266526, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:42:11,798, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:42:12,911, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:42:12.911377', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:42:12,913, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:42:12,916, 2685266526, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:42:12,917, 2685266526, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:42:12,918, 2685266526, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:42:12,919, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:42:14,074, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:42:14.074274', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:42:14,095, 2685266526, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:42:14,194, 2685266526, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:42:14,196, 2685266526, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:42:15,140, 2685266526, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:42:15,141, 2685266526, INFO, Removing punctuations from text ]
[2024-12-14 16:42:22,633, 2685266526, INFO, Removing punctuations from text successful ]
[2024-12-14 16:42:22,634, 2685266526, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 16:42:22,635, 2685266526, INFO, Error in pre-processing the text: unhashable type: 'list' ]
[2024-12-14 16:42:22,637, 3987913102, INFO, Error in initiating the data transformation pipeline unhashable type: 'list' ]
[2024-12-14 16:43:01,015, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:43:01,017, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:43:01,020, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:43:01,021, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:43:01,357, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:43:02,964, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:43:02,965, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:43:04,488, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:43:04,514, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:43:04,515, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:43:05,829, 632345916, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:43:05,831, 632345916, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:43:05,832, 632345916, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:43:05,833, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:43:07,110, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:43:07.110703', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:43:07,111, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:43:07,115, 632345916, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:43:07,116, 632345916, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:43:07,117, 632345916, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:43:07,118, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:43:08,400, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:43:08.400253', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:43:08,419, 632345916, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:43:08,518, 632345916, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:43:08,520, 632345916, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:43:09,574, 632345916, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:43:09,575, 632345916, INFO, Removing punctuations from text ]
[2024-12-14 16:43:16,240, 632345916, INFO, Removing punctuations from text successful ]
[2024-12-14 16:43:16,241, 632345916, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 16:43:19,004, 632345916, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 16:43:19,007, 632345916, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 16:43:19,009, 632345916, INFO, Error in lemmatization: unhashable type: 'list' ]
[2024-12-14 16:43:19,010, 3987913102, INFO, Error in initiating the data transformation pipeline unhashable type: 'list' ]
[2024-12-14 16:44:40,429, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:44:40,430, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:44:40,432, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:44:40,434, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:44:40,762, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:44:42,375, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:44:42,376, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:44:44,096, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:44:44,115, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:44:44,116, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:44:45,660, 516278899, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:44:45,662, 516278899, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:44:45,663, 516278899, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:44:45,664, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:44:47,000, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:44:47.000987', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:44:47,001, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:44:47,004, 516278899, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:44:47,006, 516278899, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:44:47,008, 516278899, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:44:47,009, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:44:48,385, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:44:48.385734', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:44:48,407, 516278899, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:44:48,510, 516278899, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:44:48,511, 516278899, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:44:49,516, 516278899, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:44:49,518, 516278899, INFO, Removing punctuations from text ]
[2024-12-14 16:44:58,277, 516278899, INFO, Removing punctuations from text successful ]
[2024-12-14 16:44:58,278, 516278899, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 16:44:59,686, 516278899, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 16:44:59,690, 516278899, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 16:45:24,055, 516278899, INFO, Lemmatization successful ]
[2024-12-14 16:45:24,059, 516278899, INFO, Generating embeddings for the text data ]
[2024-12-14 16:45:24,061, 516278899, ERROR, Error during transformation: unhashable type: 'list' ]
[2024-12-14 16:45:24,062, 3987913102, INFO, Error in initiating the data transformation pipeline unhashable type: 'list' ]
[2024-12-14 16:45:49,115, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:45:49,117, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:45:49,119, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:45:49,121, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:45:49,420, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:45:51,133, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:45:51,135, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:45:52,945, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:45:52,963, 3987913102, INFO, Initiating the DataTransformation ]
[2024-12-14 16:45:52,964, 3987913102, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:45:54,440, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:45:54,441, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:45:54,442, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:45:54,443, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:45:55,766, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:45:55.766135', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:45:55,767, 3987913102, INFO, Numerical and text pipelines created ]
[2024-12-14 16:45:55,771, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:45:55,773, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:45:55,774, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:45:55,774, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:45:56,902, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:45:56.902099', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:45:56,923, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:45:57,026, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:45:57,027, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:45:58,073, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:45:58,074, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 16:46:05,152, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 16:46:05,154, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 16:46:06,762, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 16:46:06,766, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 16:46:30,541, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 16:46:30,546, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 16:47:02,980, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 16:47:03,068, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:47:03,111, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:47:03,112, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:47:03,544, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:47:03,545, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 16:47:06,663, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 16:47:06,664, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 16:47:07,385, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 16:47:07,389, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 16:47:16,996, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 16:47:17,136, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 16:47:31,029, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 16:47:35,208, 3987913102, INFO, Error in initiating the data transformation pipeline 'DataTransformationConfig' object has no attribute 'preprocessor_obj_file_path' ]
[2024-12-14 16:48:37,850, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 16:48:37,853, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 16:48:37,854, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 16:48:37,856, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 16:48:38,209, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 16:48:39,952, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 16:48:39,953, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 16:48:41,606, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 16:48:41,624, 3057720041, INFO, Initiating the DataTransformation ]
[2024-12-14 16:48:41,625, 3057720041, INFO, Initiatig data transformation pipeline ]
[2024-12-14 16:48:42,921, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:48:42,923, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:48:42,924, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:48:42,925, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:48:44,101, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:48:44.101920', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:48:44,102, 3057720041, INFO, Numerical and text pipelines created ]
[2024-12-14 16:48:44,105, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 16:48:44,107, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 16:48:44,108, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 16:48:44,110, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 16:48:45,413, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T16:48:45.413415', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 16:48:45,433, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:48:45,534, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:48:45,535, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:48:46,653, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:48:46,654, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 16:48:57,180, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 16:48:57,182, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 16:48:58,669, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 16:48:58,673, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 16:49:21,832, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 16:49:21,837, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 16:49:54,764, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 16:49:54,859, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 16:49:54,897, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 16:49:54,900, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 16:49:55,319, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 16:49:55,320, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 16:49:58,544, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 16:49:58,546, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 16:49:59,156, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 16:49:59,161, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 16:50:08,763, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 16:50:08,909, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 16:50:22,676, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 16:50:26,804, 3057720041, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-14 16:50:26,805, 3057720041, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-14 18:56:43,229, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 18:56:43,230, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 18:56:43,232, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 18:56:43,234, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 18:56:43,670, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 18:59:10,507, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 18:59:10,509, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 18:59:10,511, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 18:59:10,512, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 18:59:10,934, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 18:59:12,695, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 18:59:12,697, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 18:59:14,446, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 18:59:14,479, 409849235, INFO, Initiating the DataTransformation ]
[2024-12-14 18:59:14,482, 409849235, INFO, Initiatig data transformation pipeline ]
[2024-12-14 18:59:16,014, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 18:59:16,017, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 18:59:16,018, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 18:59:16,019, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 18:59:17,277, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T18:59:17.276668', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 18:59:17,278, 409849235, INFO, Numerical and text pipelines created ]
[2024-12-14 18:59:17,282, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 18:59:17,283, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 18:59:17,286, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 18:59:17,287, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 18:59:18,590, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T18:59:18.590800', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 18:59:18,611, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 18:59:18,731, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 18:59:18,732, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 18:59:19,879, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 18:59:19,881, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 18:59:27,761, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 18:59:27,763, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 18:59:29,344, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 18:59:29,348, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 18:59:57,950, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 18:59:57,954, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 19:00:32,471, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 19:00:32,544, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 19:00:32,593, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 19:00:32,594, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 19:00:33,020, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 19:00:33,021, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 19:00:36,444, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 19:00:36,444, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 19:00:37,060, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 19:00:37,063, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 19:00:47,229, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 19:00:47,330, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 19:01:01,674, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 19:01:05,816, 409849235, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-14 19:01:05,817, 409849235, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-14 19:02:15,113, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 19:02:15,115, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 19:02:15,116, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 19:02:15,117, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 19:02:15,496, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 19:02:17,363, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 19:02:17,364, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 19:02:19,138, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 19:02:19,163, 3157283571, INFO, Initiating the DataTransformation ]
[2024-12-14 19:02:19,165, 3157283571, INFO, Initiatig data transformation pipeline ]
[2024-12-14 19:02:20,751, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 19:02:20,753, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 19:02:20,754, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 19:02:20,754, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 19:02:22,064, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T19:02:22.064455', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 19:02:22,065, 3157283571, INFO, Numerical and text pipelines created ]
[2024-12-14 19:02:22,068, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 19:02:22,071, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 19:02:22,073, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 19:02:22,075, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 19:02:23,389, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T19:02:23.389953', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 19:02:23,410, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 19:02:23,520, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 19:02:23,521, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 19:02:24,505, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 19:02:24,506, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 19:02:32,520, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 19:02:32,522, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 19:02:33,884, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 19:02:33,887, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 19:02:57,578, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 19:02:57,581, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 19:03:32,010, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 19:03:32,017, 3157283571, INFO, Error in initiating the data transformation pipeline setting an array element with a sequence. ]
[2024-12-14 19:07:05,793, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-14 19:07:05,794, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-14 19:07:05,796, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-14 19:07:05,797, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-14 19:07:06,248, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-14 19:07:07,972, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-14 19:07:07,974, data_ingestion, INFO, Initiating train test split ]
[2024-12-14 19:07:09,746, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-14 19:07:09,775, 409849235, INFO, Initiating the DataTransformation ]
[2024-12-14 19:07:09,776, 409849235, INFO, Initiatig data transformation pipeline ]
[2024-12-14 19:07:11,308, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 19:07:11,310, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 19:07:11,310, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 19:07:11,311, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 19:07:12,643, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T19:07:12.643063', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 19:07:12,644, 409849235, INFO, Numerical and text pipelines created ]
[2024-12-14 19:07:12,647, 3789801482, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-14 19:07:12,649, 3789801482, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-14 19:07:12,650, 3789801482, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-14 19:07:12,650, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-14 19:07:13,871, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-14T19:07:13.871089', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-14 19:07:13,891, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 19:07:14,005, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 19:07:14,006, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 19:07:15,576, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 19:07:15,577, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 19:07:23,274, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 19:07:23,276, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 19:07:25,327, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 19:07:25,330, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 19:07:49,373, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 19:07:49,376, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 19:08:24,376, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 19:08:24,457, 3789801482, INFO, Transforming all the letters into lowercase ]
[2024-12-14 19:08:24,504, 3789801482, INFO, Transforming all letters into lowercase is successful ]
[2024-12-14 19:08:24,505, 3789801482, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-14 19:08:24,954, 3789801482, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-14 19:08:24,955, 3789801482, INFO, Removing punctuations from text ]
[2024-12-14 19:08:29,090, 3789801482, INFO, Removing punctuations from text successful ]
[2024-12-14 19:08:29,091, 3789801482, INFO, Removing the stopwords from the tokenized words ]
[2024-12-14 19:08:29,792, 3789801482, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-14 19:08:29,796, 3789801482, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-14 19:08:40,572, 3789801482, INFO, Lemmatization successful ]
[2024-12-14 19:08:40,680, 3789801482, INFO, Generating embeddings for the text data ]
[2024-12-14 19:08:55,093, 3789801482, INFO, Successfully generated embeddings ]
[2024-12-14 19:08:59,413, 409849235, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-14 19:08:59,414, 409849235, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-17 21:13:57,879, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-17 21:13:57,883, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-17 21:13:57,887, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-17 21:13:57,888, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-17 21:13:58,369, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-17 21:14:00,213, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-17 21:14:00,215, data_ingestion, INFO, Initiating train test split ]
[2024-12-17 21:14:01,791, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-17 21:14:01,824, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-17 21:14:01,826, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-17 21:14:03,352, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-17 21:14:03,365, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-17 21:14:03,366, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-17 21:14:03,368, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-17 21:14:04,652, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-17T21:14:04.652361', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-17 21:14:04,653, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-17 21:14:04,659, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-17 21:14:04,661, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-17 21:14:04,662, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-17 21:14:04,665, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-17 21:14:05,788, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-17T21:14:05.788288', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-17 21:14:05,805, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-17 21:14:05,930, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-17 21:14:05,931, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-17 21:14:06,936, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-17 21:14:06,937, data_transformation, INFO, Removing punctuations from text ]
[2024-12-17 21:14:13,806, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-17 21:14:13,807, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-17 21:14:15,172, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-17 21:14:15,176, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-17 21:14:40,823, data_transformation, INFO, Lemmatization successful ]
[2024-12-17 21:14:40,826, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-17 21:15:09,941, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-17 21:15:09,944, data_transformation, INFO, Error in initiating the data transformation pipeline setting an array element with a sequence. ]
[2024-12-17 21:18:08,871, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-17 21:18:08,873, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-17 21:18:08,875, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-17 21:18:08,876, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-17 21:18:09,294, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-17 21:18:10,783, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-17 21:18:10,784, data_ingestion, INFO, Initiating train test split ]
[2024-12-17 21:18:12,255, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-17 21:18:12,283, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-17 21:18:12,284, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-17 21:18:13,633, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-17 21:18:13,635, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-17 21:18:13,636, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-17 21:18:13,637, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-17 21:18:14,964, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-17T21:18:14.964954', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-17 21:18:14,965, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-17 21:18:14,968, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-17 21:18:14,970, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-17 21:18:14,971, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-17 21:18:14,972, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-17 21:18:16,084, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-17T21:18:16.084179', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-17 21:18:16,101, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-17 21:18:16,202, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-17 21:18:16,203, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-17 21:18:17,121, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-17 21:18:17,122, data_transformation, INFO, Removing punctuations from text ]
[2024-12-17 21:18:24,203, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-17 21:18:24,204, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-17 21:18:25,394, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-17 21:18:25,398, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-17 21:18:45,832, data_transformation, INFO, Lemmatization successful ]
[2024-12-17 21:18:45,836, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-17 21:19:14,661, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-17 21:19:14,663, data_transformation, INFO, Error in initiating the data transformation pipeline setting an array element with a sequence. ]
[2024-12-17 21:20:01,234, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-17 21:20:01,236, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-17 21:20:01,239, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-17 21:20:01,240, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-17 21:20:01,571, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-17 21:20:02,993, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-17 21:20:02,994, data_ingestion, INFO, Initiating train test split ]
[2024-12-17 21:20:04,472, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-17 21:20:04,505, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-17 21:20:04,507, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-17 21:20:05,859, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-17 21:20:05,862, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-17 21:20:05,864, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-17 21:20:05,865, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-17 21:20:07,168, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-17T21:20:07.168721', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-17 21:20:07,169, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-17 21:20:07,172, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-17 21:20:07,176, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-17 21:20:07,177, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-17 21:20:07,178, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-17 21:20:08,469, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-17T21:20:08.469247', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-17 21:20:08,487, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-17 21:20:08,593, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-17 21:20:08,594, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-17 21:20:09,600, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-17 21:20:09,602, data_transformation, INFO, Removing punctuations from text ]
[2024-12-17 21:20:16,500, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-17 21:20:16,500, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-17 21:20:17,852, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-17 21:20:17,856, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-17 21:20:43,208, data_transformation, INFO, Lemmatization successful ]
[2024-12-17 21:20:43,211, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-17 21:21:13,622, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-17 21:21:13,691, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-17 21:21:13,739, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-17 21:21:13,740, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-17 21:21:14,436, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-17 21:21:14,437, data_transformation, INFO, Removing punctuations from text ]
[2024-12-17 21:21:17,246, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-17 21:21:17,247, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-17 21:21:17,791, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-17 21:21:17,794, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-17 21:21:27,382, data_transformation, INFO, Lemmatization successful ]
[2024-12-17 21:21:27,478, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-17 21:21:40,144, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-17 21:21:44,032, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-17 21:21:44,034, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-18 10:48:37,125, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-18 10:48:37,126, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-18 10:48:37,130, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-18 10:48:37,132, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-18 10:48:37,486, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-18 10:48:39,025, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-18 10:48:39,028, data_ingestion, INFO, Initiating train test split ]
[2024-12-18 10:48:40,723, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-18 10:48:40,748, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-18 10:48:40,749, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-18 10:48:42,116, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 10:48:42,119, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 10:48:42,121, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 10:48:42,122, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 10:48:43,255, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T10:48:43.255579', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 10:48:43,256, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-18 10:48:43,259, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 10:48:43,262, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 10:48:43,263, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 10:48:43,264, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 10:48:44,369, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T10:48:44.369629', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 10:48:44,386, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 10:48:44,482, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 10:48:44,484, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 10:48:45,410, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 10:48:45,411, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 10:48:52,200, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 10:48:52,201, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 10:48:53,380, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 10:48:53,384, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 10:49:13,629, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 10:49:13,633, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 10:49:48,394, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 10:49:48,472, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 10:49:48,514, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 10:49:48,515, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 10:49:48,911, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 10:49:48,912, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 10:49:52,054, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 10:49:52,055, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 10:49:54,304, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 10:49:54,316, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 10:50:04,919, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 10:50:05,015, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 10:50:18,884, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 10:50:22,873, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-18 10:50:22,875, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-18 11:29:59,664, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-18 11:29:59,666, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-18 11:29:59,669, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-18 11:29:59,670, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-18 11:29:59,999, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-18 11:30:01,584, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-18 11:30:01,586, data_ingestion, INFO, Initiating train test split ]
[2024-12-18 11:30:03,221, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-18 11:30:03,244, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-18 11:30:03,245, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-18 11:30:04,804, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 11:30:04,806, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 11:30:04,808, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 11:30:04,809, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 11:30:06,098, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T11:30:06.098858', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 11:30:06,099, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-18 11:30:06,103, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 11:30:06,106, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 11:30:06,107, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 11:30:06,109, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 11:30:07,455, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T11:30:07.455232', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 11:30:07,476, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 11:30:07,574, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 11:30:07,575, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 11:30:08,774, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 11:30:08,775, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 11:30:16,255, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 11:30:16,256, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 11:30:18,106, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 11:30:18,110, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 11:30:41,834, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 11:30:41,837, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 11:31:17,451, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 11:31:17,521, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 11:31:17,566, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 11:31:17,567, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 11:31:17,966, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 11:31:17,967, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 11:31:20,884, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 11:31:20,885, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 11:31:21,539, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 11:31:21,543, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 11:31:31,375, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 11:31:31,471, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 11:31:46,600, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 11:31:50,844, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-18 11:31:50,845, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-18 11:35:08,780, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-18 11:35:08,782, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-18 11:35:08,785, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-18 11:35:08,786, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-18 11:35:09,122, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-18 11:35:10,639, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-18 11:35:10,640, data_ingestion, INFO, Initiating train test split ]
[2024-12-18 11:35:12,450, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-18 11:35:12,469, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-18 11:35:12,470, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-18 11:35:14,053, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 11:35:14,055, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 11:35:14,056, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 11:35:14,056, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 11:35:15,437, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T11:35:15.437645', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 11:35:15,437, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-18 11:35:15,440, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 11:35:15,442, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 11:35:15,443, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 11:35:15,444, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 11:35:16,572, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T11:35:16.572066', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 11:35:16,589, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 11:35:16,697, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 11:35:16,698, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 11:35:17,544, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 11:35:17,545, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 11:35:28,143, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 11:35:28,144, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 11:35:29,479, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 11:35:29,483, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 11:35:54,619, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 11:35:54,622, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 11:36:28,188, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 11:36:28,261, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 11:36:28,308, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 11:36:28,309, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 11:36:28,694, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 11:36:28,695, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 11:36:31,765, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 11:36:31,767, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 11:36:32,666, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 11:36:32,671, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 11:36:42,179, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 11:36:42,276, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 11:36:56,518, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 11:37:00,409, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-18 11:37:00,410, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-18 12:55:37,510, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-18 12:55:37,512, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-18 12:55:37,515, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-18 12:55:37,516, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-18 12:55:37,891, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-18 12:55:39,692, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-18 12:55:39,693, data_ingestion, INFO, Initiating train test split ]
[2024-12-18 12:55:41,511, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-18 12:55:41,529, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-18 12:55:41,533, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-18 12:55:43,177, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 12:55:43,179, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 12:55:43,181, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 12:55:43,181, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 12:55:44,570, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T12:55:44.570531', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 12:55:44,571, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-18 12:55:44,575, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 12:55:44,577, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 12:55:44,578, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 12:55:44,579, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 12:55:46,116, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T12:55:46.116612', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 12:55:46,136, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 12:55:46,259, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 12:55:46,260, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 12:55:47,677, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 12:55:47,678, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 12:55:55,688, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 12:55:55,689, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 12:55:57,490, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 12:55:57,493, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 12:56:22,155, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 12:56:22,159, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 12:56:59,389, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 12:56:59,460, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 12:56:59,506, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 12:56:59,507, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 12:56:59,947, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 12:56:59,949, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 12:57:03,370, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 12:57:03,371, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 12:57:04,079, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 12:57:04,083, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 12:57:15,720, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 12:57:15,824, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 12:57:31,793, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 12:57:36,696, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-18 12:57:36,698, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-18 12:59:13,077, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-18 12:59:13,079, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-18 12:59:13,081, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-18 12:59:13,082, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-18 12:59:13,465, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-18 12:59:15,322, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-18 12:59:15,323, data_ingestion, INFO, Initiating train test split ]
[2024-12-18 12:59:17,084, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-18 12:59:17,125, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-18 12:59:17,126, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-18 12:59:18,713, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 12:59:18,716, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 12:59:18,717, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 12:59:18,718, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 12:59:20,032, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T12:59:20.032811', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 12:59:20,033, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-18 12:59:20,035, data_transformation, INFO, Transforming training data ]
[2024-12-18 12:59:20,044, data_transformation, INFO, Error in initiating the data transformation pipeline Specifying the columns using strings is only supported for dataframes. ]
[2024-12-18 13:00:16,111, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-18 13:00:16,113, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-18 13:00:16,115, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-18 13:00:16,117, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-18 13:00:16,541, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-18 13:00:18,338, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-18 13:00:18,339, data_ingestion, INFO, Initiating train test split ]
[2024-12-18 13:00:20,074, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-18 13:00:20,106, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-18 13:00:20,107, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-18 13:00:21,630, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 13:00:21,636, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 13:00:21,638, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 13:00:21,639, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 13:00:22,950, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T13:00:22.950244', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 13:00:22,952, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-18 13:00:22,953, data_transformation, INFO, Transforming training data ]
[2024-12-18 13:00:22,956, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 13:00:22,960, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 13:00:22,961, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 13:00:22,961, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 13:00:24,312, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T13:00:24.312639', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 13:00:24,332, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 13:00:24,453, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 13:00:24,454, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 13:00:25,615, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 13:00:25,616, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 13:00:34,131, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 13:00:34,132, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 13:00:35,818, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 13:00:35,822, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 13:01:05,581, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 13:01:05,585, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 13:01:40,043, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 13:01:40,114, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 13:01:40,162, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 13:01:40,163, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 13:01:40,597, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 13:01:40,598, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 13:01:44,266, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 13:01:44,267, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 13:01:44,906, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 13:01:44,910, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 13:01:55,329, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 13:01:55,432, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 13:02:09,735, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 13:02:09,877, data_transformation, INFO, Transforming test data ]
[2024-12-18 13:02:14,128, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-18 13:02:14,129, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-18 13:04:42,221, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-18 13:04:42,223, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-18 13:04:42,225, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-18 13:04:42,226, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-18 13:04:42,625, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-18 13:04:44,423, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-18 13:04:44,425, data_ingestion, INFO, Initiating train test split ]
[2024-12-18 13:04:46,196, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-18 13:04:46,232, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-18 13:04:46,234, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-18 13:04:48,028, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 13:04:48,033, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 13:04:48,034, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 13:04:48,037, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 13:04:49,629, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T13:04:49.629099', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 13:04:49,630, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-18 13:04:49,631, data_transformation, INFO, Transforming training data ]
[2024-12-18 13:04:49,636, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-18 13:04:49,640, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-18 13:04:49,642, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-18 13:04:49,644, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-18 13:04:51,057, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-18T13:04:51.057281', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-18 13:04:51,076, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 13:04:51,187, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 13:04:51,188, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 13:04:52,420, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 13:04:52,421, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 13:05:01,168, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 13:05:01,169, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 13:05:02,800, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 13:05:02,803, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 13:05:32,581, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 13:05:32,584, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 13:06:10,215, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 13:06:10,289, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-18 13:06:10,336, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-18 13:06:10,337, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-18 13:06:10,774, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-18 13:06:10,775, data_transformation, INFO, Removing punctuations from text ]
[2024-12-18 13:06:14,144, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-18 13:06:14,145, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-18 13:06:14,789, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-18 13:06:14,793, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-18 13:06:25,457, data_transformation, INFO, Lemmatization successful ]
[2024-12-18 13:06:25,555, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-18 13:06:41,327, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-18 13:06:41,461, data_transformation, INFO, Transforming test data ]
[2024-12-18 13:06:45,740, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-18 13:06:45,741, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-20 19:03:53,506, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-20 19:03:53,508, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-20 19:03:53,511, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-20 19:03:53,513, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-20 19:03:53,999, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-20 19:03:55,932, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-20 19:03:55,934, data_ingestion, INFO, Initiating train test split ]
[2024-12-20 19:03:57,686, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-20 19:03:57,725, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-20 19:03:57,726, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-20 19:03:59,508, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:03:59,532, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:03:59,533, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:03:59,536, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:04:01,527, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:04:01.527083', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:04:01,528, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-20 19:04:01,529, data_transformation, INFO, Transforming training data ]
[2024-12-20 19:04:01,539, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:04:01,543, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:04:01,545, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:04:01,546, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:04:03,148, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:04:03.148154', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:04:03,168, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-20 19:04:03,309, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-20 19:04:03,310, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-20 19:04:04,521, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-20 19:04:04,522, data_transformation, INFO, Removing punctuations from text ]
[2024-12-20 19:04:13,275, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-20 19:04:13,276, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-20 19:04:15,111, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-20 19:04:15,114, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-20 19:04:45,946, data_transformation, INFO, Lemmatization successful ]
[2024-12-20 19:04:45,949, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-20 19:05:20,712, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-20 19:05:20,784, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-20 19:05:20,832, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-20 19:05:20,833, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-20 19:05:21,270, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-20 19:05:21,271, data_transformation, INFO, Removing punctuations from text ]
[2024-12-20 19:05:24,936, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-20 19:05:24,937, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-20 19:05:25,680, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-20 19:05:25,685, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-20 19:05:36,553, data_transformation, INFO, Lemmatization successful ]
[2024-12-20 19:05:36,667, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-20 19:05:52,261, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-20 19:05:52,411, data_transformation, INFO, Transforming test data ]
[2024-12-20 19:05:56,894, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-20 19:05:56,896, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-20 19:14:25,473, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-20 19:14:25,475, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-20 19:14:25,476, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-20 19:14:25,478, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-20 19:14:25,923, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-20 19:14:27,716, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-20 19:14:27,717, data_ingestion, INFO, Initiating train test split ]
[2024-12-20 19:14:29,441, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-20 19:14:29,474, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-20 19:14:29,475, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-20 19:14:31,068, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:14:31,072, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:14:31,073, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:14:31,074, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:14:32,484, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:14:32.484453', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:14:32,485, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-20 19:14:32,486, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-20 19:14:32,487, data_transformation, INFO, Error in initiating the data transformation pipeline 'DataTransformation' object has no attribute 'preprocessing_obj' ]
[2024-12-20 19:16:40,753, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-20 19:16:40,755, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-20 19:16:40,757, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-20 19:16:40,758, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-20 19:16:41,152, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-20 19:16:42,960, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-20 19:16:42,961, data_ingestion, INFO, Initiating train test split ]
[2024-12-20 19:16:44,662, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-20 19:16:44,692, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-20 19:16:44,693, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-20 19:16:46,244, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:16:46,246, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:16:46,247, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:16:46,248, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:16:47,582, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:16:47.582848', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:16:47,583, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-20 19:16:47,584, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-20 19:16:47,585, data_transformation, INFO, Error in initiating the data transformation pipeline 'DataTransformation' object has no attribute 'preprocessing_obj' ]
[2024-12-20 19:17:26,910, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-20 19:17:26,911, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-20 19:17:26,914, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-20 19:17:26,915, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-20 19:17:27,344, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-20 19:17:29,105, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-20 19:17:29,106, data_ingestion, INFO, Initiating train test split ]
[2024-12-20 19:17:30,790, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-20 19:17:30,822, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-20 19:17:30,823, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-20 19:17:32,306, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:17:32,310, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:17:32,310, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:17:32,312, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:17:33,725, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:17:33.725412', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:17:33,726, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-20 19:17:33,726, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-20 19:17:33,727, data_transformation, INFO, Error in initiating the data transformation pipeline 'DataTransformation' object has no attribute 'preprocessing_obj' ]
[2024-12-20 19:18:28,068, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-20 19:18:28,070, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-20 19:18:28,072, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-20 19:18:28,074, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-20 19:18:28,461, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-20 19:18:30,256, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-20 19:18:30,258, data_ingestion, INFO, Initiating train test split ]
[2024-12-20 19:18:31,941, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-20 19:18:31,976, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-20 19:18:31,978, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-20 19:18:33,516, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:18:33,519, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:18:33,520, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:18:33,521, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:18:34,945, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:18:34.945640', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:18:34,946, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-20 19:18:34,947, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-20 19:18:34,950, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:18:34,954, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:18:34,955, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:18:34,956, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:18:36,225, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:18:36.225219', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:18:36,246, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-20 19:18:36,361, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-20 19:18:36,362, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-20 19:18:37,500, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-20 19:18:37,501, data_transformation, INFO, Removing punctuations from text ]
[2024-12-20 19:18:45,578, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-20 19:18:45,579, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-20 19:18:47,174, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-20 19:18:47,178, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-20 19:19:16,759, data_transformation, INFO, Lemmatization successful ]
[2024-12-20 19:19:16,763, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-20 19:19:52,410, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-20 19:19:52,419, data_transformation, INFO, Transforming test data ]
[2024-12-20 19:19:52,483, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-20 19:19:52,530, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-20 19:19:52,531, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-20 19:19:52,969, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-20 19:19:52,970, data_transformation, INFO, Removing punctuations from text ]
[2024-12-20 19:19:56,480, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-20 19:19:56,482, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-20 19:19:57,125, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-20 19:19:57,130, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-20 19:20:08,414, data_transformation, INFO, Lemmatization successful ]
[2024-12-20 19:20:08,517, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-20 19:20:23,721, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-20 19:20:23,859, data_transformation, INFO, Transforming test data ]
[2024-12-20 19:20:28,269, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-20 19:20:28,270, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-20 19:23:57,373, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-20 19:23:57,375, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-20 19:23:57,377, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-20 19:23:57,377, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-20 19:23:57,764, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-20 19:23:59,549, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-20 19:23:59,550, data_ingestion, INFO, Initiating train test split ]
[2024-12-20 19:24:01,255, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-20 19:24:01,289, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-20 19:24:01,290, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-20 19:24:02,862, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:24:02,866, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:24:02,867, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:24:02,868, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:24:04,220, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:24:04.220266', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:24:04,221, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-20 19:24:04,222, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-20 19:24:04,225, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 19:24:04,228, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 19:24:04,229, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 19:24:04,231, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 19:24:05,630, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T19:24:05.630332', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 19:24:05,652, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-20 19:24:05,773, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-20 19:24:05,774, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-20 19:24:07,112, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-20 19:24:07,113, data_transformation, INFO, Removing punctuations from text ]
[2024-12-20 19:24:15,451, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-20 19:24:15,452, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-20 19:24:16,816, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-20 19:24:16,819, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-20 19:24:48,642, data_transformation, INFO, Lemmatization successful ]
[2024-12-20 19:24:48,646, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-20 19:25:23,500, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-20 19:25:23,508, data_transformation, INFO, Transforming test data ]
[2024-12-20 19:25:23,574, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-20 19:25:23,621, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-20 19:25:23,622, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-20 19:25:24,068, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-20 19:25:24,068, data_transformation, INFO, Removing punctuations from text ]
[2024-12-20 19:25:27,377, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-20 19:25:27,378, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-20 19:25:28,014, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-20 19:25:28,018, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-20 19:25:39,063, data_transformation, INFO, Lemmatization successful ]
[2024-12-20 19:25:39,169, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-20 19:25:54,061, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-20 19:25:54,194, data_transformation, INFO, Transforming test data ]
[2024-12-20 19:25:58,491, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-20 19:25:58,492, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-20 22:40:32,446, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-20 22:40:32,448, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-20 22:40:32,454, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-20 22:40:32,455, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-20 22:40:32,876, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-20 22:40:36,574, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-20 22:40:36,577, data_ingestion, INFO, Initiating train test split ]
[2024-12-20 22:40:38,207, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-20 22:40:38,237, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-20 22:40:38,238, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-20 22:40:39,554, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 22:40:39,569, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 22:40:39,570, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 22:40:39,571, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 22:40:40,943, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T22:40:40.943514', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 22:40:40,944, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-20 22:40:40,945, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-20 22:40:40,951, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-20 22:40:40,953, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-20 22:40:40,954, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-20 22:40:40,954, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-20 22:40:42,298, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-20T22:40:42.298836', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-20 22:40:42,318, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-20 22:40:42,431, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-20 22:40:42,432, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-20 22:40:43,336, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-20 22:40:43,338, data_transformation, INFO, Removing punctuations from text ]
[2024-12-20 22:40:53,787, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-20 22:40:53,788, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-20 22:40:55,582, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-20 22:40:55,586, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-20 22:41:29,274, data_transformation, INFO, Lemmatization successful ]
[2024-12-20 22:41:29,278, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-20 22:42:01,633, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-20 22:42:01,640, data_transformation, INFO, Transforming test data ]
[2024-12-20 22:42:01,701, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-20 22:42:01,740, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-20 22:42:01,741, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-20 22:42:02,124, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-20 22:42:02,125, data_transformation, INFO, Removing punctuations from text ]
[2024-12-20 22:42:05,581, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-20 22:42:05,582, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-20 22:42:06,245, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-20 22:42:06,249, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-20 22:42:16,224, data_transformation, INFO, Lemmatization successful ]
[2024-12-20 22:42:16,320, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-20 22:42:29,871, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-20 22:42:30,002, data_transformation, INFO, Transforming test data ]
[2024-12-20 22:42:34,130, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-20 22:42:34,131, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-21 11:16:43,917, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-21 11:16:43,921, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-21 11:16:43,925, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-21 11:16:43,926, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-21 11:16:44,380, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-21 11:16:46,034, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-21 11:16:46,035, data_ingestion, INFO, Initiating train test split ]
[2024-12-21 11:16:47,602, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-21 11:16:47,633, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-21 11:16:47,634, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-21 11:16:48,972, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-21 11:16:48,986, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-21 11:16:48,988, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-21 11:16:48,988, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-21 11:16:50,308, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-21T11:16:50.308907', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-21 11:16:50,309, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-21 11:16:50,310, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-21 11:16:50,319, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-21 11:16:50,324, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-21 11:16:50,325, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-21 11:16:50,326, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-21 11:16:51,436, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-21T11:16:51.436856', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-21 11:16:51,455, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-21 11:16:51,564, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-21 11:16:51,566, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-21 11:16:52,564, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-21 11:16:52,566, data_transformation, INFO, Removing punctuations from text ]
[2024-12-21 11:16:59,095, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-21 11:16:59,097, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-21 11:17:00,540, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-21 11:17:00,543, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-21 11:17:25,707, data_transformation, INFO, Lemmatization successful ]
[2024-12-21 11:17:25,710, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-21 11:17:55,938, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-21 11:17:55,946, data_transformation, INFO, Transforming test data ]
[2024-12-21 11:17:56,008, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-21 11:17:56,050, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-21 11:17:56,051, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-21 11:17:56,460, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-21 11:17:56,462, data_transformation, INFO, Removing punctuations from text ]
[2024-12-21 11:17:59,262, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-21 11:17:59,263, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-21 11:17:59,835, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-21 11:17:59,838, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-21 11:18:08,510, data_transformation, INFO, Lemmatization successful ]
[2024-12-21 11:18:08,597, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-21 11:18:20,794, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-21 11:18:20,913, data_transformation, INFO, Transforming test data ]
[2024-12-21 11:18:25,986, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-21 11:18:25,988, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-21 12:35:19,866, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-21 12:35:19,868, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-21 12:35:19,872, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-21 12:35:19,874, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-21 12:35:20,407, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-21 12:35:21,942, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-21 12:35:21,944, data_ingestion, INFO, Initiating train test split ]
[2024-12-21 12:35:23,537, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-21 12:35:23,575, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-21 12:35:23,577, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-21 12:35:25,143, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-21 12:35:25,150, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-21 12:35:25,151, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-21 12:35:25,153, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-21 12:35:26,427, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-21T12:35:26.427045', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-21 12:35:26,429, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-21 12:35:26,430, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-21 12:35:26,441, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-21 12:35:26,445, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-21 12:35:26,446, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-21 12:35:26,448, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-21 12:35:27,504, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-21T12:35:27.504270', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-21 12:35:27,528, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-21 12:35:27,638, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-21 12:35:27,639, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-21 12:35:28,638, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-21 12:35:28,639, data_transformation, INFO, Removing punctuations from text ]
[2024-12-21 12:35:35,441, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-21 12:35:35,442, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-21 12:35:36,762, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-21 12:35:36,766, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-21 12:36:02,592, data_transformation, INFO, Lemmatization successful ]
[2024-12-21 12:36:02,596, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-21 12:36:36,613, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-21 12:36:36,622, data_transformation, INFO, Transforming test data ]
[2024-12-21 12:36:36,686, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-21 12:36:36,730, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-21 12:36:36,731, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-21 12:36:37,439, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-21 12:36:37,440, data_transformation, INFO, Removing punctuations from text ]
[2024-12-21 12:36:40,125, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-21 12:36:40,126, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-21 12:36:40,655, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-21 12:36:40,659, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-21 12:36:50,244, data_transformation, INFO, Lemmatization successful ]
[2024-12-21 12:36:50,338, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-21 12:37:03,061, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-21 12:37:03,171, data_transformation, INFO, Transforming test data ]
[2024-12-21 12:37:07,287, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-21 12:37:07,289, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-21 21:12:44,971, 1339312711, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 21:12:44,981, 1339312711, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 21:12:44,984, 1056863132, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 21:12:57,190, 1339312711, INFO, Fitting the UMAP to the given data ]
[2024-12-21 21:18:04,859, 1056863132, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 21:18:32,740, 1339312711, INFO, Fitting the UMAP to the given data ]
[2024-12-21 21:19:30,970, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-21 21:19:30,971, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-21 21:19:30,980, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-21 21:19:30,982, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-21 21:19:31,487, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-21 21:19:33,097, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-21 21:19:33,098, data_ingestion, INFO, Initiating train test split ]
[2024-12-21 21:19:34,708, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-21 21:19:34,738, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-21 21:19:34,739, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-21 21:19:36,197, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-21 21:19:36,207, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-21 21:19:36,208, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-21 21:19:36,210, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-21 21:19:37,770, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-21T21:19:37.770690', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-21 21:19:37,771, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-21 21:19:37,772, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-21 21:19:37,783, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-21 21:19:37,787, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-21 21:19:37,788, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-21 21:19:37,789, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-21 21:19:39,147, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-21T21:19:39.147751', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-21 21:19:39,165, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-21 21:19:39,327, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-21 21:19:39,330, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-21 21:19:40,348, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-21 21:19:40,349, data_transformation, INFO, Removing punctuations from text ]
[2024-12-21 21:19:48,189, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-21 21:19:48,190, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-21 21:19:49,801, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-21 21:19:49,804, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-21 21:20:17,787, data_transformation, INFO, Lemmatization successful ]
[2024-12-21 21:20:17,791, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-21 21:20:46,268, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-21 21:20:46,278, data_transformation, INFO, Transforming test data ]
[2024-12-21 21:20:46,338, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-21 21:20:46,380, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-21 21:20:46,380, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-21 21:20:46,758, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-21 21:20:46,758, data_transformation, INFO, Removing punctuations from text ]
[2024-12-21 21:20:49,552, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-21 21:20:49,552, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-21 21:20:50,093, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-21 21:20:50,097, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-21 21:20:58,915, data_transformation, INFO, Lemmatization successful ]
[2024-12-21 21:20:59,005, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-21 21:21:10,862, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-21 21:21:10,972, data_transformation, INFO, Transforming test data ]
[2024-12-21 21:21:14,828, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-21 21:21:14,830, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-21 21:21:24,444, 1288257958, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 21:21:24,446, 1288257958, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 21:21:24,447, 3421967610, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 21:21:24,472, 1288257958, INFO, Fitting the UMAP to the given data ]
[2024-12-21 21:26:32,652, 3421967610, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 21:26:35,586, 1288257958, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-21 21:28:02,271, 1288257958, INFO, Successfully transformed the data using UMAP model ]
[2024-12-21 21:28:02,272, 3421967610, INFO,  Transforming the data with the DBSCAN model ]
[2024-12-21 21:28:02,273, 3421967610, INFO,  Error in transforming the data with the DBSCAN model : 'DBSCAN' object has no attribute 'transform' ]
[2024-12-21 21:38:34,046, 1288257958, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 21:38:34,049, 1288257958, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 21:38:34,050, 3421967610, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 21:38:38,560, 1288257958, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 21:38:38,562, 1288257958, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 21:38:38,564, 2268417745, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 21:38:39,722, 1288257958, INFO, Fitting the UMAP to the given data ]
[2024-12-21 21:44:00,234, 2268417745, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 21:44:10,655, 1288257958, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-21 21:45:58,558, 1288257958, INFO, Successfully transformed the data using UMAP model ]
[2024-12-21 21:45:58,560, 2268417745, INFO,  Transforming the data with the DBSCAN model ]
[2024-12-21 21:45:58,561, 2268417745, INFO,  Error in transforming the data with the DBSCAN model : 'DBSCAN' object has no attribute 'predict' ]
[2024-12-21 21:47:30,491, 1288257958, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 21:47:30,493, 1288257958, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 21:47:30,495, 205375639, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 21:47:32,210, 1288257958, INFO, Fitting the UMAP to the given data ]
[2024-12-21 21:52:48,106, 205375639, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 22:07:53,864, 1288257958, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 22:07:53,866, 1288257958, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 22:07:53,868, 2979237254, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 22:07:57,119, 1288257958, INFO, Fitting the UMAP to the given data ]
[2024-12-21 22:13:16,792, 2979237254, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 22:13:35,766, 1288257958, INFO, Fitting the UMAP to the given data ]
[2024-12-21 22:14:29,595, 1288257958, INFO, Error fitting the data with UMP : CPUDispatcher(<function nn_descent at 0x0000017540AEBCA0>) returned a result with an error set ]
[2024-12-21 22:14:45,645, 1288257958, INFO, Fitting the UMAP to the given data ]
[2024-12-21 22:16:52,159, 2979237254, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 22:58:45,076, 1288257958, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 22:58:45,077, 1288257958, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 22:58:45,079, 2985086336, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 22:59:33,470, 1288257958, INFO, Fitting the UMAP to the given data ]
[2024-12-21 23:04:49,439, 2985086336, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 23:04:52,493, 1288257958, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-21 23:06:16,246, 1288257958, INFO, Successfully transformed the data using UMAP model ]
[2024-12-21 23:06:16,248, 2985086336, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-21 23:06:16,250, 2985086336, INFO, Error in transforming the data with the DBSCAN model: Expected 2D array, got scalar array instead:
array=None.
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ]
[2024-12-21 23:07:20,357, 1288257958, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 23:07:20,359, 1288257958, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 23:07:20,360, 2612399257, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 23:08:28,999, 1288257958, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 23:08:29,002, 1288257958, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 23:08:29,003, 2612399257, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 23:08:30,521, 1288257958, INFO, Fitting the UMAP to the given data ]
[2024-12-21 23:13:56,110, 2612399257, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 23:13:59,169, 1288257958, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-21 23:15:26,330, 1288257958, INFO, Successfully transformed the data using UMAP model ]
[2024-12-21 23:15:26,332, 2612399257, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-21 23:15:26,332, 2612399257, INFO, Error in transforming the data with the DBSCAN model: 'NoneType' object has no attribute 'ndim' ]
[2024-12-21 23:17:29,536, 646644459, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-21 23:17:29,539, 646644459, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-21 23:17:29,540, 2612399257, INFO, Initializing the DBSCAN clustering model ]
[2024-12-21 23:17:30,266, 646644459, INFO, Fitting the UMAP to the given data ]
[2024-12-21 23:22:58,695, 2612399257, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-21 23:23:01,843, 646644459, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-21 23:24:27,723, 646644459, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-21 23:24:27,725, 2612399257, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-21 23:24:28,438, 2612399257, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-22 12:08:27,911, 646644459, INFO, Fitting the UMAP to the given data ]
[2024-12-22 12:13:20,493, 646644459, INFO, Fitting the UMAP to the given data ]
[2024-12-22 12:19:08,546, 2612399257, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-22 12:19:19,570, 646644459, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-22 12:20:50,542, 646644459, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-22 12:20:50,544, 2612399257, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-22 12:20:51,346, 2612399257, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-22 12:27:08,589, 646644459, INFO, Fitting the UMAP to the given data ]
[2024-12-22 12:32:51,621, 2612399257, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-22 12:32:55,061, 2612399257, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-22 12:32:58,200, 2612399257, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-22 12:33:03,107, 646644459, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-22 12:34:26,437, 646644459, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-22 12:35:57,426, 646644459, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-22 12:35:57,427, 2612399257, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-22 12:35:58,204, 2612399257, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-22 15:52:27,665, 2120710121, INFO, Initialized clustering model ]
[2024-12-22 15:52:27,667, 646644459, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-22 15:52:27,670, 646644459, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-22 15:52:27,671, 2612399257, INFO, Initializing the DBSCAN clustering model ]
[2024-12-22 15:52:27,672, 2120710121, INFO, Fitting the training data to the clustering model ]
[2024-12-22 15:52:27,674, 2120710121, INFO,  Error in initiating the clustering pipeline : 'DataTransformationConfig' object has no attribute 'transformed_train_file' ]
[2024-12-22 15:53:42,464, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-22 15:53:42,466, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-22 15:53:42,468, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-22 15:53:42,470, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-22 15:53:42,823, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-22 15:53:44,426, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-22 15:53:44,427, data_ingestion, INFO, Initiating train test split ]
[2024-12-22 15:53:46,104, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-22 15:53:46,138, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-22 15:53:46,140, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-22 15:53:47,632, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 15:53:47,644, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-22 15:53:47,646, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-22 15:53:47,648, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-22 15:53:48,808, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-22T15:53:48.808931', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-22 15:53:48,809, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-22 15:53:48,809, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-22 15:53:48,812, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 15:53:48,816, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-22 15:53:48,817, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-22 15:53:48,817, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-22 15:53:49,873, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-22T15:53:49.873114', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-22 15:53:49,891, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 15:53:49,990, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 15:53:49,990, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 15:53:51,034, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 15:53:51,036, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 15:53:58,753, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-22 15:53:58,754, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-22 15:54:00,298, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-22 15:54:00,303, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-22 15:54:30,491, data_transformation, INFO, Lemmatization successful ]
[2024-12-22 15:54:30,495, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-22 15:55:01,729, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-22 15:55:01,740, data_transformation, INFO, Transforming test data ]
[2024-12-22 15:55:01,808, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 15:55:01,855, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 15:55:01,856, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 15:55:02,283, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 15:55:02,284, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 15:55:05,315, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-22 15:55:05,317, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-22 15:55:05,895, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-22 15:55:05,897, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-22 15:55:16,141, data_transformation, INFO, Lemmatization successful ]
[2024-12-22 15:55:16,244, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-22 15:55:29,707, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-22 15:55:29,839, data_transformation, INFO, Transforming test data ]
[2024-12-22 15:55:33,816, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-22 15:55:33,817, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-22 15:55:33,959, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-22 15:55:45,538, 2120710121, INFO, Initialized clustering model ]
[2024-12-22 15:55:45,540, 705939419, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-22 15:55:45,541, 705939419, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-22 15:55:45,542, 3653109047, INFO, Initializing the DBSCAN clustering model ]
[2024-12-22 15:55:45,543, 2120710121, INFO, Fitting the training data to the clustering model ]
[2024-12-22 15:55:45,545, 705939419, INFO, Fitting the UMAP to the given data ]
[2024-12-22 15:55:45,547, 705939419, INFO, Error fitting the data with UMP : could not convert string to float: 'artifacts\\transformed_train_data.csv' ]
[2024-12-22 15:55:45,550, 2120710121, INFO,  Error in initiating the clustering pipeline : could not convert string to float: 'artifacts\\transformed_train_data.csv' ]
[2024-12-22 16:01:40,679, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-22 16:01:40,680, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-22 16:01:40,683, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-22 16:01:40,684, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-22 16:01:41,037, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-22 16:01:42,672, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-22 16:01:42,673, data_ingestion, INFO, Initiating train test split ]
[2024-12-22 16:01:44,406, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-22 16:01:44,438, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-22 16:01:44,439, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-22 16:01:45,797, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 16:01:45,800, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-22 16:01:45,801, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-22 16:01:45,802, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-22 16:01:46,944, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-22T16:01:46.944983', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-22 16:01:46,945, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-22 16:01:46,946, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-22 16:01:46,950, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 16:01:46,954, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-22 16:01:46,955, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-22 16:01:46,956, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-22 16:01:48,209, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-22T16:01:48.209601', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-22 16:01:48,228, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 16:01:48,344, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 16:01:48,345, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 16:01:49,518, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 16:01:49,520, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 16:01:58,409, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-22 16:01:58,410, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-22 16:01:59,948, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-22 16:01:59,952, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-22 16:02:26,886, data_transformation, INFO, Lemmatization successful ]
[2024-12-22 16:02:26,889, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-22 16:02:58,921, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-22 16:02:58,932, data_transformation, INFO, Transforming test data ]
[2024-12-22 16:02:58,998, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 16:02:59,040, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 16:02:59,041, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 16:02:59,412, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 16:02:59,413, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 16:03:02,391, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-22 16:03:02,391, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-22 16:03:03,012, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-22 16:03:03,015, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-22 16:03:14,777, data_transformation, INFO, Lemmatization successful ]
[2024-12-22 16:03:14,878, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-22 16:03:29,176, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-22 16:03:29,290, data_transformation, INFO, Transforming test data ]
[2024-12-22 16:03:33,320, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-22 16:03:33,321, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-22 16:03:33,392, data_transformation, INFO, Error in initiating the data transformation pipeline Shape of passed values is (103304, 26), indices imply (103304, 2) ]
[2024-12-22 16:10:05,295, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-22 16:10:05,296, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-22 16:10:05,298, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-22 16:10:05,299, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-22 16:10:05,683, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-22 16:10:07,430, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-22 16:10:07,432, data_ingestion, INFO, Initiating train test split ]
[2024-12-22 16:10:08,906, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-22 16:10:08,933, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-22 16:10:08,935, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-22 16:10:10,410, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 16:10:10,413, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-22 16:10:10,414, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-22 16:10:10,415, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-22 16:10:11,709, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-22T16:10:11.708574', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-22 16:10:11,710, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-22 16:10:11,711, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-22 16:10:11,714, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 16:10:11,719, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-22 16:10:11,720, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-22 16:10:11,721, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-22 16:10:12,750, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-22T16:10:12.750829', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-22 16:10:12,768, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 16:10:12,873, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 16:10:12,874, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 16:10:13,855, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 16:10:13,857, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 16:10:21,279, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-22 16:10:21,280, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-22 16:10:22,636, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-22 16:10:22,640, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-22 16:10:49,026, data_transformation, INFO, Lemmatization successful ]
[2024-12-22 16:10:49,029, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-22 16:11:21,909, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-22 16:11:21,922, data_transformation, INFO, Transforming test data ]
[2024-12-22 16:11:22,000, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 16:11:22,045, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 16:11:22,047, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 16:11:22,495, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 16:11:22,498, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 16:11:25,779, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-22 16:11:25,781, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-22 16:11:26,368, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-22 16:11:26,373, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-22 16:11:37,835, data_transformation, INFO, Lemmatization successful ]
[2024-12-22 16:11:37,934, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-22 16:11:51,763, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-22 16:11:51,877, data_transformation, INFO, Transforming test data ]
[2024-12-22 16:11:55,862, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-22 16:11:55,864, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-22 16:12:00,650, data_transformation, INFO, Returning the input train feature as an array and test feature as array respectively ]
[2024-12-22 16:12:21,520, 2120710121, INFO, Initialized clustering model ]
[2024-12-22 16:12:21,522, 705939419, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-22 16:12:21,524, 705939419, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-22 16:12:21,525, 3653109047, INFO, Initializing the DBSCAN clustering model ]
[2024-12-22 16:12:21,527, 2120710121, INFO, Fitting the training data to the clustering model ]
[2024-12-22 16:12:21,528, 705939419, INFO, Fitting the UMAP to the given data ]
[2024-12-22 16:12:21,530, 705939419, INFO, Error fitting the data with UMP : could not convert string to float: 'artifacts\\transformed_train_data.csv' ]
[2024-12-22 16:12:21,531, 2120710121, INFO,  Error in initiating the clustering pipeline : could not convert string to float: 'artifacts\\transformed_train_data.csv' ]
[2024-12-22 16:14:33,421, 2366838217, INFO, Initialized clustering model ]
[2024-12-22 16:14:33,423, 705939419, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-22 16:14:33,424, 705939419, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-22 16:14:33,425, 3653109047, INFO, Initializing the DBSCAN clustering model ]
[2024-12-22 16:14:33,428, 2366838217, INFO, Fitting the training data to the clustering model ]
[2024-12-22 16:14:33,873, 705939419, INFO, Fitting the UMAP to the given data ]
[2024-12-22 16:20:17,764, 3653109047, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-22 16:20:20,629, 3653109047, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-22 16:20:23,612, 3653109047, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-22 16:20:23,613, 2366838217, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-22 16:20:57,848, 2366838217, INFO, Transforming the test data to ]
[2024-12-22 16:20:58,066, 705939419, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-22 16:21:58,987, 705939419, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-22 16:21:58,988, 3653109047, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-22 16:21:59,654, 3653109047, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-22 16:21:59,655, 2366838217, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-22 21:22:07,332, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-22 21:22:07,336, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-22 21:22:07,348, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-22 21:22:07,350, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-22 21:22:07,833, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-22 21:22:09,715, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-22 21:22:09,716, data_ingestion, INFO, Initiating train test split ]
[2024-12-22 21:22:11,484, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-22 21:22:34,320, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-22 21:22:34,321, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-22 21:22:34,324, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-22 21:22:34,325, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-22 21:22:34,748, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-22 21:22:36,458, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-22 21:22:36,460, data_ingestion, INFO, Initiating train test split ]
[2024-12-22 21:22:38,308, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-22 21:44:55,514, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 21:46:33,644, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 21:46:33,650, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 21:46:33,652, data_transformation, INFO, Error in pre-processing the text: Can only use .str accessor with string values! ]
[2024-12-22 21:46:52,171, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 21:46:52,177, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 21:46:52,179, data_transformation, INFO, Error in pre-processing the text: 'str' object has no attribute 'str' ]
[2024-12-22 21:47:36,319, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 21:47:36,324, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 21:47:36,325, data_transformation, INFO, Error in pre-processing the text: Can only use .str accessor with string values! ]
[2024-12-22 21:49:21,330, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 21:49:21,341, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 21:49:21,444, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 21:49:21,445, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 21:49:22,478, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 21:49:22,480, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 21:49:25,707, data_transformation, INFO, Error in pre-processing the text: 'float' object is not iterable ]
[2024-12-22 21:51:02,437, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-22 21:51:02,438, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-22 21:51:02,441, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-22 21:51:02,443, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-22 21:51:02,765, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-22 21:51:04,541, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-22 21:51:04,543, data_ingestion, INFO, Initiating train test split ]
[2024-12-22 21:51:06,325, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-22 21:51:09,379, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 21:51:09,391, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 21:51:09,520, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 21:51:09,521, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 21:51:10,547, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 21:51:10,548, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 21:51:13,441, data_transformation, INFO, Error in pre-processing the text: 'float' object is not iterable ]
[2024-12-22 21:54:31,563, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-22 21:54:31,566, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-22 21:54:31,567, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-22 21:54:31,569, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-22 21:54:31,978, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-22 21:54:33,907, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-22 21:54:33,908, data_ingestion, INFO, Initiating train test split ]
[2024-12-22 21:54:35,726, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-22 21:54:39,811, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 21:54:39,828, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 21:54:39,959, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 21:54:39,960, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 21:54:41,247, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 21:54:41,249, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 21:54:44,041, data_transformation, INFO, Error in pre-processing the text: 'float' object is not iterable ]
[2024-12-22 21:55:43,640, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-22 21:55:43,642, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-22 21:55:43,644, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-22 21:55:43,645, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-22 21:55:43,992, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-22 21:55:47,281, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-22 21:55:47,282, data_ingestion, INFO, Initiating train test split ]
[2024-12-22 21:55:50,450, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-22 21:55:51,886, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-22 21:55:51,902, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-22 21:55:52,025, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-22 21:55:52,026, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-22 21:55:53,207, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-22 21:55:53,208, data_transformation, INFO, Removing punctuations from text ]
[2024-12-22 21:55:53,209, data_transformation, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-23 12:18:27,634, data_clustering, INFO, Initialized clustering model ]
[2024-12-23 12:18:27,635, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-23 12:18:27,637, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-23 12:18:27,638, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-23 12:18:27,639, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-23 12:18:28,108, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-23 12:23:53,088, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-23 12:23:56,235, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 12:23:59,288, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 12:23:59,289, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-23 12:24:35,297, data_clustering, INFO, Transforming the test data to ]
[2024-12-23 12:24:35,560, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-23 12:25:34,239, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-23 12:25:34,240, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 12:25:34,992, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 12:25:34,993, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-23 12:25:35,799, data_clustering, INFO,  Error in initiating the clustering pipeline : Length of values (103304) does not match length of index (103306) ]
[2024-12-23 12:52:27,590, data_clustering, INFO, Initialized clustering model ]
[2024-12-23 12:52:27,592, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-23 12:52:27,593, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-23 12:52:27,595, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-23 12:52:27,596, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-23 12:52:28,084, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-23 12:57:30,973, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-23 12:57:34,083, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 12:57:37,287, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 12:57:37,288, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-23 12:57:59,511, data_clustering, INFO, Transforming the test data to ]
[2024-12-23 12:57:59,704, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-23 12:58:56,938, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-23 12:58:56,939, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 12:58:57,619, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 12:58:57,621, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-23 12:58:58,336, data_clustering, INFO,  Error in initiating the clustering pipeline : Length of values (103304) does not match length of index (103306) ]
[2024-12-23 13:01:20,584, data_clustering, INFO, Initialized clustering model ]
[2024-12-23 13:01:20,585, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-23 13:01:20,586, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-23 13:01:20,587, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-23 13:01:20,588, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-23 13:01:21,096, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-23 13:07:01,290, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-23 13:07:04,317, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 13:07:06,899, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 13:07:06,900, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-23 13:07:40,964, data_clustering, INFO, Transforming the test data to ]
[2024-12-23 13:07:41,205, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-23 13:08:40,943, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-23 13:08:40,945, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 13:08:41,599, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 13:08:41,600, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-23 13:08:43,734, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-23 13:08:44,225, data_clustering, INFO,  Error in initiating the clustering pipeline : Length of values (103304) does not match length of index (44275) ]
[2024-12-23 13:10:19,075, data_clustering, INFO, Initialized clustering model ]
[2024-12-23 13:10:19,077, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-23 13:10:19,078, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-23 13:10:19,080, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-23 13:10:19,081, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-23 13:10:19,570, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-23 13:15:57,843, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-23 13:16:00,810, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 13:16:03,632, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 13:16:03,633, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-23 13:16:38,418, data_clustering, INFO, Transforming the test data to ]
[2024-12-23 13:16:38,612, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-23 13:17:40,425, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-23 13:17:40,427, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 13:17:40,979, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 13:17:40,981, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-23 13:17:42,982, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-23 13:17:43,837, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-23 13:17:43,838, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-23 18:56:02,964, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 18:56:02,966, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 18:56:02,969, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 18:56:02,970, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 18:56:03,472, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 18:56:05,185, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 18:56:05,187, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 18:56:06,660, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-23 18:56:06,688, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-23 18:56:06,689, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-23 18:56:07,976, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 18:56:07,990, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 18:56:07,991, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 18:56:07,993, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 18:56:09,344, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T18:56:09.344758', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 18:56:09,345, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-23 18:56:09,346, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-23 18:56:09,354, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 18:56:09,357, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 18:56:09,358, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 18:56:09,360, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 18:56:10,483, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T18:56:10.483738', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 18:56:10,504, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 18:56:10,506, data_transformation, INFO, Error in pre-processing the text: 'str' object has no attribute 'str' ]
[2024-12-23 18:56:10,508, data_transformation, INFO, Error in initiating the data transformation pipeline 'str' object has no attribute 'str' ]
[2024-12-23 19:02:05,556, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 19:02:05,558, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 19:02:05,559, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 19:02:05,561, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 19:02:05,895, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 19:02:07,461, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 19:02:07,462, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 19:02:08,909, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-23 19:02:08,939, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-23 19:02:08,940, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-23 19:02:10,283, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:02:10,287, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:02:10,289, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:02:10,290, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:02:11,547, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:02:11.547219', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:02:11,548, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-23 19:02:11,549, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-23 19:02:11,554, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:02:11,556, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:02:11,557, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:02:11,558, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:02:12,674, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:02:12.674243', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:02:12,691, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 19:02:12,822, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 19:02:12,824, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 19:02:13,855, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 19:02:13,856, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 19:02:13,857, data_transformation, INFO, Error in pre-processing the text: expected string or buffer ]
[2024-12-23 19:02:13,859, data_transformation, INFO, Error in initiating the data transformation pipeline expected string or buffer ]
[2024-12-23 19:04:40,674, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 19:04:40,676, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 19:04:40,676, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 19:04:40,679, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 19:04:41,000, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 19:04:42,661, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 19:04:42,663, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 19:04:44,199, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-23 19:04:44,230, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-23 19:04:44,231, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-23 19:04:45,488, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:04:45,490, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:04:45,491, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:04:45,493, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:04:46,557, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:04:46.557161', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:04:46,558, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-23 19:04:46,559, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-23 19:04:46,565, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:04:46,566, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:04:46,567, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:04:46,568, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:04:47,765, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:04:47.765823', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:04:47,787, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 19:04:47,917, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 19:04:47,918, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 19:04:49,039, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 19:04:49,040, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 19:04:55,629, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 19:04:55,631, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 19:04:55,632, data_transformation, INFO, Error in pre-processing the text: unhashable type: 'list' ]
[2024-12-23 19:04:55,632, data_transformation, INFO, Error in initiating the data transformation pipeline unhashable type: 'list' ]
[2024-12-23 19:05:29,011, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 19:05:29,013, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 19:05:29,014, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 19:05:29,016, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 19:05:29,448, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 19:07:04,860, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 19:07:04,861, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 19:07:04,863, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 19:07:04,865, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 19:07:05,204, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 19:07:06,854, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 19:07:06,854, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 19:07:08,267, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-23 19:07:08,294, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-23 19:07:08,296, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-23 19:07:09,546, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:07:09,549, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:07:09,550, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:07:09,551, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:07:10,883, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:07:10.883020', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:07:10,884, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-23 19:07:10,885, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-23 19:07:10,890, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:07:10,891, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:07:10,892, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:07:10,893, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:07:12,037, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:07:12.037932', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:07:12,054, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 19:07:12,167, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 19:07:12,169, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 19:07:13,152, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 19:07:13,154, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 19:07:19,977, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 19:07:19,978, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 19:07:21,371, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 19:07:21,375, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 19:07:46,468, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 19:07:46,471, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 19:08:17,225, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 19:08:17,231, data_transformation, INFO, Transforming test data ]
[2024-12-23 19:08:17,291, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 19:08:17,334, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 19:08:17,335, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 19:08:17,713, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 19:08:17,714, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 19:08:20,462, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 19:08:20,464, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 19:08:21,014, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 19:08:21,018, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 19:08:29,449, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 19:08:29,537, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 19:08:41,587, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 19:08:41,703, data_transformation, INFO, Transforming test data ]
[2024-12-23 19:08:45,511, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-23 19:08:45,514, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-23 19:08:49,689, data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-23 19:46:57,804, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 19:46:57,806, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 19:46:57,808, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 19:46:57,810, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 19:46:58,192, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 19:46:59,979, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 19:46:59,980, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 19:47:01,704, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-23 19:47:01,734, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-23 19:47:01,736, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-23 19:47:03,102, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:47:03,110, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:47:03,112, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:47:03,114, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:47:05,047, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:47:05.047924', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:47:05,052, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-23 19:47:05,055, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-23 19:47:05,073, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:47:05,081, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:47:05,087, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:47:05,091, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:47:07,322, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:47:07.322866', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:47:07,339, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 19:47:07,453, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 19:47:07,454, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 19:47:08,414, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 19:47:08,417, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 19:47:16,012, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 19:47:16,013, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 19:47:17,532, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 19:47:17,536, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 19:47:44,520, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 19:47:44,523, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 19:48:16,591, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 19:48:16,598, data_transformation, INFO, Transforming test data ]
[2024-12-23 19:48:16,665, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 19:48:16,717, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 19:48:16,719, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 19:48:17,107, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 19:48:17,108, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 19:48:19,928, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 19:48:19,929, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 19:48:20,573, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 19:48:20,577, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 19:48:30,315, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 19:48:30,414, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 19:48:43,516, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 19:48:43,628, data_transformation, INFO, Transforming test data ]
[2024-12-23 19:48:47,120, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-23 19:48:47,121, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-23 19:48:51,560, data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-23 19:48:51,844, data_clustering, INFO, Initialized clustering model ]
[2024-12-23 19:48:51,845, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-23 19:48:51,846, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-23 19:48:51,847, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-23 19:48:51,848, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-23 19:48:51,849, data_clustering, INFO,  Error in initiating the clustering pipeline : 'numpy.ndarray' object has no attribute 'values' ]
[2024-12-23 19:50:09,916, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 19:50:09,917, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 19:50:09,920, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 19:50:09,921, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 19:50:10,249, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 19:50:11,936, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 19:50:11,938, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 19:50:55,195, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 19:50:55,197, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 19:50:55,199, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 19:50:55,201, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 19:50:55,531, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 19:50:57,115, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 19:50:57,116, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 19:50:58,809, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-23 19:50:58,840, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-23 19:50:58,841, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-23 19:51:00,345, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:51:00,349, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:51:00,351, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:51:00,352, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:51:01,596, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:51:01.596452', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:51:01,597, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-23 19:51:01,598, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-23 19:51:01,603, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 19:51:01,605, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 19:51:01,606, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 19:51:01,607, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 19:51:02,845, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T19:51:02.845152', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 19:51:02,866, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 19:51:02,994, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 19:51:02,995, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 19:51:04,123, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 19:51:04,125, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 19:51:11,348, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 19:51:11,349, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 19:51:13,016, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 19:51:13,019, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 19:51:40,534, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 19:51:40,537, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 19:52:11,982, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 19:52:11,990, data_transformation, INFO, Transforming test data ]
[2024-12-23 19:52:12,052, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 19:52:12,097, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 19:52:12,098, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 19:52:12,478, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 19:52:12,479, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 19:52:15,600, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 19:52:15,601, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 19:52:16,176, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 19:52:16,180, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 19:52:25,993, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 19:52:26,088, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 19:52:41,301, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 19:52:41,416, data_transformation, INFO, Transforming test data ]
[2024-12-23 19:52:45,355, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-23 19:52:45,357, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-23 19:52:49,546, data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-23 19:52:49,835, data_clustering, INFO, Initialized clustering model ]
[2024-12-23 19:52:49,835, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-23 19:52:49,836, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-23 19:52:49,837, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-23 19:52:49,838, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-23 19:52:49,839, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-23 19:58:35,772, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-23 19:58:38,901, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 19:58:41,617, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 19:58:41,619, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-23 19:59:15,082, data_clustering, INFO, Transforming the test data to ]
[2024-12-23 19:59:15,084, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-23 20:00:14,989, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-23 20:00:14,990, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 20:00:15,685, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 20:00:15,686, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-23 20:00:17,544, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-23 20:00:17,545, data_clustering, INFO,  Error in initiating the clustering pipeline : read_csv() got an unexpected keyword argument 'index' ]
[2024-12-23 20:02:00,028, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 20:02:00,029, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 20:02:00,031, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 20:02:00,032, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 20:02:00,410, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 20:02:02,186, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 20:02:02,187, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 20:02:03,870, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-23 20:02:03,899, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-23 20:02:03,900, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-23 20:02:05,197, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:02:05,200, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:02:05,202, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 20:02:05,203, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 20:02:06,314, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T20:02:06.314349', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 20:02:06,316, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-23 20:02:06,317, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-23 20:02:06,322, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:02:06,324, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:02:06,325, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 20:02:06,326, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 20:02:09,569, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T20:02:09.569110', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 20:02:09,639, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:02:09,868, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:02:09,869, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:02:10,874, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 20:02:10,876, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 20:02:18,068, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 20:02:18,070, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 20:02:19,399, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 20:02:19,403, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 20:02:46,530, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 20:02:46,533, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 20:03:18,220, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 20:03:18,228, data_transformation, INFO, Transforming test data ]
[2024-12-23 20:03:18,291, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:03:18,346, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:03:18,347, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:03:18,777, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 20:03:18,779, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 20:03:21,879, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 20:03:21,881, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 20:03:22,495, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 20:03:22,498, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 20:03:32,034, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 20:03:32,130, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 20:03:46,149, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 20:03:46,268, data_transformation, INFO, Transforming test data ]
[2024-12-23 20:03:50,363, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-23 20:03:50,365, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-23 20:03:54,786, data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-23 20:03:55,071, data_clustering, INFO, Initialized clustering model ]
[2024-12-23 20:03:55,072, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-23 20:03:55,073, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-23 20:03:55,075, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-23 20:03:55,077, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-23 20:03:55,077, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-23 20:09:33,459, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-23 20:09:36,562, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 20:09:39,759, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 20:09:39,760, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-23 20:10:15,852, data_clustering, INFO, Transforming the test data to ]
[2024-12-23 20:10:15,854, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-23 20:11:20,622, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-23 20:11:20,624, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 20:11:21,213, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 20:11:21,215, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-23 20:11:23,132, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-23 20:11:23,984, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-23 20:11:23,985, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-23 20:18:45,302, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:18:45,306, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:19:10,916, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:19:10,963, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:19:10,964, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:19:10,965, data_transformation, INFO, Error in pre-processing the text: 'str' object has no attribute 'str' ]
[2024-12-23 20:21:15,326, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:21:15,330, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:22:52,051, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:22:52,055, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:23:05,352, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:23:05,356, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:23:05,358, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:23:05,360, data_transformation, INFO, Error in pre-processing the text: 'str' object has no attribute 'str' ]
[2024-12-23 20:23:27,575, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:23:27,582, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:23:27,582, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:23:27,584, data_transformation, INFO, Error in pre-processing the text: 'str' object has no attribute 'str' ]
[2024-12-23 20:23:42,366, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:23:42,372, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:23:42,374, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:23:42,375, data_transformation, INFO, Error in pre-processing the text: 'str' object has no attribute 'str' ]
[2024-12-23 20:24:07,484, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:24:07,489, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:24:07,491, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:24:07,492, data_transformation, INFO, Error in pre-processing the text: 'str' object has no attribute 'str' ]
[2024-12-23 20:25:03,162, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-23 20:25:03,164, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-23 20:25:03,167, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-23 20:25:03,168, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-23 20:25:03,529, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-23 20:25:05,104, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-23 20:25:05,105, data_ingestion, INFO, Initiating train test split ]
[2024-12-23 20:25:06,855, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-23 20:25:06,891, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-23 20:25:06,893, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-23 20:25:08,327, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:25:08,331, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:25:08,332, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 20:25:08,333, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 20:25:09,587, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T20:25:09.587772', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 20:25:09,588, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-23 20:25:09,589, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-23 20:25:09,594, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:25:09,596, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:25:09,597, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-23 20:25:09,598, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-23 20:25:10,814, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-23T20:25:10.814020', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-23 20:25:10,832, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:25:10,948, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:25:10,949, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:25:11,951, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 20:25:11,953, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 20:25:20,571, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 20:25:20,573, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 20:25:21,905, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 20:25:21,909, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 20:25:49,115, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 20:25:49,118, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 20:26:20,821, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 20:26:20,829, data_transformation, INFO, Transforming test data ]
[2024-12-23 20:26:20,892, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:26:20,941, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:26:20,942, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:26:21,375, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 20:26:21,377, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 20:26:24,255, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 20:26:24,256, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 20:26:24,878, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 20:26:24,882, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 20:26:34,794, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 20:26:34,890, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-23 20:26:48,379, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-23 20:26:48,519, data_transformation, INFO, Transforming test data ]
[2024-12-23 20:26:52,465, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-23 20:26:52,466, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-23 20:26:56,854, data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-23 20:26:57,166, data_clustering, INFO, Initialized clustering model ]
[2024-12-23 20:26:57,166, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-23 20:26:57,167, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-23 20:26:57,168, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-23 20:26:57,169, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-23 20:26:57,170, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-23 20:32:47,192, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-23 20:32:50,388, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 20:32:53,431, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 20:32:53,432, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-23 20:33:28,205, data_clustering, INFO, Transforming the test data to ]
[2024-12-23 20:33:28,207, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-23 20:34:28,179, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-23 20:34:28,181, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-23 20:34:28,857, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-23 20:34:28,859, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-23 20:34:30,783, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-23 20:34:31,624, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-23 20:34:31,625, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-23 20:35:01,227, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:35:01,229, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:35:29,161, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:35:29,166, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:35:29,167, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:35:29,216, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 20:35:29,226, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 20:35:29,517, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 20:35:29,518, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 20:35:29,578, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 20:35:29,581, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 20:35:30,482, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 20:39:29,538, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:39:29,542, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:40:03,141, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:40:03,274, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:40:03,276, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:40:04,211, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 20:40:04,213, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 20:40:11,162, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 20:40:11,163, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 20:40:12,295, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 20:40:12,299, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 20:40:34,689, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 20:55:04,182, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:55:04,187, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:55:11,574, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:55:11,576, data_transformation, INFO, Error in pre-processing the text: 'str' object has no attribute 'str' ]
[2024-12-23 20:56:22,412, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 20:56:22,417, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 20:56:25,743, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 20:56:25,858, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 20:56:25,860, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 20:56:27,039, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 20:56:27,041, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 20:56:34,229, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 20:56:34,230, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 20:56:35,657, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 20:56:35,660, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 20:57:02,975, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 21:00:45,356, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 21:00:45,359, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 21:00:48,577, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 21:00:48,688, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 21:00:48,689, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 21:00:50,181, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 21:00:50,182, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 21:00:56,978, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 21:00:56,979, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 21:00:58,262, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 21:00:58,265, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 21:01:21,874, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 21:01:58,240, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 21:01:58,362, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 21:01:58,364, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 21:01:59,273, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 21:01:59,274, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 21:02:07,889, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 21:02:07,890, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 21:02:09,140, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 21:02:09,144, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 21:02:31,822, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 22:19:17,170, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 22:19:17,173, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 22:19:39,976, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 22:19:40,147, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 22:19:40,148, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 22:19:42,158, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 22:19:42,159, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 22:19:48,659, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 22:19:48,660, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 22:19:50,003, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 22:19:50,007, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 22:20:11,159, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 22:20:30,355, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 22:20:30,474, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 22:20:30,475, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 22:20:31,627, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 22:20:31,628, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 22:20:39,271, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 22:20:39,272, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 22:20:40,588, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 22:20:40,590, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 22:21:02,214, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 22:24:20,513, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 22:24:20,517, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 22:24:23,344, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 22:24:23,512, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 22:24:23,513, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 22:24:24,626, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 22:24:24,629, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 22:24:31,429, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 22:24:31,430, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 22:24:32,826, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 22:24:32,829, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 22:24:57,993, data_transformation, INFO, Error in lemmatization: 'review_full' ]
[2024-12-23 22:26:06,943, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 22:26:06,946, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 22:26:58,653, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 22:26:58,659, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 22:27:00,220, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 22:27:00,344, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 22:27:00,345, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 22:27:01,456, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 22:27:01,458, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 22:27:08,277, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 22:27:08,278, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 22:27:09,716, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 22:27:09,720, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 22:27:09,721, data_transformation, INFO, Error in lemmatization: 'review_full' ]
[2024-12-23 22:28:27,880, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 22:28:27,884, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 22:28:29,002, data_transformation, INFO, Error in pre-processing the text: 'review_full' ]
[2024-12-23 22:30:51,123, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 22:30:51,127, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 22:30:54,986, data_transformation, INFO, Error in pre-processing the text: "None of [Index(['review_full'], dtype='object')] are in the [index]" ]
[2024-12-23 22:32:04,773, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 22:32:04,777, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 22:35:12,956, data_transformation, INFO, getting set of stopwords from NLTK from English language ]
[2024-12-23 22:35:12,961, data_transformation, INFO, WordNetLemmatizer from NLTK library is instantiated ]
[2024-12-23 22:35:15,649, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 22:35:15,773, data_transformation, INFO, Transforming all letters into lowercase is successful ]
[2024-12-23 22:35:15,774, data_transformation, INFO, Splitting the sentences into words (Tokenization) ]
[2024-12-23 22:35:16,842, data_transformation, INFO, Splitting the sentences into words is successful (Tokenization) ]
[2024-12-23 22:35:16,844, data_transformation, INFO, Removing punctuations from text ]
[2024-12-23 22:35:23,281, data_transformation, INFO, Removing punctuations from text successful ]
[2024-12-23 22:35:23,282, data_transformation, INFO, Removing the stopwords from the tokenized words ]
[2024-12-23 22:35:24,668, data_transformation, INFO, Removing the stopwords from the tokenized words is successful ]
[2024-12-23 22:35:24,672, data_transformation, INFO, Initiating the lemmatization of the remaining words after stopword removal ]
[2024-12-23 22:35:49,289, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 22:43:02,731, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 22:43:02,736, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 22:43:04,426, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 22:43:04,558, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 22:43:10,527, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 22:43:10,530, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 22:43:34,757, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 22:43:46,692, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 22:43:46,799, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 22:43:52,298, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 22:43:52,310, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 22:44:15,088, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 22:44:58,740, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 22:44:58,743, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 22:45:05,396, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 22:45:05,507, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 22:45:11,104, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 22:45:11,108, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 22:45:33,149, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 23:01:23,547, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 23:01:23,556, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 23:01:23,777, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 23:01:23,786, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 23:01:24,570, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 23:03:01,051, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 23:03:01,054, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 23:03:42,051, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 23:03:42,054, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 23:03:46,572, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 23:03:46,577, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 23:03:46,793, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 23:03:46,794, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 23:03:47,551, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 23:06:02,596, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 23:06:02,598, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 23:06:06,877, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 23:06:06,882, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 23:06:07,166, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 23:06:07,168, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 23:06:07,980, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 23:08:31,066, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 23:08:31,068, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 23:08:33,851, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 23:08:33,856, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 23:08:34,122, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 23:08:34,123, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 23:08:34,895, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 23:12:58,444, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 23:12:58,446, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 23:13:01,427, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 23:13:01,434, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 23:13:01,712, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 23:13:01,714, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 23:13:02,534, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 23:15:00,836, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 23:15:00,838, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 23:15:01,755, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 23:15:01,761, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 23:15:01,986, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 23:15:01,987, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 23:15:02,778, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 23:16:48,339, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 23:16:48,342, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 23:16:51,235, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 23:16:51,240, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 23:16:51,473, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 23:16:51,474, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 23:16:52,259, data_transformation, INFO, Lemmatization successful ]
[2024-12-23 23:18:38,153, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-23 23:18:38,156, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-23 23:18:42,457, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-23 23:18:42,591, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-23 23:18:48,563, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-23 23:18:48,566, data_transformation, INFO, Lemmatizing the words ]
[2024-12-23 23:19:11,169, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 00:07:14,460, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 00:07:14,478, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 00:07:20,187, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 00:07:20,190, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 00:07:23,291, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 00:07:23,399, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 00:07:29,583, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 00:07:29,586, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 00:07:52,270, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 00:13:56,592, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 00:13:56,601, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 00:13:58,776, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 00:13:58,881, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 00:14:05,127, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 00:14:05,130, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 00:14:29,669, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 00:25:50,815, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 00:25:50,818, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 00:25:54,724, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 00:25:54,827, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 00:26:00,881, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 00:26:00,885, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 00:26:24,050, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 00:35:26,064, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 00:35:26,072, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 00:35:28,746, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 00:35:28,885, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 00:35:35,203, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 00:35:35,205, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 00:35:57,967, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 10:32:29,799, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 10:32:29,803, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 10:32:56,270, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 10:32:56,274, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 10:32:59,264, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 10:32:59,435, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 10:33:05,367, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 10:33:05,371, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 10:33:29,009, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 10:52:03,244, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 10:52:03,248, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 10:52:05,280, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 10:52:05,449, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 10:52:11,195, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 10:52:11,199, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 10:52:34,812, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 11:25:21,375, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 11:25:21,381, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 11:25:23,189, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 11:25:23,298, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 11:25:29,336, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 11:25:29,339, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 11:25:52,131, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 12:48:55,695, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 12:48:55,701, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 12:48:57,893, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 12:48:58,076, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 12:49:03,582, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 12:49:03,585, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 12:49:23,658, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 18:07:47,065, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 18:07:47,085, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 18:07:50,197, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 18:07:50,316, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 18:07:56,796, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 18:07:56,799, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 18:08:20,334, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:06:27,776, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:07:21,650, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:07:21,652, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:07:23,310, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 20:07:23,327, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 20:07:23,328, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 20:07:23,329, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 20:07:24,645, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T20:07:24.645070', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 20:07:24,646, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-24 20:07:24,647, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 20:07:24,666, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 20:07:24,671, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 20:07:24,672, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 20:07:24,673, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 20:07:25,952, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T20:07:25.952651', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 20:07:25,977, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:07:26,111, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:07:32,435, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:07:32,438, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:08:02,035, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:08:02,037, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:09:58,951, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:09:58,960, data_transformation, INFO, Transforming test data ]
[2024-12-24 20:09:58,975, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:09:59,028, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:10:01,790, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:10:01,793, data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:10:12,244, data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:10:12,250, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:11:05,053, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:11:05,185, data_transformation, INFO, Transforming test data ]
[2024-12-24 20:11:07,650, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-24 20:11:07,651, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 20:11:12,255, data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-24 20:12:35,693, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:12:35,695, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:12:36,660, text_data_transformation, INFO, Error in initiating the data transformation pipeline "[('rating_review',)] not found in axis" ]
[2024-12-24 20:13:46,008, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:13:46,010, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:13:46,962, text_data_transformation, INFO, Error in initiating the data transformation pipeline "[('rating_review',)] not found in axis" ]
[2024-12-24 20:14:32,917, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:14:32,919, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:14:34,180, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 20:14:34,185, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 20:14:34,185, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 20:14:34,186, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 20:14:35,516, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T20:14:35.516399', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 20:14:35,518, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 20:14:35,519, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 20:14:35,526, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:14:35,530, text_data_transformation, INFO, Error in preprocessing the text: 'int' object has no attribute 'lower' ]
[2024-12-24 20:14:35,531, text_data_transformation, INFO, Error in initiating the data transformation pipeline 'int' object has no attribute 'lower' ]
[2024-12-24 20:17:24,879, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:17:24,881, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:17:26,204, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 20:17:26,208, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 20:17:26,210, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 20:17:26,211, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 20:17:27,513, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T20:17:27.513625', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 20:17:27,514, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 20:17:27,514, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 20:17:27,519, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:17:27,650, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:17:34,342, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:17:34,345, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:18:02,959, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:18:02,962, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:19:53,723, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:19:53,724, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 20:19:53,733, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:19:53,779, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:19:56,341, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:19:56,344, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:20:06,022, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:20:06,028, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:20:53,268, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:20:53,269, text_data_transformation, INFO, Error in initiating the data transformation pipeline (slice(None, None, None), 0) ]
[2024-12-24 20:23:22,116, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:23:22,118, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:23:23,168, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 20:23:23,172, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 20:23:23,173, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 20:23:23,174, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 20:23:24,338, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T20:23:24.338209', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 20:23:24,340, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 20:23:24,340, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 20:23:24,346, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:23:24,457, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:23:30,299, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:23:30,302, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:23:56,465, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:23:56,467, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:25:46,034, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:25:46,035, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 20:25:46,044, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:25:46,092, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:25:48,634, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:25:48,637, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:25:57,539, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:25:57,545, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:26:43,512, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:26:43,514, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 20:26:44,664, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 20:26:44,665, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 20:26:44,666, text_data_transformation, INFO, Error in initiating the data transformation pipeline setting an array element with a sequence. ]
[2024-12-24 20:31:41,849, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:31:41,851, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:31:42,860, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 20:31:42,864, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 20:31:42,866, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 20:31:42,867, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 20:31:43,976, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T20:31:43.976689', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 20:31:43,977, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 20:31:43,978, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 20:31:43,984, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:31:44,097, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:31:50,494, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:31:50,497, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:32:16,412, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:32:16,415, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:34:06,989, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:34:06,991, text_data_transformation, INFO, Error in initiating the data transformation pipeline 0 ]
[2024-12-24 20:37:18,793, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:37:18,795, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:37:19,900, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 20:37:19,904, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 20:37:19,906, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 20:37:19,908, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 20:37:21,097, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T20:37:21.097913', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 20:37:21,098, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 20:37:21,099, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 20:37:21,104, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:37:21,219, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:37:27,329, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:37:27,332, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:37:53,752, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:37:53,755, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:39:42,551, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:39:42,552, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 20:39:42,563, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:39:42,616, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:39:45,061, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:39:45,065, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:39:54,441, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:39:54,447, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:40:39,825, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:40:39,826, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 20:40:40,833, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 20:40:40,835, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 20:40:40,836, text_data_transformation, INFO, Error in initiating the data transformation pipeline name 'input_train_arr' is not defined ]
[2024-12-24 20:42:54,643, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 20:42:54,645, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 20:42:55,819, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 20:42:55,823, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 20:42:55,824, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 20:42:55,824, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 20:42:57,186, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T20:42:57.186951', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 20:42:57,188, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 20:42:57,189, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 20:42:57,195, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:42:57,314, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:43:03,320, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:43:03,323, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:43:29,357, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:43:29,360, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:45:19,994, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:45:19,995, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 20:45:20,003, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 20:45:20,051, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 20:45:22,666, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 20:45:22,670, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 20:45:34,475, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 20:45:34,481, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 20:46:19,865, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 20:46:19,867, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 20:46:21,080, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 20:46:21,081, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 20:46:58,247, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-24 20:50:10,506, data_clustering, INFO, Initialized clustering model ]
[2024-12-24 20:50:10,508, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-24 20:50:10,510, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-24 20:50:10,512, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-24 20:50:10,513, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-24 20:50:10,515, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-24 20:50:10,518, data_clustering, INFO, Error fitting the data with UMP : setting an array element with a sequence. ]
[2024-12-24 20:50:10,519, data_clustering, INFO,  Error in initiating the clustering pipeline : setting an array element with a sequence. ]
[2024-12-24 21:49:06,928, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 21:49:06,930, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 21:49:08,212, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 21:49:08,223, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 21:49:08,224, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 21:49:08,225, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 21:49:09,729, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T21:49:09.729932', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 21:49:09,730, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 21:49:09,730, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 21:49:09,737, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 21:49:09,863, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 21:49:16,078, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 21:49:16,081, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 21:49:42,662, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 21:49:42,665, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 21:51:33,993, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 21:51:33,994, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 21:51:34,002, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 21:51:34,056, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 21:51:36,602, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 21:51:36,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 21:51:45,392, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 21:51:45,398, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 21:52:31,542, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 21:52:31,544, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 21:52:32,712, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 21:52:32,714, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 21:53:09,755, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-24 21:53:09,816, data_clustering, INFO, Initialized clustering model ]
[2024-12-24 21:53:09,817, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-24 21:53:09,819, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-24 21:53:09,820, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-24 21:53:09,822, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-24 21:53:09,850, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-24 21:53:09,851, data_clustering, INFO, Error fitting the data with UMP : setting an array element with a sequence. ]
[2024-12-24 21:53:09,852, data_clustering, INFO,  Error in initiating the clustering pipeline : setting an array element with a sequence. ]
[2024-12-24 23:41:20,363, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:41:25,158, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:41:25,159, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 23:41:26,375, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 23:41:26,379, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 23:41:26,379, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 23:41:26,381, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 23:41:27,514, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T23:41:27.514327', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 23:41:27,515, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 23:41:27,516, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 23:41:27,518, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:41:27,637, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:41:33,050, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:41:33,054, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:41:56,781, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:41:56,784, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:41:56,942, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:41:56,943, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:41:56,953, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:41:57,005, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:41:59,337, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:41:59,340, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:42:07,984, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:42:07,990, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:42:08,065, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:42:08,067, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:42:09,173, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 23:42:09,174, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 23:42:09,176, text_data_transformation, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'tolist' ]
[2024-12-24 23:43:45,648, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:43:45,650, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 23:43:46,790, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 23:43:46,793, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 23:43:46,794, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 23:43:46,795, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 23:43:48,053, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T23:43:48.053312', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 23:43:48,055, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 23:43:48,055, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 23:43:48,060, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:43:48,178, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:43:55,879, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:43:55,882, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:44:19,787, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:44:19,790, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:44:19,931, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:44:19,932, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:44:19,940, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:44:19,990, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:44:22,544, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:44:22,548, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:44:31,041, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:44:31,047, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:44:31,119, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:44:31,121, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:44:32,156, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 23:44:32,157, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 23:44:32,158, text_data_transformation, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'tolist' ]
[2024-12-24 23:45:11,989, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:45:11,996, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 23:45:13,885, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 23:45:13,888, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 23:45:13,889, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 23:45:13,890, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 23:45:15,183, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T23:45:15.183108', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 23:45:15,184, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 23:45:15,185, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 23:45:15,189, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:45:15,302, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:45:21,211, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:45:21,214, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:45:45,375, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:45:45,378, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:45:45,517, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:45:45,518, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:45:45,527, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:45:45,573, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:45:48,052, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:45:48,055, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:45:56,887, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:45:56,893, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:45:56,952, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:45:56,954, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:45:58,044, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 23:45:58,045, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 23:45:58,049, text_data_transformation, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'tolist' ]
[2024-12-24 23:46:52,605, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:46:52,607, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 23:46:53,726, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 23:46:53,729, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 23:46:53,731, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 23:46:53,732, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 23:46:54,835, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T23:46:54.835454', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 23:46:54,836, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 23:46:54,837, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 23:46:54,842, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:46:54,954, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:47:00,625, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:47:00,629, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:47:25,854, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:47:25,858, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:47:25,987, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:47:25,989, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:47:25,998, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:47:26,044, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:47:28,398, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:47:28,401, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:47:36,593, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:47:36,601, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:47:36,675, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:47:36,676, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:47:37,790, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 23:47:37,791, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 23:47:37,795, text_data_transformation, INFO, Error in initiating the data transformation pipeline 'DataFrame' object has no attribute 'to_list' ]
[2024-12-24 23:48:37,919, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:48:37,921, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 23:48:39,287, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 23:48:39,291, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 23:48:39,292, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 23:48:39,293, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 23:48:40,485, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T23:48:40.485894', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 23:48:40,487, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 23:48:40,487, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 23:48:40,491, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:48:40,611, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:48:46,347, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:48:46,350, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:49:10,398, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:49:10,401, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:49:10,539, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:49:10,540, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:49:10,550, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:49:10,599, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:49:12,967, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:49:12,971, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:49:24,697, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:49:24,703, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-24 23:49:24,769, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-24 23:49:24,771, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:49:25,774, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 23:49:25,776, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 23:49:25,788, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-24 23:51:55,521, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:51:55,523, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 23:51:56,680, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 23:51:56,683, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 23:51:56,684, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 23:51:56,685, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 23:51:56,687, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:51:56,688, text_data_transformation, INFO, Error in preprocessing the text: 'numpy.ndarray' object has no attribute 'map' ]
[2024-12-24 23:51:56,691, text_data_transformation, INFO, Error in initiating the data transformation pipeline 'numpy.ndarray' object has no attribute 'map' ]
[2024-12-24 23:53:08,333, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:53:08,335, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 23:53:09,357, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 23:53:09,360, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 23:53:09,361, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 23:53:09,363, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 23:53:09,366, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:53:09,518, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:53:15,571, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:53:15,574, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:53:43,535, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:53:43,536, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:53:43,545, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:53:43,584, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:53:45,949, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:53:45,950, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:53:54,588, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:53:54,590, text_data_transformation, INFO, Transforming test data ]
[2024-12-24 23:53:54,651, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-24 23:53:54,652, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-24 23:53:55,631, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-24 23:58:37,151, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-24 23:58:37,154, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-24 23:58:38,388, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-24 23:58:38,392, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-24 23:58:38,393, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-24 23:58:38,394, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-24 23:58:39,591, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-24T23:58:39.591518', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-24 23:58:39,592, text_data_transformation, INFO, Text pipeline created ]
[2024-12-24 23:58:39,594, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-24 23:58:39,598, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-24 23:58:39,735, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-24 23:58:45,333, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-24 23:58:45,336, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-24 23:59:09,944, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-24 23:59:09,947, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 00:00:48,142, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 00:00:48,143, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 00:00:48,151, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 00:00:48,197, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 00:00:50,633, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 00:00:50,639, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 00:00:59,955, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 00:00:59,955, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 00:01:46,335, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 00:01:46,337, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 00:01:47,604, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-25 00:01:47,605, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-25 00:02:22,011, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-25 00:03:05,441, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 00:03:05,443, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 00:03:05,444, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 00:03:05,446, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 00:03:05,447, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 00:03:05,469, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 00:03:05,470, data_clustering, INFO, Error fitting the data with UMP : setting an array element with a sequence. ]
[2024-12-25 00:03:05,472, data_clustering, INFO,  Error in initiating the clustering pipeline : setting an array element with a sequence. ]
[2024-12-25 00:05:25,697, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 00:05:25,699, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 00:05:25,700, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 00:05:25,701, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 00:05:25,703, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 00:05:25,704, data_clustering, INFO,  Error in initiating the clustering pipeline : 'list' object has no attribute 'info' ]
[2024-12-25 00:06:17,772, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-25 00:06:17,774, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-25 00:06:18,970, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 00:06:18,973, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 00:06:18,974, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-25 00:06:18,975, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-25 00:06:20,266, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-25T00:06:20.266322', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-25 00:06:20,267, text_data_transformation, INFO, Text pipeline created ]
[2024-12-25 00:06:20,268, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-25 00:06:20,272, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 00:06:20,384, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 00:06:28,201, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 00:06:28,204, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 00:06:52,695, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 00:06:52,698, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 00:08:34,184, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 00:08:34,184, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 00:08:34,202, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 00:08:34,379, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 00:08:38,458, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 00:08:38,467, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 00:08:47,040, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 00:08:47,043, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 00:09:29,191, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 00:09:29,192, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 00:09:30,443, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-25 00:09:30,445, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-25 00:10:08,027, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-25 00:10:08,083, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 00:10:08,085, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 00:10:08,087, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 00:10:08,088, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 00:10:08,089, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 00:10:08,091, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 00:10:08,092, data_clustering, INFO, Error fitting the data with UMP : 'list' object has no attribute 'values' ]
[2024-12-25 00:10:08,093, data_clustering, INFO,  Error in initiating the clustering pipeline : 'list' object has no attribute 'values' ]
[2024-12-25 00:12:07,294, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-25 00:12:07,295, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-25 00:12:08,938, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 00:12:08,951, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 00:12:08,955, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-25 00:12:08,960, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-25 00:12:10,966, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-25T00:12:10.965468', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-25 00:12:10,968, text_data_transformation, INFO, Text pipeline created ]
[2024-12-25 00:12:10,969, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-25 00:12:10,975, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 00:12:11,103, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 00:12:17,175, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 00:12:17,178, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 00:12:45,556, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 00:12:45,559, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 00:14:38,068, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 00:14:38,069, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 00:14:38,078, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 00:14:38,135, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 00:14:40,710, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 00:14:40,716, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 00:14:50,033, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 00:14:50,035, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 00:15:37,508, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 00:15:37,510, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 00:15:38,795, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-25 00:15:38,796, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-25 00:16:16,328, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-25 00:16:16,419, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 00:16:16,421, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 00:16:16,422, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 00:16:16,424, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 00:16:16,426, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 00:16:16,428, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 00:22:11,396, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 00:22:15,153, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 00:22:19,109, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 00:22:19,111, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 00:22:54,028, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 00:22:54,029, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 00:23:50,770, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 00:23:50,771, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 00:23:51,727, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 00:23:51,728, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 00:23:53,871, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 00:23:54,688, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 00:23:54,689, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 00:25:37,593, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 00:25:37,596, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 00:25:51,396, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 00:25:51,496, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 00:25:57,729, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 00:25:57,732, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 00:26:25,187, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 00:26:25,190, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 00:26:27,036, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 00:26:27,180, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 00:26:34,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 00:26:34,786, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 00:26:58,066, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 00:34:20,521, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 00:34:20,523, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 00:34:20,524, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 00:34:20,526, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 00:34:20,527, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 00:34:20,529, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 00:34:20,533, data_clustering, INFO, Error fitting the data with UMP : could not convert string to float: '[ 0.21323821 -0.07814479  0.20939179  0.20641913  0.01826742 -0.24649127\n  0.2517338  -0.15993796 -0.26636913  0.03167932 -0.1314872  -0.20031883\n  0.0609978  -0.13113654 -0.27672032 -0.00511233 -0.01731855  0.18865573\n -0.11152753  0.154916    0.12296078 -0.03320168  0.07468986  0.14557195\n -0.02399172]' ]
[2024-12-25 00:34:20,534, data_clustering, INFO,  Error in initiating the clustering pipeline : could not convert string to float: '[ 0.21323821 -0.07814479  0.20939179  0.20641913  0.01826742 -0.24649127\n  0.2517338  -0.15993796 -0.26636913  0.03167932 -0.1314872  -0.20031883\n  0.0609978  -0.13113654 -0.27672032 -0.00511233 -0.01731855  0.18865573\n -0.11152753  0.154916    0.12296078 -0.03320168  0.07468986  0.14557195\n -0.02399172]' ]
[2024-12-25 10:03:30,071, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-25 10:03:30,073, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-25 10:03:31,338, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 10:03:31,342, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 10:03:31,343, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-25 10:03:31,344, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-25 10:03:32,804, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-25T10:03:32.803904', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-25 10:03:32,805, text_data_transformation, INFO, Text pipeline created ]
[2024-12-25 10:03:32,806, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-25 10:03:32,810, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 10:03:32,955, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 10:03:38,638, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 10:03:38,642, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 10:04:03,538, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 10:04:03,541, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 10:05:42,933, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 10:05:42,934, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 10:05:42,943, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 10:05:42,996, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 10:05:45,593, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 10:05:45,599, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 10:05:54,357, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 10:05:54,359, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 10:06:39,202, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 10:06:39,204, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 10:06:40,640, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-25 10:06:40,642, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-25 10:07:16,117, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-25 10:19:27,809, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-25 10:19:27,811, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-25 10:19:28,937, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 10:19:28,948, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 10:19:28,949, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-25 10:19:28,950, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-25 10:19:30,047, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-25T10:19:30.047376', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-25 10:19:30,048, text_data_transformation, INFO, Text pipeline created ]
[2024-12-25 10:19:30,049, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-25 10:19:30,054, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 10:19:30,164, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 10:19:35,881, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 10:19:35,884, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 10:20:02,191, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 10:20:02,194, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 10:21:53,349, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 10:21:53,351, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 10:21:53,360, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 10:21:53,402, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 10:21:55,642, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 10:21:55,648, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 10:22:05,625, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 10:22:05,626, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 10:22:57,424, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 10:22:57,426, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 10:23:00,092, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-25 10:23:00,095, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-25 10:23:39,627, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-25 12:01:34,007, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-25 12:01:34,009, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-25 12:01:35,136, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 12:01:35,139, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 12:01:35,141, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-25 12:01:35,142, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-25 12:01:36,252, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-25T12:01:36.252912', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-25 12:01:36,253, text_data_transformation, INFO, Text pipeline created ]
[2024-12-25 12:01:36,253, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-25 12:01:36,258, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 12:01:36,372, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 12:01:42,209, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 12:01:42,213, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 12:02:05,226, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 12:02:05,229, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 12:03:43,661, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 12:03:43,662, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 12:03:43,671, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 12:03:43,721, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 12:03:46,358, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 12:03:46,365, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 12:03:55,812, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 12:03:55,814, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 12:04:38,448, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 12:04:38,449, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 12:04:39,797, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-25 12:04:39,798, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-25 12:04:44,857, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-25 12:10:00,454, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 12:10:00,456, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 12:10:00,458, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 12:10:00,460, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 12:10:00,461, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 12:10:00,462, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 12:10:00,465, data_clustering, INFO, Error fitting the data with UMP : could not convert string to float: '[ 0.21323821 -0.07814479  0.20939179  0.20641913  0.01826742 -0.24649127\n  0.2517338  -0.15993796 -0.26636913  0.03167932 -0.1314872  -0.20031883\n  0.0609978  -0.13113654 -0.27672032 -0.00511233 -0.01731855  0.18865573\n -0.11152753  0.154916    0.12296078 -0.03320168  0.07468986  0.14557195\n -0.02399172]' ]
[2024-12-25 12:10:00,467, data_clustering, INFO,  Error in initiating the clustering pipeline : could not convert string to float: '[ 0.21323821 -0.07814479  0.20939179  0.20641913  0.01826742 -0.24649127\n  0.2517338  -0.15993796 -0.26636913  0.03167932 -0.1314872  -0.20031883\n  0.0609978  -0.13113654 -0.27672032 -0.00511233 -0.01731855  0.18865573\n -0.11152753  0.154916    0.12296078 -0.03320168  0.07468986  0.14557195\n -0.02399172]' ]
[2024-12-25 12:10:59,804, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 12:10:59,806, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 12:10:59,807, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 12:10:59,809, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 12:10:59,811, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 12:10:59,812, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 12:10:59,816, data_clustering, INFO, Error fitting the data with UMP : could not convert string to float: '[ 0.21323821 -0.07814479  0.20939179  0.20641913  0.01826742 -0.24649127\n  0.2517338  -0.15993796 -0.26636913  0.03167932 -0.1314872  -0.20031883\n  0.0609978  -0.13113654 -0.27672032 -0.00511233 -0.01731855  0.18865573\n -0.11152753  0.154916    0.12296078 -0.03320168  0.07468986  0.14557195\n -0.02399172]' ]
[2024-12-25 12:10:59,817, data_clustering, INFO,  Error in initiating the clustering pipeline : could not convert string to float: '[ 0.21323821 -0.07814479  0.20939179  0.20641913  0.01826742 -0.24649127\n  0.2517338  -0.15993796 -0.26636913  0.03167932 -0.1314872  -0.20031883\n  0.0609978  -0.13113654 -0.27672032 -0.00511233 -0.01731855  0.18865573\n -0.11152753  0.154916    0.12296078 -0.03320168  0.07468986  0.14557195\n -0.02399172]' ]
[2024-12-25 12:12:31,485, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 12:12:31,487, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 12:12:31,489, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 12:12:31,490, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 12:12:31,490, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 12:12:31,492, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 12:15:01,814, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 12:15:02,610, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 12:15:03,422, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 12:15:03,423, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 12:15:23,918, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 12:15:23,920, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 12:15:23,921, data_clustering, ERROR, Error transforming the data using the UMAP model: could not convert string to float: '[ 0.21323821 -0.07814479  0.20939179  0.20641913  0.01826742 -0.24649127\n  0.2517338  -0.15993796 -0.26636913  0.03167932 -0.1314872  -0.20031883\n  0.0609978  -0.13113654 -0.27672032 -0.00511233 -0.01731855  0.18865573\n -0.11152753  0.154916    0.12296078 -0.03320168  0.07468986  0.14557195\n -0.02399172]' ]
[2024-12-25 12:15:23,922, data_clustering, INFO,  Error in initiating the clustering pipeline : could not convert string to float: '[ 0.21323821 -0.07814479  0.20939179  0.20641913  0.01826742 -0.24649127\n  0.2517338  -0.15993796 -0.26636913  0.03167932 -0.1314872  -0.20031883\n  0.0609978  -0.13113654 -0.27672032 -0.00511233 -0.01731855  0.18865573\n -0.11152753  0.154916    0.12296078 -0.03320168  0.07468986  0.14557195\n -0.02399172]' ]
[2024-12-25 12:20:29,152, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-25 12:20:29,154, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-25 12:20:30,179, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 12:20:30,182, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 12:20:30,182, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-25 12:20:30,184, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-25 12:20:31,372, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-25T12:20:31.372591', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-25 12:20:31,373, text_data_transformation, INFO, Text pipeline created ]
[2024-12-25 12:20:31,374, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-25 12:20:31,378, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 12:20:31,513, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 12:20:39,818, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 12:20:39,821, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 12:21:05,029, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 12:21:05,033, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 12:22:53,079, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 12:22:53,080, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 12:22:53,088, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 12:22:53,140, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 12:22:55,553, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 12:22:55,560, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 12:23:05,056, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 12:23:05,058, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-25 12:23:53,072, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-25 12:23:53,072, text_data_transformation, INFO, Transforming test data ]
[2024-12-25 12:23:54,460, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-25 12:23:54,462, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-25 12:24:00,400, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-25 12:24:00,496, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 12:24:00,498, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 12:24:00,500, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 12:24:00,501, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 12:24:00,503, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 12:24:00,506, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 12:24:49,696, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 12:24:49,697, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 12:24:49,699, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 12:24:49,701, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 12:24:49,702, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 12:24:49,703, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 12:30:33,226, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 12:30:35,787, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 12:30:38,682, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 12:30:38,683, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 12:31:11,550, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 12:31:11,552, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 12:32:08,592, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 12:32:08,593, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 12:32:09,251, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 12:32:09,252, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 12:32:11,282, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 12:32:12,024, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 12:32:12,025, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 12:32:23,196, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 12:32:23,199, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 12:35:20,612, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 12:35:20,726, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 12:35:27,674, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 12:35:27,677, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 12:35:51,383, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 12:42:35,417, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 12:42:35,419, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 12:42:35,420, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 12:42:35,422, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 12:42:35,423, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 12:42:35,426, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 12:42:35,428, data_clustering, INFO, Error fitting the data with UMP : could not convert string to float: 'artifacts\\transformed_train_data.csv' ]
[2024-12-25 12:42:35,430, data_clustering, INFO,  Error in initiating the clustering pipeline : could not convert string to float: 'artifacts\\transformed_train_data.csv' ]
[2024-12-25 12:43:19,981, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 12:43:19,982, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 12:43:19,984, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 12:43:19,985, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 12:43:19,986, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 12:43:19,988, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 12:43:55,658, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 12:43:55,660, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 12:43:55,661, data_clustering, INFO, The n_neighbours =100, min_dist =0.5, metric=cosine ]
[2024-12-25 12:43:55,663, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 12:43:55,664, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 12:43:55,666, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 12:46:14,773, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 12:46:18,758, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 12:46:22,563, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 12:46:22,564, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 12:46:41,951, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 12:46:41,953, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 12:47:00,599, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 12:47:00,601, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 12:47:01,499, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 12:47:01,501, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 12:47:03,525, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 12:47:04,377, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 12:47:04,378, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 12:47:13,024, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 12:47:13,029, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 13:15:47,179, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 13:15:47,181, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 13:15:47,181, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 13:15:47,183, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 13:15:47,185, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 13:18:10,069, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 13:18:10,664, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 13:18:11,204, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 13:18:11,205, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 13:18:30,766, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 13:18:30,767, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 13:18:48,618, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 13:18:48,619, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 13:18:48,840, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 13:18:48,841, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 13:18:52,530, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 13:18:53,646, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 13:18:53,647, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 13:19:43,504, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 13:19:43,509, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 13:19:51,504, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 13:19:51,507, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 13:20:09,463, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 13:20:09,571, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 13:20:24,725, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 13:20:24,728, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 13:21:07,717, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 13:21:07,823, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 13:21:14,795, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 13:21:14,800, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 13:21:43,167, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 13:37:15,932, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 13:37:15,937, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 13:37:34,992, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 13:37:34,994, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 13:38:54,697, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 13:38:54,700, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 13:38:59,968, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 13:38:59,991, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 13:39:01,021, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 13:39:01,023, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 13:39:05,204, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 13:39:32,609, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 13:39:32,612, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 13:39:41,243, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 13:39:41,265, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 13:39:42,234, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 13:39:42,236, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 13:39:46,394, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 13:42:14,879, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 13:42:14,880, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 13:42:14,881, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 13:42:14,883, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 13:42:14,886, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 13:44:12,521, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 13:44:13,146, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 13:44:13,667, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 13:44:13,668, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 13:44:21,381, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 13:44:21,383, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 13:44:38,966, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 13:44:38,967, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 13:44:39,152, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 13:44:39,154, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 13:44:41,338, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 13:44:42,158, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 13:44:42,159, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 13:45:36,443, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 13:45:36,445, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:11:46,105, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 14:11:46,107, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 14:11:46,108, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 14:11:46,110, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 14:11:46,111, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 14:14:09,259, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 14:14:09,985, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 14:14:10,797, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 14:14:10,798, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 14:14:30,330, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 14:14:30,332, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 14:14:47,984, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 14:14:47,986, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 14:14:48,186, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 14:14:48,187, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 14:14:50,166, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 14:14:51,007, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 14:14:51,008, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 14:14:52,469, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:14:52,473, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:16:45,914, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 14:16:45,916, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 14:16:45,917, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 14:16:45,918, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 14:16:45,920, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 14:19:04,121, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 14:19:04,971, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 14:19:05,826, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 14:19:05,827, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 14:19:25,093, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 14:19:25,094, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 14:19:43,164, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 14:19:43,166, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 14:19:43,347, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 14:19:43,349, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 14:19:45,353, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 14:19:46,099, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 14:19:46,101, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 14:19:50,927, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:19:50,932, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:21:25,644, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:21:25,647, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:21:32,436, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 14:21:32,510, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 14:21:36,157, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 14:21:36,159, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 14:21:53,189, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 14:24:56,146, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:24:56,149, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:25:01,239, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 14:25:01,304, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 14:25:05,191, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 14:25:05,193, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 14:25:18,267, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 14:28:58,886, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:28:58,888, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:29:06,078, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 14:29:06,147, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 14:29:10,031, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 14:29:10,033, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 14:29:22,371, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 14:33:02,789, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:33:02,792, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:33:06,679, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 14:33:06,755, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 14:33:10,613, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 14:33:10,616, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 14:33:23,873, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 14:36:14,663, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:36:14,666, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:36:17,842, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 14:36:17,908, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 14:36:21,674, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 14:36:21,676, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 14:36:34,715, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 14:39:08,507, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 14:39:08,541, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 14:39:10,389, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 14:39:10,393, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 14:39:16,620, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 14:42:06,967, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:42:06,969, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:42:10,893, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 14:42:10,923, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 14:42:12,715, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 14:42:12,716, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 14:42:18,813, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 14:48:43,203, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 14:48:43,205, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 14:48:43,208, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 14:48:43,210, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 14:48:43,212, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 14:51:17,763, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 14:51:18,610, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 14:51:19,810, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 14:51:19,811, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 14:51:42,051, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 14:51:42,054, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 14:52:01,639, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 14:52:01,640, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 14:52:01,945, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 14:52:01,947, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 14:52:04,054, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 14:52:04,908, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 14:52:04,909, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 14:52:06,493, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:52:06,498, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 14:53:04,930, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 14:53:04,932, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 14:53:04,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 14:53:04,950, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 14:53:09,781, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 14:55:24,763, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 14:55:24,765, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 14:55:24,766, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 14:55:24,768, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 14:55:24,770, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 14:57:58,264, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 14:57:59,198, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 14:58:00,175, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 14:58:00,176, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 14:58:21,024, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 14:58:21,025, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 14:58:40,465, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 14:58:40,467, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 14:58:40,736, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 14:58:40,737, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 14:58:42,787, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 14:58:43,637, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 14:58:43,639, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 14:58:45,307, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 14:58:45,313, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 15:49:33,545, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 15:49:33,547, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 15:49:33,548, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 15:49:33,549, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 15:49:33,553, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 15:51:42,922, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 15:51:43,964, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 15:51:45,160, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 15:51:45,161, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 15:52:02,567, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 15:52:02,569, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 15:52:19,124, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 15:52:19,125, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 15:52:19,435, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 15:52:19,436, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 15:52:21,160, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 15:52:21,880, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 15:52:21,882, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 15:52:23,306, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 15:52:23,311, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 15:53:08,988, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 15:53:08,990, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 15:53:08,992, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 15:53:08,993, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 15:53:08,995, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 15:55:17,024, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 15:55:18,074, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 15:55:19,137, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 15:55:19,138, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 15:55:36,792, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 15:55:36,794, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 15:55:53,205, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 15:55:53,207, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 15:55:53,418, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 15:55:53,419, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 15:55:55,152, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 15:55:55,864, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 15:55:55,865, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 15:55:57,253, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 15:55:57,257, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 16:04:06,615, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 16:04:06,617, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 16:04:06,618, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 16:04:06,618, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 16:04:06,620, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 16:06:18,590, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 16:06:19,457, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 16:06:20,551, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 16:06:20,553, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 16:06:39,702, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 16:06:39,703, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 16:06:56,043, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 16:06:56,043, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 16:06:56,309, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 16:06:56,310, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 16:06:58,046, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 16:06:58,748, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 16:06:58,750, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 16:07:00,131, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 16:07:00,136, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 16:10:03,976, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 16:10:03,978, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 16:10:03,980, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 16:10:03,981, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 16:10:03,983, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 16:12:13,682, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 16:12:14,298, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 16:12:14,942, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 16:12:14,943, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 16:12:32,168, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 16:12:32,170, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 16:12:48,120, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 16:12:48,121, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 16:12:48,340, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 16:12:48,341, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 16:12:50,159, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 16:12:50,900, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 16:12:50,901, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 17:34:07,908, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 17:34:07,912, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 17:34:07,913, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 17:34:07,915, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 17:34:07,916, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 17:36:50,548, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 17:36:51,292, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 17:36:52,046, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 17:36:52,047, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 17:37:13,613, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 17:37:13,613, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 17:37:33,651, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 17:37:33,652, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 17:37:33,874, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 17:37:33,876, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 17:37:36,181, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 17:37:37,187, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 17:37:37,189, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 17:37:38,776, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 17:37:38,793, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 17:40:12,581, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 17:40:12,582, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 17:40:12,583, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 17:40:12,584, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 17:40:12,587, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 17:42:49,237, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 17:47:13,206, data_clustering, INFO, Error in fitting the data to the DBSCAN cluster model: Unable to allocate 767. KiB for an array with shape (98129, 1) and data type int64 ]
[2024-12-25 17:47:13,349, data_clustering, INFO,  Error in initiating the clustering pipeline : Unable to allocate 767. KiB for an array with shape (98129, 1) and data type int64 ]
[2024-12-25 17:54:06,622, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 17:54:06,624, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 17:54:06,626, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 17:54:06,627, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 17:54:06,628, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 17:56:38,167, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 18:00:57,872, data_clustering, INFO, Error in fitting the data to the DBSCAN cluster model: Unable to allocate 64.0 MiB for an array with shape (1299, 12913) and data type float32 ]
[2024-12-25 18:00:57,947, data_clustering, INFO,  Error in initiating the clustering pipeline : Unable to allocate 64.0 MiB for an array with shape (1299, 12913) and data type float32 ]
[2024-12-25 18:01:59,740, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 18:01:59,742, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 18:01:59,744, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 18:01:59,745, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 18:01:59,747, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 18:04:25,779, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 18:08:09,330, data_clustering, INFO, Error in fitting the data to the DBSCAN cluster model: Unable to allocate 64.0 MiB for an array with shape (1299, 12913) and data type float32 ]
[2024-12-25 18:08:09,423, data_clustering, INFO,  Error in initiating the clustering pipeline : Unable to allocate 64.0 MiB for an array with shape (1299, 12913) and data type float32 ]
[2024-12-25 18:09:27,204, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 18:09:27,205, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 18:09:27,207, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 18:09:27,209, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 18:09:27,210, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 18:11:54,605, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 18:11:55,321, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:11:56,115, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:11:56,116, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 18:12:17,211, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 18:12:17,213, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 18:12:40,646, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 18:12:40,648, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:12:40,880, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:12:40,881, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 18:12:43,123, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:12:44,197, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:12:44,198, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 18:13:48,529, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 18:13:48,531, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 18:13:48,532, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 18:13:48,534, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 18:13:48,535, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 18:16:13,186, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 18:16:15,358, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:16:17,034, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:16:17,036, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 18:16:36,860, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 18:16:36,862, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 18:16:54,813, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 18:16:54,814, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:16:55,074, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:16:55,075, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 18:16:57,039, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:16:57,831, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:16:57,832, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 18:17:57,640, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 18:17:57,642, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 18:17:57,644, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 18:17:57,645, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 18:17:57,648, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 18:20:20,138, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 18:20:21,169, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:20:22,130, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:20:22,131, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 18:20:44,100, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 18:20:44,102, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 18:21:02,620, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 18:21:02,621, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:21:02,828, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:21:02,830, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 18:21:04,802, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:21:05,661, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:21:05,663, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 18:21:49,683, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 18:21:49,685, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 18:21:49,687, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 18:21:49,689, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 18:21:49,690, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 18:24:13,203, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 18:24:13,901, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:24:14,771, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:24:14,772, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 18:24:34,740, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 18:24:34,742, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 18:24:52,995, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 18:24:52,997, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:24:53,221, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:24:53,222, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 18:24:55,089, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:24:55,939, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:24:55,940, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 18:25:46,839, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 18:25:46,841, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 18:25:46,843, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 18:25:46,844, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 18:25:46,846, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 18:28:12,435, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 18:28:13,148, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:28:13,956, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:28:13,958, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 18:28:34,740, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 18:28:34,742, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 18:28:53,072, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 18:28:53,073, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:28:53,321, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:28:53,322, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 18:28:55,151, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:28:55,866, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:28:55,867, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 18:33:22,284, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 18:33:22,285, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 18:33:22,288, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 18:33:22,290, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 18:33:22,291, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 18:35:40,611, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 18:35:41,549, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:35:42,560, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:35:42,561, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 18:36:02,168, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 18:36:02,170, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 18:36:21,168, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 18:36:21,169, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 18:36:21,353, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 18:36:21,354, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 18:36:23,345, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:36:24,199, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 18:36:24,200, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 18:36:25,711, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 18:36:25,724, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 18:59:05,407, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 18:59:05,408, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 18:59:05,410, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 18:59:05,412, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 18:59:05,414, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 19:01:35,454, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 19:01:36,296, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 19:01:37,263, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 19:01:37,265, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 19:01:56,532, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 19:01:56,533, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 19:02:15,796, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 19:02:15,797, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 19:02:16,034, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 19:02:16,036, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 19:02:18,075, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 19:02:18,778, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 19:02:18,779, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 19:04:15,813, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 19:04:15,814, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 19:04:15,816, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 19:04:15,817, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 19:04:15,819, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 19:06:42,769, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 19:06:43,527, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 19:06:44,432, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 19:06:44,433, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 19:07:06,936, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 19:07:06,938, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 19:07:24,721, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 19:07:24,723, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 19:07:24,953, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 19:07:24,955, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 19:07:26,718, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 19:07:27,457, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 19:07:27,458, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 19:08:21,687, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 19:08:21,689, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 19:08:21,692, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 19:08:21,694, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 19:08:21,696, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 19:10:48,414, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 19:10:49,080, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 19:10:49,773, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 19:10:49,774, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 19:11:09,664, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 19:11:09,665, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 19:11:27,593, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 19:11:27,594, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 19:11:27,784, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 19:11:27,786, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 19:11:29,778, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 19:11:30,609, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 19:11:30,611, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 19:11:32,044, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 19:11:32,051, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 19:14:42,952, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 19:14:42,954, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 19:14:42,955, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 19:14:42,956, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 19:14:42,958, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 19:16:59,191, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 19:16:59,970, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 19:17:00,844, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 19:17:00,845, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 19:17:20,451, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 19:17:20,453, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 19:17:38,287, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 19:17:38,289, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 19:17:38,520, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 19:17:38,522, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 19:17:40,315, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 19:17:41,171, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 19:17:41,172, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:13:28,668, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:13:28,669, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:13:28,671, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:13:28,672, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:13:28,674, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:15:46,573, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:15:47,401, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:15:48,171, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:15:48,172, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:16:06,744, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:16:06,745, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:16:24,583, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:16:24,585, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:16:24,867, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:16:24,869, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:16:26,771, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:16:27,484, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:16:27,487, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:18:24,363, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:18:24,365, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:18:24,367, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:18:24,368, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:18:24,371, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:20:48,297, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:20:49,129, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:20:50,097, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:20:50,098, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:21:09,000, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:21:09,002, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:21:27,426, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:21:27,427, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:21:27,690, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:21:27,691, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:21:29,642, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:21:30,498, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:21:30,499, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:22:13,971, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:22:13,973, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:22:13,975, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:22:13,976, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:22:13,979, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:24:33,634, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:24:34,433, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:24:35,328, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:24:35,330, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:24:54,343, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:24:54,344, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:25:11,779, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:25:11,780, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:25:12,039, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:25:12,040, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:25:14,042, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:25:14,803, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:25:14,804, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:26:04,775, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:26:04,777, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:26:04,779, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:26:04,780, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:26:04,782, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:28:27,566, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:28:28,416, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:28:29,218, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:28:29,219, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:28:48,969, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:28:48,970, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:29:06,179, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:29:06,181, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:29:06,428, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:29:06,429, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:29:08,216, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:29:08,955, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:29:08,957, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:30:15,291, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:30:15,293, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:30:15,294, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:30:15,295, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:30:15,297, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:32:38,850, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:32:39,608, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:32:40,312, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:32:40,313, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:32:59,933, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:32:59,934, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:33:19,106, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:33:19,108, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:33:19,332, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:33:19,334, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:33:21,284, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:33:22,124, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:33:22,125, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:33:43,896, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 20:33:43,901, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 20:34:45,703, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:34:45,705, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:34:45,707, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:34:45,708, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:34:45,710, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:37:34,086, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:37:35,488, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:37:36,428, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:37:36,430, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:37:59,769, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:37:59,771, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:38:21,837, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:38:21,839, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:38:22,069, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:38:22,069, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:38:24,121, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:38:24,988, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:38:24,990, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:41:44,242, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:41:44,244, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:41:44,245, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:41:44,246, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:41:44,249, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:44:06,916, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:44:07,392, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:44:07,859, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:44:07,860, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:44:26,635, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:44:26,637, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:44:44,655, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:44:44,656, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:44:44,860, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:44:44,864, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:44:49,559, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:44:50,928, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:44:50,929, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:45:44,911, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:45:44,912, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:45:44,914, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:45:44,915, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:45:44,916, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:48:04,890, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:48:05,401, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:48:05,964, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:48:05,965, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:48:24,775, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:48:24,776, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:48:44,521, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:48:44,523, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:48:44,702, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:48:44,704, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:48:46,552, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:48:47,392, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:48:47,393, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:49:57,911, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:49:57,913, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:49:57,915, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:49:57,917, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:49:57,919, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:52:20,885, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:52:21,530, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:52:22,158, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:52:22,159, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:52:41,601, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:52:41,602, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:52:59,756, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:52:59,758, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:52:59,924, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:52:59,926, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:53:01,693, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:53:02,547, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:53:02,548, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:53:41,148, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:53:41,150, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:53:41,151, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:53:41,152, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:53:41,154, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:56:04,037, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:56:04,620, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:56:05,378, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:56:05,380, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 20:56:24,821, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 20:56:24,823, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 20:56:42,356, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 20:56:42,359, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:56:42,564, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:56:42,565, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 20:56:44,437, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:56:45,283, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 20:56:45,284, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 20:57:23,272, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 20:57:23,273, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 20:57:23,274, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 20:57:23,276, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 20:57:23,278, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 20:59:43,604, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 20:59:44,322, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 20:59:45,221, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 20:59:45,222, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 21:00:03,941, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 21:00:03,943, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 21:00:22,837, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 21:00:22,839, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 21:00:23,064, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 21:00:23,065, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 21:00:24,884, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 21:00:25,592, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 21:00:25,593, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 21:01:19,235, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 21:01:19,238, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 21:01:19,239, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 21:01:19,241, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 21:01:19,243, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 21:03:38,963, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 21:03:39,733, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 21:03:40,675, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 21:03:40,676, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 21:04:01,315, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 21:04:01,321, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 21:04:20,796, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 21:04:20,797, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 21:04:21,019, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 21:04:21,021, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 21:04:22,779, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 21:04:23,549, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 21:04:23,550, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 21:05:24,211, data_clustering, INFO, Initialized clustering model ]
[2024-12-25 21:05:24,213, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-25 21:05:24,214, data_clustering, INFO, Initializing the DBSCAN clustering model ]
[2024-12-25 21:05:24,216, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-25 21:05:24,218, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-25 21:08:00,298, data_clustering, INFO, Fitting the data to the DBSCAN cluster model ]
[2024-12-25 21:08:03,229, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 21:08:04,141, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 21:08:04,142, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-25 21:08:24,808, data_clustering, INFO, Transforming the test data to ]
[2024-12-25 21:08:24,810, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-25 21:08:44,917, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-25 21:08:44,919, data_clustering, INFO, Transforming the data using the DBSCAN clustering model ]
[2024-12-25 21:08:45,151, data_clustering, INFO, Successfully assigned clusters using DBSCAN ]
[2024-12-25 21:08:45,152, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-25 21:08:47,054, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 21:08:47,827, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-25 21:08:47,828, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-25 21:24:33,355, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 21:24:33,359, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 21:25:06,076, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 21:25:06,079, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 21:25:15,473, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 21:25:15,475, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 21:25:16,487, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 21:25:16,493, text_data_transformation, INFO, Error in preprocessing the text: 'float' object has no attribute 'lower' ]
[2024-12-25 21:25:26,981, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 21:25:26,983, text_data_transformation, INFO, Error in preprocessing the text: 'numpy.ndarray' object has no attribute 'map' ]
[2024-12-25 21:26:18,519, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 21:26:18,522, text_data_transformation, INFO, Error in preprocessing the text: 'int' object has no attribute 'lower' ]
[2024-12-25 21:26:34,300, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 21:26:34,348, text_data_transformation, INFO, Error in preprocessing the text: 'float' object has no attribute 'lower' ]
[2024-12-25 21:27:44,922, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 21:27:45,027, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 21:27:51,575, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 21:27:51,578, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 21:28:19,250, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 21:34:21,892, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 21:34:21,895, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 21:34:25,007, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 21:34:25,009, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 21:34:39,275, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 21:34:39,388, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 21:34:45,536, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 21:34:45,538, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 21:35:09,212, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 21:49:05,627, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 21:49:05,630, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 21:50:28,304, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 21:50:28,307, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 21:50:30,785, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 21:50:30,892, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 21:50:36,889, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 21:50:36,893, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 21:50:59,514, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 22:00:46,861, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 22:00:46,864, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 22:00:58,543, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 22:00:58,545, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 22:01:01,194, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 22:01:01,317, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 22:01:07,974, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 22:01:07,977, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 22:01:30,711, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 22:46:10,158, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 22:46:10,166, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 22:46:13,488, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 22:46:13,586, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 22:46:21,195, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 22:46:21,198, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 22:46:46,556, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-25 22:59:07,028, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-25 22:59:07,033, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-25 22:59:24,395, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-25 22:59:24,506, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-25 22:59:30,941, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-25 22:59:30,945, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-25 23:00:00,033, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 12:29:58,814, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 12:29:58,833, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 12:33:42,146, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-26 12:33:42,148, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-26 12:33:42,152, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-26 12:33:42,155, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-26 12:33:42,627, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-26 12:33:44,141, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-26 12:33:44,142, data_ingestion, INFO, Initiating train test split ]
[2024-12-26 12:33:45,702, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-26 12:34:36,464, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-26 12:34:36,466, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-26 12:34:38,043, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 12:34:38,046, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 12:34:38,046, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-26 12:34:38,048, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-26 12:34:39,246, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-26T12:34:39.246112', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-26 12:34:39,247, text_data_transformation, INFO, Text pipeline created ]
[2024-12-26 12:34:39,248, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-26 12:34:39,251, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 12:34:39,369, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 12:34:45,276, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 12:34:45,280, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 12:35:11,849, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 12:35:11,851, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-26 12:36:59,206, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-26 12:36:59,207, text_data_transformation, INFO, Transforming test data ]
[2024-12-26 12:36:59,218, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 12:36:59,270, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 12:37:01,744, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 12:37:01,750, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 12:37:24,083, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 12:37:24,089, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-26 12:38:30,549, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-26 12:38:30,551, text_data_transformation, INFO, Transforming test data ]
[2024-12-26 12:38:31,734, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-26 12:38:31,735, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-26 12:46:53,359, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-26 12:47:02,699, data_clustering, INFO, Initialized clustering model ]
[2024-12-26 12:47:02,700, data_clustering, INFO, Initiating the UMAP model for dimensionality reduction ]
[2024-12-26 12:47:02,702, data_clustering, INFO, Initializing the K-Means clustering model ]
[2024-12-26 12:47:02,704, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-26 12:47:02,705, data_clustering, INFO, Fitting the UMAP to the given data ]
[2024-12-26 12:49:41,430, data_clustering, INFO, Fitting the data to the K-Means cluster model ]
[2024-12-26 12:49:41,833, data_clustering, INFO, Transforming the data using the K-Means clustering model ]
[2024-12-26 12:49:41,945, data_clustering, INFO, Successfully assigned clusters using K-Means ]
[2024-12-26 12:49:41,947, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-26 12:50:03,311, data_clustering, INFO, Transforming the test data to ]
[2024-12-26 12:50:03,312, data_clustering, INFO, Transforming the given data using fitted UMAP model ]
[2024-12-26 12:50:23,971, data_clustering, INFO, Output shape after UMAP transform: (44275, 2) ]
[2024-12-26 12:50:23,972, data_clustering, INFO, Transforming the data using the K-Means clustering model ]
[2024-12-26 12:50:24,016, data_clustering, INFO, Successfully assigned clusters using K-Means ]
[2024-12-26 12:50:24,017, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-26 12:50:26,123, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-26 12:50:26,988, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-26 12:50:26,989, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-26 12:50:45,516, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 12:50:45,520, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 12:51:10,925, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 12:51:10,954, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 12:51:12,251, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 12:51:12,252, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 12:51:16,948, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 12:53:47,587, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 12:53:47,590, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 12:53:54,551, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 12:53:54,578, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 12:53:55,855, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 12:53:55,857, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 12:54:00,444, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 13:01:53,156, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 13:01:53,159, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 13:01:59,103, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 13:01:59,128, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 13:02:00,439, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 13:02:00,440, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 13:02:05,381, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 16:46:36,934, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 16:46:36,937, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 16:46:46,612, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 16:46:46,640, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 16:46:47,756, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 16:46:47,757, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 16:46:51,666, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 16:55:35,109, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 16:55:35,112, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 16:55:56,084, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 16:55:56,087, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 16:55:59,501, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 16:55:59,525, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 16:56:00,603, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 16:56:00,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 16:56:04,780, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 16:57:44,563, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 16:57:44,566, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 16:57:46,899, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 16:57:46,922, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 16:57:48,091, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 16:57:48,092, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 16:57:52,311, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:04:31,245, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:04:31,248, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:04:53,847, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:04:53,892, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:04:56,592, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:04:56,594, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:05:00,968, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:07:18,255, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:07:18,257, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:07:28,552, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:07:28,579, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:07:29,761, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:07:29,762, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:07:33,939, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:17:37,316, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,318, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,325, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,327, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,330, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,333, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,336, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,340, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,343, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,346, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,350, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,354, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,358, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,361, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,364, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,366, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,368, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,370, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,534, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,538, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,542, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,545, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,549, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,552, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,556, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,559, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,562, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,565, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,569, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,572, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,575, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,579, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:37,583, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:17:37,586, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 17:17:45,885, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:17:46,050, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:17:46,176, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:17:46,330, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:17:46,493, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:17:46,580, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:17:46,645, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:17:46,683, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:17:46,739, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:17:46,832, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:17:46,843, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:17:47,009, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:17:47,037, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:17:47,246, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:17:47,368, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:17:47,561, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:17:55,368, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:17:55,372, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:17:55,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:17:55,514, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:17:55,613, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:17:55,615, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:17:55,696, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:17:55,698, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:17:56,033, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:17:56,036, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:17:56,063, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:17:56,066, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:17:56,365, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:17:56,366, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:17:56,518, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:17:56,522, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 17:18:40,903, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:18:42,258, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:18:42,424, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:18:42,540, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:18:42,547, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:18:42,860, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:18:43,665, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:18:44,274, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 17:26:08,797, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,805, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,807, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,810, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,813, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,817, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,820, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,824, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,827, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,894, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,899, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:08,913, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:09,082, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:09,094, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:09,099, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:09,104, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:09,110, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:26:17,336, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:26:17,503, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:26:17,866, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:26:17,923, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:26:18,063, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:26:18,108, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:26:18,397, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:26:18,474, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:26:18,503, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:26:18,568, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:26:18,648, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:26:18,700, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:26:18,753, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:26:18,881, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:26:18,918, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:26:19,068, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:26:27,113, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:26:27,316, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:26:27,537, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:26:27,632, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:26:27,928, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:26:28,272, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:26:28,314, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:26:28,325, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:29:11,669, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,681, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,683, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,688, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,692, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,696, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,699, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,703, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,707, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,781, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,786, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,796, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,801, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,806, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,972, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,978, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:11,983, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:29:20,707, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:29:20,881, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:29:21,073, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:29:21,298, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:29:21,530, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:29:21,715, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:29:21,941, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:29:21,973, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:29:22,098, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:29:22,115, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:29:22,137, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:29:22,199, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:29:22,258, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:29:22,293, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:29:22,383, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:29:22,473, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:29:29,922, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:29:30,295, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:29:30,626, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:29:31,077, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:29:31,394, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:29:31,400, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:29:31,598, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:29:31,731, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:51:43,866, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:51:43,878, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:51:43,883, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:51:43,889, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:51:43,894, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:51:43,899, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:51:43,904, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:51:43,910, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 17:51:44,556, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:51:44,880, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:52:06,844, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 17:52:28,436, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 17:52:28,713, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 17:52:48,493, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:15:29,390, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:15:29,766, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:15:47,383, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:15:47,499, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:15:47,800, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:16:04,734, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:25:13,325, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:25:13,644, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:25:33,885, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:26:01,509, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:26:02,205, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:26:27,500, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:26:31,910, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:26:32,102, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:26:32,291, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:26:32,373, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:26:33,128, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 18:26:33,134, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 18:26:33,139, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 18:26:33,144, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 18:26:33,149, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 18:26:33,154, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 18:26:33,707, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:26:33,930, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:26:46,363, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:26:46,432, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:26:47,812, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:26:48,070, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:26:48,182, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:27:03,133, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:37:57,577, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:37:57,919, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:38:17,093, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 18:38:23,231, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 18:38:23,481, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 18:38:40,347, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:22:30,479, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:22:30,817, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:22:49,639, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:22:58,582, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:22:58,828, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:23:15,702, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:33:15,799, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:33:16,108, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:33:38,102, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:33:50,748, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:33:51,048, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:34:15,265, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:35:08,280, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:35:08,871, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:35:20,190, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:35:20,521, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:35:21,046, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:35:21,494, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:35:30,821, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:35:34,062, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:35:34,352, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:35:39,571, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:35:40,123, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:35:52,280, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:38:38,821, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,831, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,833, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,836, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,838, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,842, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,847, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,851, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,856, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,962, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,975, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:38,991, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:39,160, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:39,168, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:39,174, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:39,178, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:39,184, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:38:48,400, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:38:48,517, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:38:48,578, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:38:48,715, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:38:48,734, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:38:48,805, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:38:48,920, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:38:48,988, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:38:49,283, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:38:49,469, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:38:49,555, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:38:49,737, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:38:49,743, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:38:49,857, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:38:50,050, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:38:50,143, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:38:58,818, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:38:58,885, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:38:59,364, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:38:59,422, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:38:59,891, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:39:00,083, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:39:00,154, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:39:00,497, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:46:22,335, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,345, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,347, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,350, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,353, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,357, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,360, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,364, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,369, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,424, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,610, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,618, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,626, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,630, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,635, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,641, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:22,645, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:46:31,873, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:46:32,087, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:46:32,397, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:46:32,484, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:46:32,608, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:46:32,690, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:46:32,981, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:46:33,227, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:46:33,543, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:46:33,748, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:46:33,793, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:46:33,871, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:46:33,992, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:46:34,061, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:46:34,100, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:46:34,295, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:46:42,674, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:46:43,286, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:46:43,477, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:46:43,931, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:46:44,771, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:46:45,156, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:46:45,234, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:46:45,240, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:54:57,731, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,741, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,744, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,749, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,752, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,755, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,759, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,762, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,766, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,839, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,844, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:57,852, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:58,035, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:58,042, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:58,047, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:58,051, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:54:58,057, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,655, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,664, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,667, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,670, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,674, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,678, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,681, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,684, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,688, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,750, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,756, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,761, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,767, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,949, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,956, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,962, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:06,967, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:16,207, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:55:16,387, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:55:16,403, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:55:16,586, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:55:17,051, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:55:17,103, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:55:17,234, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:55:17,311, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:55:17,454, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:55:17,656, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:55:17,657, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:55:17,892, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:55:17,976, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:55:18,080, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:55:18,253, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:55:18,331, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:55:27,317, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:55:27,743, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:55:27,859, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:55:28,073, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:55:28,459, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:55:56,684, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,693, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,696, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,698, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,700, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,703, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,707, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,711, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,715, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,791, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,795, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,960, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,969, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,974, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,979, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,984, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:55:56,989, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 19:56:06,094, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:56:06,287, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:56:06,609, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:56:06,686, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:56:06,795, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:56:06,862, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:56:07,351, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:56:07,386, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:56:07,532, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:56:07,597, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:56:07,603, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:56:07,801, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:56:07,861, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:56:07,937, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 19:56:08,051, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:56:08,115, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 19:56:16,159, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:56:16,631, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:56:16,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:56:17,822, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:56:17,897, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:56:17,974, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:56:18,027, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 19:56:18,165, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 20:13:11,663, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 20:13:11,672, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 20:13:11,677, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 20:13:11,682, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 20:13:11,687, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 20:13:11,692, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 20:13:11,697, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 20:13:11,703, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 20:13:12,327, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 20:13:12,657, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 20:13:16,129, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 20:13:16,459, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 20:13:17,174, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 20:13:17,528, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 20:13:35,208, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 20:13:38,760, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 20:13:40,071, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 20:17:40,587, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 20:17:40,920, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 20:17:42,280, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 20:17:42,569, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 20:17:44,398, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 20:17:44,720, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 20:18:00,882, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 20:18:02,459, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 20:18:04,816, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 20:19:36,878, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 20:19:37,303, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 20:19:37,988, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 20:19:38,442, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 20:20:04,310, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 20:20:05,075, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:19:53,269, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:19:53,421, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:19:59,734, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:19:59,742, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:20:22,978, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:20:22,980, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:22,989, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:22,995, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:22,998, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,072, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,074, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,084, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,086, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,099, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,100, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,114, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,115, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,129, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,132, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,146, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,148, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,162, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,164, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,178, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,180, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,194, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,196, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,210, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,212, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,225, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,228, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,240, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,243, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,256, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,259, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,270, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,273, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,285, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,287, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,299, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,300, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,316, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,335, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,356, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,358, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,372, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,378, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,389, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,392, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,403, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,405, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,417, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,418, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,430, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,431, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,444, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,446, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,458, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,460, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,472, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,475, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,486, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,488, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,499, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,502, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,513, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,515, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,528, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,529, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,544, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,546, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,561, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,563, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,578, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,581, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,596, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,599, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,623, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,628, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,643, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,646, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,662, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,666, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,686, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,701, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,720, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,725, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,738, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,742, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,754, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,758, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,770, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,773, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,784, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,786, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:20:23,800, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:20:23,802, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:21:26,858, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:21:26,861, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:21:26,866, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:21:26,869, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:21:26,882, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:21:26,973, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:21:31,995, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:21:32,000, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:21:51,646, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:33:36,695, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:33:36,700, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:33:44,005, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:33:44,027, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:33:45,230, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:33:45,233, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:33:49,404, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:34:07,998, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:34:08,001, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:34:10,758, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:34:10,781, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:34:11,908, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:34:11,911, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:34:16,167, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:38:17,935, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:38:17,938, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:38:23,676, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:38:23,697, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:38:24,921, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:38:24,922, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:38:28,673, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:41:59,908, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:41:59,911, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:42:01,762, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:42:01,783, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:42:02,831, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:42:02,834, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:42:06,840, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:44:27,364, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:44:27,367, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:44:31,784, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:44:31,808, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:44:32,838, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:44:32,840, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:44:37,017, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:50:34,823, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,826, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,834, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,836, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,839, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,841, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,843, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,846, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,848, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,852, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,854, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,857, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,858, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,861, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,862, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,866, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,867, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,871, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,875, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,878, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,905, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,930, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,935, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,961, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,963, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:34,989, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:34,991, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:35,021, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:35,050, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:35,079, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:35,099, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:35,108, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:35,113, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:50:35,117, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:50:40,292, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:50:40,322, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:50:40,363, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:50:40,393, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:50:40,396, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:50:40,415, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:50:40,421, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:50:40,432, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:50:40,437, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:50:40,442, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:50:40,454, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:50:40,458, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:50:40,459, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:50:40,464, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:50:40,479, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:50:40,485, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:50:41,900, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:50:41,901, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:50:42,008, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:50:42,008, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:50:42,073, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:50:42,073, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:50:42,076, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:50:42,077, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:50:42,087, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:50:42,087, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:50:42,088, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:50:42,088, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:50:42,236, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:50:42,237, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:50:42,252, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:50:42,253, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:50:57,094, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:50:57,240, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:50:57,340, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:50:57,469, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:50:57,950, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:50:57,955, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:50:58,378, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:50:58,703, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:52:34,994, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:52:35,000, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:52:35,004, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:52:35,007, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:52:35,011, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:52:35,014, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:52:35,020, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:52:35,024, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:52:35,028, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:52:35,031, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:52:35,035, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:52:35,039, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:52:35,044, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:52:35,047, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:52:35,051, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:52:35,054, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:52:35,143, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:52:35,236, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:52:36,133, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:52:36,193, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:52:41,438, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:52:41,439, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:52:42,301, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:52:42,307, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:52:50,853, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:52:50,900, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:52:51,628, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:52:51,669, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:52:53,544, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:52:53,545, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:52:53,933, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:52:53,973, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:52:54,218, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:52:54,220, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:52:56,885, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:52:56,886, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:52:57,306, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:52:57,602, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:53:07,817, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:53:09,159, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:53:13,947, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:53:16,920, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:53:16,971, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:53:18,148, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:53:18,184, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:53:20,689, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:53:20,763, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:53:20,814, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:53:20,816, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:53:22,879, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:53:22,880, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:53:25,989, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:53:25,990, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:53:42,899, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:54:09,721, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,723, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,732, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,734, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,736, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,740, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,742, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,745, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,747, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,749, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,752, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,757, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,759, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,761, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,763, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,766, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,770, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,773, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,817, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,820, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,824, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,827, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,830, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,834, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,839, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,843, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,847, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,850, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,855, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,858, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,884, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,973, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:09,977, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:09,980, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:18,143, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:18,165, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:18,182, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:18,195, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:18,468, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:18,515, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:18,704, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:18,741, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:18,787, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:18,794, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:18,822, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:18,838, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:18,929, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:18,958, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:19,081, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:19,129, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:20,020, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:20,021, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:20,148, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:20,149, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:20,281, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:20,281, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:20,547, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:20,548, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:20,583, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:20,584, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:20,650, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:20,652, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:20,829, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:20,830, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:20,846, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:20,847, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:35,876, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,878, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:35,888, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,891, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:35,894, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,897, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:35,900, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,903, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:35,905, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,908, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:35,911, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,914, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:35,918, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,920, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:35,923, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,940, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:35,958, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:35,963, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:36,040, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:36,143, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:36,148, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:36,154, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:36,158, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:36,161, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:36,165, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:36,169, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:36,173, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:36,176, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:36,180, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:36,184, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:36,188, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:36,192, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:36,196, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:54:36,199, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:54:44,591, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:44,623, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:44,706, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:44,712, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:44,736, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:44,744, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:44,872, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:44,902, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:45,156, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:45,188, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:45,194, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:45,225, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:45,327, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:45,362, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:45,559, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:54:45,588, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:54:46,232, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:46,233, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:46,378, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:46,378, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:46,457, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:46,457, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:46,541, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:46,542, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:46,867, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:46,867, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:46,908, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:46,908, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:47,058, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:47,059, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:54:47,280, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:54:47,282, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:55:01,740, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:55:01,821, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:55:02,609, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:55:02,622, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:55:02,665, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:55:02,783, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:55:03,155, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:55:03,165, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:56:45,761, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:56:45,768, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:56:45,772, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:56:45,775, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:56:45,778, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:56:45,781, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:56:45,785, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:56:45,788, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:56:45,792, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:56:45,795, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:56:45,799, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:56:45,803, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:56:45,807, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:56:45,811, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:56:45,815, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:56:45,821, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:56:46,373, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:56:46,460, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:56:46,904, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:56:46,981, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:56:52,704, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:56:52,707, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:56:53,218, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:56:53,219, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:56:57,853, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:56:57,897, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:56:58,075, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:56:58,134, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:56:58,991, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:56:59,025, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:57:00,462, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:57:00,463, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:57:00,601, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:57:00,602, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:57:01,494, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:57:01,495, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:57:04,988, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:57:05,236, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:57:13,101, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:57:13,752, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:57:15,166, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:57:23,317, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:57:23,365, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:57:23,579, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:57:23,631, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:57:23,825, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:57:23,905, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:57:27,442, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:57:27,444, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:57:27,706, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:57:27,707, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:57:27,842, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:57:27,843, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:57:43,201, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:57:43,461, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:57:43,467, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:58:48,609, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:58:48,613, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:58:48,616, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:58:48,619, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:58:48,623, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:58:48,626, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:58:48,630, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:58:48,633, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:58:48,638, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:58:48,641, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:58:48,645, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 21:58:48,648, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 21:58:48,928, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:58:49,018, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:58:49,897, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:58:49,970, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:58:55,202, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:58:55,203, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:58:56,010, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:58:56,011, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:58:58,123, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:58:58,171, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:58:59,195, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:58:59,234, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:59:01,535, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:59:01,537, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:59:02,796, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:59:02,801, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:59:06,072, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:59:06,109, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:59:08,422, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:59:08,679, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:59:08,967, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:59:08,969, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:59:14,188, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:59:16,124, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:59:35,334, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 21:59:36,075, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:59:36,144, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:59:36,358, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 21:59:36,411, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 21:59:41,539, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:59:41,541, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 21:59:41,870, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 21:59:41,871, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:00:02,943, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:00:03,009, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:00:08,139, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:00:08,145, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:00:08,742, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:00:09,286, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:00:43,916, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:01:23,563, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:01:23,613, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:01:24,472, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:01:24,513, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:01:25,690, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:01:25,737, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:01:25,953, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:01:26,000, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:01:27,007, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:01:27,009, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:01:27,844, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:01:27,845, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:01:28,844, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:01:28,845, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:01:29,078, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:01:29,080, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:01:39,612, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:01:40,208, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:01:40,298, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:01:40,341, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:01:41,236, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:01:41,250, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:01:42,914, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:01:42,915, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:02:14,988, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:02:28,607, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:02:28,671, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:02:33,053, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:02:33,054, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:02:51,981, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:04:09,018, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:04:09,020, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:04:09,025, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:04:09,050, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:04:10,236, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:04:10,237, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:04:14,588, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:09:10,758, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,761, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,770, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,772, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,774, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,775, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,779, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,782, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,786, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,788, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,791, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,794, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,797, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,799, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,803, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,806, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,808, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,810, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,813, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,837, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,857, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:09:10,839, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,863, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,872, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:09:10,886, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:09:10,864, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,890, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,899, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:09:10,914, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:09:10,892, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,928, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:09:10,941, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:09:10,916, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:10,960, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:09:10,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:09:10,945, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:10,980, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:09:11,004, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:09:10,977, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:11,028, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:09:11,007, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:11,038, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:09:11,042, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:11,045, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:11,049, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:11,054, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:09:11,056, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:09:11,063, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:09:11,077, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:09:11,102, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:09:12,464, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:09:12,465, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:09:12,509, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:09:12,511, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:09:12,614, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:09:12,615, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:09:12,726, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:09:12,727, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:09:12,919, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:09:12,920, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:09:13,010, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:09:13,011, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:09:13,013, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:09:13,013, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:09:13,046, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:09:13,047, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:09:19,292, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:09:19,321, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:09:19,356, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:09:19,916, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:09:20,080, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:09:20,106, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:09:20,282, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:09:20,338, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:11:01,295, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:11:01,298, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:11:01,302, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:11:01,306, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:11:01,310, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:11:01,313, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:11:01,317, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:11:01,320, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:11:01,324, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:11:01,328, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:11:01,332, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:11:01,335, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:11:01,339, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:11:01,380, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:11:01,383, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:11:01,387, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:11:01,455, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:11:01,561, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:11:01,718, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:11:01,783, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:11:08,410, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:11:08,411, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:11:08,600, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:11:08,607, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:11:15,652, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:11:15,667, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:11:15,689, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:11:15,692, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:11:16,163, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:11:16,196, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:11:18,192, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:11:18,193, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:11:18,224, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:11:18,225, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:11:18,654, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:11:18,655, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:11:22,148, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:11:22,159, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:11:31,520, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:11:32,114, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:11:32,559, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:11:39,420, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:11:39,472, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:11:40,267, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:11:40,318, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:11:41,388, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:11:41,435, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:11:43,557, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:11:43,558, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:11:44,375, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:11:44,377, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:11:45,399, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:11:45,399, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:11:59,139, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:11:59,937, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:12:00,669, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:13:03,435, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:13:03,439, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:13:03,443, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:13:03,446, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:13:03,449, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:13:03,453, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:13:03,457, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:13:03,460, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:13:03,464, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:13:03,467, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:13:03,471, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:13:03,475, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:13:03,572, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:13:03,625, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:13:04,432, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:13:04,518, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:13:10,965, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:13:10,966, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:13:11,500, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:13:11,504, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:13:14,044, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:13:14,092, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:13:14,223, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:13:14,271, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:13:17,620, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:13:17,621, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:13:17,817, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:13:17,820, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:13:20,650, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:13:20,715, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:13:23,679, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:13:23,680, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:13:23,821, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:13:24,262, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:13:30,989, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:13:31,537, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:13:48,611, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:13:50,008, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:13:50,074, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:13:50,547, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:13:50,616, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:13:55,309, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:13:55,311, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:13:56,036, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:13:56,048, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:14:19,639, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:14:19,691, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:14:22,208, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:14:22,711, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:14:24,015, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:14:24,018, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:15:00,894, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:15:41,037, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:15:41,076, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:15:41,152, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:15:41,193, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:15:41,620, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:15:41,669, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:15:42,651, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:15:42,693, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:15:44,131, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:15:44,132, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:15:44,243, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:15:44,244, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:15:44,700, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:15:44,701, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:15:45,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:15:45,668, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:15:53,305, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:15:53,341, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:15:55,655, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:15:55,702, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:15:55,789, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:15:55,789, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:15:56,011, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:15:56,833, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:16:23,323, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:16:41,528, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:16:41,588, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:16:46,176, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:16:46,178, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:17:05,455, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:18:23,394, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:18:23,397, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:18:23,402, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:18:23,426, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:18:24,624, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:18:24,626, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:18:28,673, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:25:36,620, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:25:36,667, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:25:38,593, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:25:38,596, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:25:45,002, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:28:40,141, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:28:40,144, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:28:42,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:28:42,998, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:28:44,647, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:28:44,648, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:28:53,290, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:30:04,859, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:30:04,861, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:30:06,331, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:30:06,372, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:30:08,534, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:30:08,536, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:30:14,595, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:34:47,053, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:34:47,056, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:34:52,323, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:34:52,325, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:34:55,128, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:34:55,160, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:34:56,801, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:34:56,803, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:35:02,829, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:37:19,896, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:37:19,899, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:37:22,589, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:37:22,623, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:37:24,258, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:37:24,259, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:37:30,764, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:40:06,574, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:40:06,577, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:40:09,711, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:40:09,714, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:40:11,509, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:40:11,543, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:40:13,119, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:40:13,121, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:40:19,074, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:41:45,615, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:41:45,617, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:41:47,522, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:41:47,552, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:41:49,230, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:41:49,231, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:41:55,610, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-26 22:43:02,364, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-26 22:43:02,366, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-26 22:43:03,360, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-26 22:43:03,391, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-26 22:43:05,126, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-26 22:43:05,127, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-26 22:43:11,314, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 07:56:53,486, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 07:56:53,489, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 07:56:58,564, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 07:56:58,602, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 07:57:00,181, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 07:57:00,183, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 07:57:05,914, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:16:36,217, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,219, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,225, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,228, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,230, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,232, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,234, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,236, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,237, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,240, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,241, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,243, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,245, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,247, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,249, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,252, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,257, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,259, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,428, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,431, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,438, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,444, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,448, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,451, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,456, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,459, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,464, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,466, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,469, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,473, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:16:36,477, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:16:36,480, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,075, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,078, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,086, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,088, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,090, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,092, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,096, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,099, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,102, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,105, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,108, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,112, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,115, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,119, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,122, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,125, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,128, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,131, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,196, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,199, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,207, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,364, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,370, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,374, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,380, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,383, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,387, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,391, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,395, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,399, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:18:38,402, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:18:38,406, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:29,958, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:29,961, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,205, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,208, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,216, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,218, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,220, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,222, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,224, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,226, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,228, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,232, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,235, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,238, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,240, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,242, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,244, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,247, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,250, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,252, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,319, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,322, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,443, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,494, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,499, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,503, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,507, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,511, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,515, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,519, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,524, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,527, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:19:39,537, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:19:39,540, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,865, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,867, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,875, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,877, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,879, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,881, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,883, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,886, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,889, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,892, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,895, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,899, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,903, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,906, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,913, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,916, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:07,919, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:07,929, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:08,008, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:08,023, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:08,035, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:08,205, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:08,209, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:08,212, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:08,217, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:08,220, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:08,226, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:08,229, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:08,233, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:08,237, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:08,243, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:21:08,251, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:21:16,386, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:21:16,563, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:21:16,693, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:21:16,855, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:21:17,042, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:21:17,189, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:21:17,192, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:21:17,285, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:21:17,334, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:21:17,450, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:21:17,758, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:21:17,924, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:21:18,079, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:21:18,281, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:21:18,306, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:21:18,460, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:21:27,329, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:21:27,333, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:21:28,230, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:21:28,233, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:21:28,444, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:21:28,447, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:21:28,854, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:21:28,857, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:21:29,546, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:21:29,550, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:21:29,822, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:21:29,825, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:21:29,948, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:21:29,951, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:21:31,321, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:21:31,323, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:22:26,166, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,169, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,177, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,179, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,181, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,183, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,185, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,187, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,189, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,193, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,196, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,199, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,201, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,203, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,205, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,207, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,210, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,214, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,293, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,465, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,469, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,472, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,478, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,481, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,485, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,488, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,493, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,497, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,502, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,505, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:26,510, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:22:26,513, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:22:34,165, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:22:34,199, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:22:34,325, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:22:34,364, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:22:34,455, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:22:34,587, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:22:34,619, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:22:34,772, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:22:35,013, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:22:35,089, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:22:35,171, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:22:35,184, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:22:35,243, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:22:35,285, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:22:35,331, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:22:35,433, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:22:44,467, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:22:44,470, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:22:44,603, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:22:44,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:22:45,481, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:22:45,484, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:22:45,627, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:22:45,630, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:22:45,770, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:22:45,772, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:22:45,844, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:22:45,847, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:22:46,058, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:22:46,060, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:22:46,560, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:22:46,565, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:23:30,379, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:23:32,362, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:23:32,642, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:23:32,741, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:23:33,203, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:23:33,296, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:23:33,605, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:23:33,741, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:23:42,643, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:23:42,775, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:23:44,846, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:23:45,214, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:23:45,327, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:23:45,530, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:23:45,596, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:23:45,800, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:23:46,029, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:23:46,250, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:23:46,268, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:23:46,446, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:23:46,449, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:23:46,633, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:23:52,724, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:23:52,731, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:23:55,035, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:23:55,037, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:23:55,327, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:23:55,330, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:23:55,549, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:23:55,551, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:23:55,855, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:23:55,858, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:23:56,215, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:23:56,217, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:23:56,294, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:23:56,297, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:24:24,819, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:24:26,977, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:24:27,194, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:24:27,234, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:24:27,313, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:24:27,389, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:24:27,952, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:28:49,186, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,189, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,195, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,197, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,199, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,200, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,203, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,207, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,210, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,213, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,215, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,217, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,219, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,221, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,224, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,228, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,230, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,232, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,355, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,360, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,364, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,366, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,368, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,371, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,373, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,377, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,380, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,383, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,501, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:28:49,511, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,518, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,521, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:28:49,523, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:28:49,599, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:28:49,655, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:28:49,783, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:28:49,832, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:28:49,975, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:28:50,010, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:28:50,158, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:28:50,181, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:28:50,351, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:28:50,372, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:28:50,597, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:28:50,670, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:28:50,904, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:28:58,038, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:28:58,197, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:28:58,631, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:28:58,633, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:28:58,933, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:28:58,936, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:28:58,973, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:28:58,975, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:28:59,119, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:28:59,121, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:28:59,494, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:28:59,496, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:28:59,784, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:28:59,787, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:28:59,879, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:28:59,881, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:29:07,561, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:29:07,563, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:29:32,578, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:29:32,627, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:29:33,486, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:29:33,611, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:29:34,141, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:29:34,291, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:29:35,023, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:29:52,246, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:32:53,238, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:32:53,442, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:32:54,557, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:32:54,724, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:32:55,661, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:32:55,826, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:33:02,973, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:33:02,975, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:33:04,666, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:33:04,668, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:33:05,262, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:33:05,332, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:33:05,335, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:33:05,448, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:33:05,553, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:33:05,717, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:33:07,422, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:33:07,597, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:33:15,054, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:33:15,057, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:33:15,301, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:33:15,303, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:33:17,669, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:33:17,672, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:33:40,433, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:33:41,624, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:33:42,646, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:33:51,573, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:33:52,449, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:33:55,941, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:35:10,161, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:35:10,339, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:35:21,080, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:35:21,082, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:35:55,946, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:38:02,550, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 08:38:02,553, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 08:38:02,561, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:38:02,697, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:38:10,551, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:38:10,560, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:38:36,169, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:45:38,801, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:45:38,804, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-27 08:46:01,406, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:46:01,408, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-27 08:46:18,339, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:46:18,491, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:46:24,972, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:46:24,975, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:46:48,157, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:49:25,362, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-27 08:49:26,296, dictionary, INFO, adding document #10000 to Dictionary<52617 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:27,233, dictionary, INFO, adding document #20000 to Dictionary<80819 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:28,211, dictionary, INFO, adding document #30000 to Dictionary<105225 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:29,186, dictionary, INFO, adding document #40000 to Dictionary<126995 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:30,153, dictionary, INFO, adding document #50000 to Dictionary<146554 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:31,150, dictionary, INFO, adding document #60000 to Dictionary<164974 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:32,169, dictionary, INFO, adding document #70000 to Dictionary<182093 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:33,183, dictionary, INFO, adding document #80000 to Dictionary<199417 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:34,166, dictionary, INFO, adding document #90000 to Dictionary<216046 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:35,137, dictionary, INFO, adding document #100000 to Dictionary<231331 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:49:35,566, dictionary, INFO, built Dictionary<236284 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> from 103304 documents (total 7514878 corpus positions) ]
[2024-12-27 08:49:35,567, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<236284 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> from 103304 documents (total 7514878 corpus positions)", 'datetime': '2024-12-27T08:49:35.567962', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-27 08:51:22,312, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:51:22,314, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-27 08:51:31,781, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:51:31,926, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:51:38,311, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:51:38,322, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:52:02,739, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 08:55:02,140, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-27 08:55:03,266, dictionary, INFO, adding document #10000 to Dictionary<52617 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:04,270, dictionary, INFO, adding document #20000 to Dictionary<80819 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:05,246, dictionary, INFO, adding document #30000 to Dictionary<105225 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:06,885, dictionary, INFO, adding document #40000 to Dictionary<126995 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:09,743, dictionary, INFO, adding document #50000 to Dictionary<146554 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:10,726, dictionary, INFO, adding document #60000 to Dictionary<164974 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:11,711, dictionary, INFO, adding document #70000 to Dictionary<182093 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:12,829, dictionary, INFO, adding document #80000 to Dictionary<199417 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:15,857, dictionary, INFO, adding document #90000 to Dictionary<216046 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:16,892, dictionary, INFO, adding document #100000 to Dictionary<231331 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> ]
[2024-12-27 08:55:17,276, dictionary, INFO, built Dictionary<236284 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> from 103304 documents (total 7514878 corpus positions) ]
[2024-12-27 08:55:17,277, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<236284 unique tokens: ['(bordering', 'It', 'Its', 'Id', 'Khan']...> from 103304 documents (total 7514878 corpus positions)", 'datetime': '2024-12-27T08:55:17.277901', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-27 08:57:29,716, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:57:29,719, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-27 08:57:40,525, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 08:57:40,706, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 08:57:47,710, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 08:57:47,720, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 08:58:15,742, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 09:00:38,142, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 09:00:38,248, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 09:00:44,564, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 09:00:44,737, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 09:01:07,453, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 09:01:08,968, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-27 09:01:09,540, dictionary, INFO, adding document #10000 to Dictionary<24645 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:10,075, dictionary, INFO, adding document #20000 to Dictionary<36531 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:10,610, dictionary, INFO, adding document #30000 to Dictionary<46867 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:11,126, dictionary, INFO, adding document #40000 to Dictionary<56019 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:11,646, dictionary, INFO, adding document #50000 to Dictionary<64084 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:12,199, dictionary, INFO, adding document #60000 to Dictionary<71731 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:12,809, dictionary, INFO, adding document #70000 to Dictionary<78920 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:13,440, dictionary, INFO, adding document #80000 to Dictionary<86228 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:14,007, dictionary, INFO, adding document #90000 to Dictionary<93314 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:14,542, dictionary, INFO, adding document #100000 to Dictionary<99679 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:01:14,728, dictionary, INFO, built Dictionary<101751 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> from 103304 documents (total 4053893 corpus positions) ]
[2024-12-27 09:01:14,729, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<101751 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> from 103304 documents (total 4053893 corpus positions)", 'datetime': '2024-12-27T09:01:14.729067', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-27 09:03:07,963, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 09:03:07,965, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-27 09:03:20,348, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 09:03:20,449, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 09:03:27,027, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 09:03:27,034, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 09:03:51,766, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 09:06:12,488, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 09:06:12,581, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 09:06:18,759, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 09:06:18,768, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 09:06:40,642, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 09:06:40,643, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-27 09:08:50,092, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 09:08:50,093, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-27 09:09:00,887, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 09:09:01,028, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 09:09:07,248, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 09:09:07,251, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 09:09:28,601, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 09:11:43,363, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 09:11:43,453, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 09:11:49,752, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 09:11:49,759, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 09:12:12,377, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 09:12:12,932, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-27 09:12:13,554, dictionary, INFO, adding document #10000 to Dictionary<24645 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:14,094, dictionary, INFO, adding document #20000 to Dictionary<36531 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:14,639, dictionary, INFO, adding document #30000 to Dictionary<46867 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:15,147, dictionary, INFO, adding document #40000 to Dictionary<56019 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:15,722, dictionary, INFO, adding document #50000 to Dictionary<64084 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:16,280, dictionary, INFO, adding document #60000 to Dictionary<71731 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:16,941, dictionary, INFO, adding document #70000 to Dictionary<78920 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:17,571, dictionary, INFO, adding document #80000 to Dictionary<86228 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:18,187, dictionary, INFO, adding document #90000 to Dictionary<93314 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:18,716, dictionary, INFO, adding document #100000 to Dictionary<99679 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:12:18,893, dictionary, INFO, built Dictionary<101751 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> from 103304 documents (total 4053893 corpus positions) ]
[2024-12-27 09:12:18,894, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<101751 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> from 103304 documents (total 4053893 corpus positions)", 'datetime': '2024-12-27T09:12:18.894851', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-27 09:14:06,568, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-27 09:14:07,085, dictionary, INFO, adding document #10000 to Dictionary<24645 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:07,615, dictionary, INFO, adding document #20000 to Dictionary<36531 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:08,219, dictionary, INFO, adding document #30000 to Dictionary<46867 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:08,857, dictionary, INFO, adding document #40000 to Dictionary<56019 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:09,550, dictionary, INFO, adding document #50000 to Dictionary<64084 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:10,219, dictionary, INFO, adding document #60000 to Dictionary<71731 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:10,878, dictionary, INFO, adding document #70000 to Dictionary<78920 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:11,532, dictionary, INFO, adding document #80000 to Dictionary<86228 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:12,453, dictionary, INFO, adding document #90000 to Dictionary<93314 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:14,505, dictionary, INFO, adding document #100000 to Dictionary<99679 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:14:14,726, dictionary, INFO, built Dictionary<101751 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> from 103304 documents (total 4053893 corpus positions) ]
[2024-12-27 09:14:14,728, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<101751 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> from 103304 documents (total 4053893 corpus positions)", 'datetime': '2024-12-27T09:14:14.728562', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-27 09:14:14,865, dictionary, INFO, discarding 65756 tokens: [('food', 65143), ('tablefortwo', 1), ('patey', 1), ('tgifmust', 1), ('badji', 1), ('dehly', 1), ('koreander', 1), ('ringo', 1), ('seeingever', 1), ('thalimet', 1)]... ]
[2024-12-27 09:14:14,867, dictionary, INFO, keeping 35995 tokens which were in no less than 2 and no more than 51652 (=50.0%) documents ]
[2024-12-27 09:14:14,949, dictionary, INFO, resulting dictionary: Dictionary<35995 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:17,716, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-27 09:15:18,248, dictionary, INFO, adding document #10000 to Dictionary<24645 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:18,772, dictionary, INFO, adding document #20000 to Dictionary<36531 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:19,518, dictionary, INFO, adding document #30000 to Dictionary<46867 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:20,118, dictionary, INFO, adding document #40000 to Dictionary<56019 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:20,638, dictionary, INFO, adding document #50000 to Dictionary<64084 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:21,176, dictionary, INFO, adding document #60000 to Dictionary<71731 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:21,703, dictionary, INFO, adding document #70000 to Dictionary<78920 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:22,346, dictionary, INFO, adding document #80000 to Dictionary<86228 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:23,028, dictionary, INFO, adding document #90000 to Dictionary<93314 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:23,660, dictionary, INFO, adding document #100000 to Dictionary<99679 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> ]
[2024-12-27 09:15:23,864, dictionary, INFO, built Dictionary<101751 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> from 103304 documents (total 4053893 corpus positions) ]
[2024-12-27 09:15:23,865, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<101751 unique tokens: ['again', 'and', 'back', 'bordering', 'cocktail']...> from 103304 documents (total 4053893 corpus positions)", 'datetime': '2024-12-27T09:15:23.865682', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-27 12:54:12,929, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,942, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:12,950, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,953, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:12,954, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,957, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:12,959, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,961, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:12,962, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,964, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:12,966, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,968, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:12,969, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,971, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:12,973, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,974, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:12,976, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:12,978, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:13,161, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:13,164, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:13,170, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:13,176, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:13,181, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:13,184, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:13,189, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:13,193, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:13,197, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:13,200, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:13,204, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:13,208, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:13,212, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 12:54:13,215, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 12:54:28,998, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:54:29,121, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:54:29,125, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:54:29,166, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:54:29,209, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:54:29,216, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:54:29,309, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:54:29,319, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:54:29,324, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:54:29,339, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:54:29,349, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:54:29,350, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:54:29,392, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:54:29,480, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:54:29,488, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:54:29,501, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:54:41,426, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:54:41,430, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:54:41,487, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:54:41,490, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:54:41,529, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:54:41,531, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:54:41,997, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:54:42,000, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:54:42,009, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:54:42,012, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:54:42,757, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:54:42,759, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:54:42,923, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:54:42,926, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:54:43,139, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:54:43,143, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:55:37,662, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 12:55:38,956, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 12:55:39,687, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 12:55:40,710, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 12:55:41,091, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 12:55:41,109, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 12:55:41,422, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 12:55:42,053, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 12:59:29,576, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:59:29,754, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:59:31,076, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 12:59:31,272, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 12:59:41,859, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:59:41,862, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 12:59:43,051, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 12:59:43,053, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:00:02,819, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:00:03,010, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:00:06,431, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:00:06,624, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:00:07,414, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:00:07,601, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:00:14,406, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:00:14,409, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:00:17,858, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:00:17,860, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:00:18,883, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:00:18,885, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:00:25,522, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:00:26,029, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:00:58,261, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:01:01,788, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:01:02,121, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:02:17,843, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:02:18,067, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:02:18,415, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:02:18,603, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:02:29,548, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:02:29,552, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:02:30,139, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:02:30,144, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:03:09,217, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:03:09,890, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:05:42,386, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:05:42,388, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:05:42,395, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:05:42,484, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:05:48,868, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:05:48,870, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:06:12,006, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:13:46,750, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,753, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,762, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,764, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,766, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,769, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,772, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,775, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,779, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,781, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,784, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,787, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,788, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,792, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,793, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,796, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,799, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,801, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,969, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,975, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,981, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,983, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,987, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,991, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:46,995, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:46,997, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:47,001, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:47,004, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:47,009, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:47,013, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:47,017, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:13:47,020, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:13:54,648, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:13:54,805, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:13:54,809, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:13:54,952, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:13:54,967, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:13:55,109, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:13:55,124, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:13:55,274, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:13:55,490, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:13:55,623, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:13:55,642, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:13:55,774, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:13:55,877, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:13:55,880, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:13:56,030, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:13:56,033, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:14:03,946, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:14:03,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:14:04,340, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:14:04,342, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:14:04,539, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:14:04,542, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:14:04,827, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:14:04,829, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:14:04,995, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:14:04,997, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:14:05,200, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:14:05,203, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:14:05,410, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:14:05,412, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:14:05,601, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:14:05,604, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:14:49,912, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:14:49,992, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:14:50,540, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:14:50,834, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:14:50,987, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:14:51,157, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:14:51,195, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:14:51,363, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:18:36,570, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:18:36,757, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:18:36,882, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:18:37,114, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:18:37,212, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:18:37,415, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:18:44,295, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:18:44,475, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:18:46,131, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:18:46,307, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:18:46,674, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:18:46,879, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:18:47,994, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:18:47,996, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:18:48,237, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:18:48,239, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:18:48,614, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:18:48,618, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:18:54,902, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:18:54,905, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:18:56,544, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:18:56,546, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:18:57,265, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:18:57,267, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:19:25,823, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:19:26,281, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:19:26,323, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:19:32,758, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:19:34,496, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:19:35,740, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:20:25,182, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:20:25,349, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:20:35,418, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:20:35,422, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:21:10,610, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:23:49,478, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:23:49,481, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:23:49,489, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:23:49,592, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:23:56,128, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:23:56,132, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:24:21,320, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:28:19,050, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,053, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,059, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,062, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,065, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,068, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,071, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,073, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,076, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,078, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,080, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,083, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,087, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,090, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,092, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,095, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,097, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,100, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,254, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,258, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,261, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,390, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:28:19,414, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,419, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,422, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,425, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,429, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,433, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,438, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,442, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,526, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:28:19,593, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:28:19,444, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,609, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:28:19,612, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:28:19,753, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:28:19,797, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:28:19,964, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:28:20,007, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:28:20,137, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:28:20,169, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:28:20,315, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:28:27,971, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:28:28,072, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:28:28,126, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:28:28,206, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:28:28,234, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:28:28,391, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:28:28,890, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:28:28,892, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:28:29,055, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:28:29,058, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:28:29,461, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:28:29,464, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:28:29,799, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:28:29,801, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:28:29,981, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:28:29,983, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:28:37,330, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:28:37,333, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:28:37,433, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:28:37,436, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:28:37,567, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:28:37,569, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:29:04,786, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:29:04,797, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:29:05,397, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:29:05,529, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:29:05,752, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:29:22,705, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:29:23,619, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:29:24,047, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:32:51,386, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:32:51,555, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:32:54,444, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:32:54,618, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:33:01,594, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:33:01,597, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:33:04,879, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:33:04,881, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:33:31,829, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:33:31,986, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:33:32,920, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:33:33,083, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:33:35,439, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:33:35,614, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:33:40,324, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:33:42,117, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:33:42,119, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:33:43,015, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:33:43,017, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:33:45,015, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:33:45,338, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:33:45,341, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:34:19,302, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:34:20,511, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:34:23,297, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:36:39,677, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:36:39,851, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:36:42,492, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:36:42,665, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:36:49,906, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:36:49,908, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:36:52,974, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:36:52,977, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:37:29,065, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:37:32,333, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 13:39:38,043, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 13:39:38,046, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 13:39:38,050, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 13:39:38,172, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 13:39:44,922, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 13:39:44,925, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 13:40:08,649, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:36:25,879, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,881, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:25,889, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,891, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:25,893, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,896, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:25,900, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,903, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:25,906, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,909, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:25,912, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,915, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:25,919, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,921, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:25,924, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,927, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:25,930, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:25,932, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:26,094, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:26,098, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:26,103, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:26,106, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:26,110, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:26,113, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:26,116, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:26,120, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:26,123, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:26,127, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:26,130, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:26,133, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:26,139, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:36:26,143, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:36:34,058, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:36:34,120, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:36:34,227, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:36:34,293, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:36:34,438, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:36:34,593, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:36:34,607, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:36:34,761, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:36:34,820, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:36:34,889, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:36:34,979, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:36:35,044, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:36:35,044, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:36:35,208, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:36:35,476, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:36:35,730, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:36:42,722, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:36:42,724, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:36:43,140, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:36:43,141, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:36:43,456, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:36:43,458, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:36:43,485, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:36:43,487, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:36:43,658, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:36:43,660, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:36:43,784, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:36:43,786, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:36:44,000, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:36:44,002, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:36:44,140, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:36:44,142, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:37:23,405, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:37:23,605, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:37:24,285, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:37:24,358, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:37:24,698, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:37:24,789, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:37:25,561, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:37:25,806, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:41:11,822, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:41:12,001, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:41:15,034, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:41:15,222, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:41:18,236, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:41:18,421, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:41:19,896, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:41:20,133, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:41:20,134, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:41:20,338, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:41:23,514, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:41:23,516, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:41:26,772, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:41:26,775, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:41:29,758, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:41:29,760, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:41:31,618, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:41:31,620, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:41:31,932, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:41:31,934, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:42:06,920, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:42:11,052, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:42:13,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:42:14,808, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:42:17,033, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:44:37,418, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:44:37,650, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:44:39,481, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:44:39,664, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:44:48,628, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:44:48,630, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:44:50,634, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:44:50,637, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:45:28,869, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:45:30,188, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-27 14:48:28,824, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-27 14:48:28,826, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-27 14:48:28,831, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-27 14:48:28,960, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-27 14:48:35,699, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-27 14:48:35,702, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-27 14:48:59,373, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 19:19:11,866, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-28 19:19:11,870, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-28 19:19:11,874, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-28 19:19:11,876, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-28 19:19:12,326, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-28 19:19:14,040, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-28 19:19:14,041, data_ingestion, INFO, Initiating train test split ]
[2024-12-28 19:19:15,641, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-28 19:19:15,685, data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-28 19:19:15,687, data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-28 19:19:17,133, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 19:19:17,148, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 19:19:17,149, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-28 19:19:17,150, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-28 19:19:18,438, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-28T19:19:18.438507', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-28 19:19:18,439, data_transformation, INFO, Numerical and text pipelines created ]
[2024-12-28 19:19:18,440, data_transformation, INFO, Fitting and transforming training data ]
[2024-12-28 19:19:18,450, data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 19:19:18,454, data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 19:19:18,456, data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-28 19:19:18,457, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-28 19:19:19,496, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-28T19:19:19.496676', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-28 19:19:19,513, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 19:19:19,625, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 19:19:24,904, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 19:19:24,907, data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 19:19:52,219, data_transformation, INFO, Lemmatization successful ]
[2024-12-28 19:19:52,222, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-28 19:21:32,804, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-28 19:21:32,811, data_transformation, INFO, Transforming test data ]
[2024-12-28 19:21:32,826, data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 19:21:32,874, data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 19:21:35,034, data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 19:21:35,037, data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 19:21:43,264, data_transformation, INFO, Lemmatization successful ]
[2024-12-28 19:21:43,270, data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-28 19:22:23,190, data_transformation, INFO, Successfully generated embeddings ]
[2024-12-28 19:22:23,300, data_transformation, INFO, Transforming test data ]
[2024-12-28 19:22:25,319, data_transformation, INFO, Saved fitted preprocessor to artifacts\preprocessor.joblib ]
[2024-12-28 19:22:25,321, data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-28 19:22:29,120, data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-28 19:30:51,923, data_ingestion, INFO, Initiating data ingestion ]
[2024-12-28 19:30:51,924, data_ingestion, INFO, Establising Connection With SQL Database ]
[2024-12-28 19:30:51,926, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2024-12-28 19:30:51,928, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2024-12-28 19:30:52,334, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2024-12-28 19:30:53,912, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2024-12-28 19:30:53,913, data_ingestion, INFO, Initiating train test split ]
[2024-12-28 19:30:55,398, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2024-12-28 19:30:55,430, text_data_transformation, INFO, Initiating the DataTransformation ]
[2024-12-28 19:30:55,431, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2024-12-28 19:30:56,711, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 19:30:56,714, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 19:30:56,715, text_data_transformation, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2024-12-28 19:30:56,717, keyedvectors, INFO, loading projection weights from ft_reviews_vectors.bin ]
[2024-12-28 19:30:57,800, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-12-28T19:30:57.800874', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2024-12-28 19:30:57,801, text_data_transformation, INFO, Text pipeline created ]
[2024-12-28 19:30:57,802, text_data_transformation, INFO, Fitting and transforming training data ]
[2024-12-28 19:30:57,806, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 19:30:57,922, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 19:31:03,111, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 19:31:03,114, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 19:31:25,953, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 19:31:25,956, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-28 19:33:01,881, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-28 19:33:01,882, text_data_transformation, INFO, Transforming test data ]
[2024-12-28 19:33:01,891, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 19:33:01,943, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 19:33:04,205, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 19:33:04,211, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 19:33:12,363, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 19:33:12,365, text_data_transformation, INFO, Generating embeddings for the text data ]
[2024-12-28 19:33:52,800, text_data_transformation, INFO, Successfully generated embeddings ]
[2024-12-28 19:33:52,801, text_data_transformation, INFO, Transforming test data ]
[2024-12-28 19:33:54,032, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2024-12-28 19:33:54,034, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2024-12-28 19:33:59,019, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2024-12-28 20:12:06,958, data_clustering, INFO, Initialized clustering model ]
[2024-12-28 20:12:06,960, data_clustering, INFO, Initializing the K-Means clustering model ]
[2024-12-28 20:12:06,961, data_clustering, INFO, Fitting the training data to the clustering model ]
[2024-12-28 20:12:07,737, data_clustering, INFO, Fitting the data to the K-Means cluster model ]
[2024-12-28 20:12:09,088, data_clustering, INFO, Transforming the data using the K-Means clustering model ]
[2024-12-28 20:12:10,230, data_clustering, INFO, Successfully assigned clusters using K-Means ]
[2024-12-28 20:12:10,232, data_clustering, INFO, saving fitted cluster model at artifacts\cluster_model.joblib ]
[2024-12-28 20:12:10,235, data_clustering, INFO, Transforming the test data to ]
[2024-12-28 20:12:10,236, data_clustering, INFO, Transforming the data using the K-Means clustering model ]
[2024-12-28 20:12:10,768, data_clustering, INFO, Successfully assigned clusters using K-Means ]
[2024-12-28 20:12:10,770, data_clustering, INFO, Attaching cluster labels to train data and test data ]
[2024-12-28 20:12:12,900, data_clustering, INFO, Saved train data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-28 20:12:13,643, data_clustering, INFO, Saved test data with cluster labels at artifacts\clustered_train_data.csv ]
[2024-12-28 20:12:13,644, data_clustering, INFO,  Returning cluster labes of train data and test resectively ]
[2024-12-28 20:13:15,440, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:13:15,443, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:25:04,909, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:25:04,925, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:25:05,312, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:25:05,313, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:25:11,009, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:26:59,136, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:26:59,139, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:27:03,935, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:27:03,943, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:27:04,325, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:27:04,326, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:27:05,779, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:28:17,773, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:28:17,776, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:28:19,721, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:28:19,730, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:28:20,098, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:28:20,100, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:28:21,360, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:30:05,360, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:30:05,363, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:30:06,304, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:30:06,313, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:30:06,690, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:30:06,692, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:30:08,243, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:33:58,076, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:33:58,078, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:33:59,918, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:33:59,928, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:34:00,326, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:34:00,328, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:34:01,727, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:34:33,131, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:34:33,134, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:34:34,676, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:34:34,685, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:34:35,106, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:34:35,107, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:34:36,462, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:36:20,136, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:36:20,139, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:36:22,026, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:36:22,037, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:36:22,437, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:36:22,439, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:36:23,888, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:36:55,568, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:36:55,571, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:36:56,377, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:36:56,387, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:36:56,748, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:36:56,750, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:36:58,135, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:38:05,924, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:38:05,926, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:38:07,184, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:38:07,192, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:38:07,543, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:38:07,544, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:38:08,947, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:38:44,097, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:38:44,099, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:38:45,335, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:38:45,345, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:38:45,732, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:38:45,734, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:38:47,066, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:39:25,445, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:39:25,447, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:39:26,560, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:39:26,571, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:39:26,934, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:39:26,936, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:39:28,465, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:42:22,388, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:42:22,390, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:42:24,519, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:42:24,527, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:42:24,926, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:42:24,928, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:42:26,514, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:43:08,032, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:43:08,034, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:43:09,303, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:43:09,311, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:43:09,713, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:43:09,715, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:43:11,092, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:43:45,373, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:43:45,376, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:43:46,526, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:43:46,535, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:43:46,883, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:43:46,884, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:43:48,377, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:44:32,524, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:44:32,527, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:44:33,962, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:44:33,971, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:44:34,376, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:44:34,377, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:44:35,883, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:45:39,343, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:45:39,345, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:45:41,539, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:45:41,548, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:45:42,004, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:45:42,006, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:45:43,505, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:46:37,616, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:46:37,619, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:46:38,299, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:46:38,309, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:46:38,754, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:46:38,755, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:46:40,320, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:47:01,085, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:47:01,088, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:47:01,788, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:47:01,797, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:47:02,205, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:47:02,206, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:47:03,598, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:52:40,795, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:52:40,797, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:52:40,802, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:52:40,805, text_data_transformation, INFO, Error in preprocessing the text: 'int' object has no attribute 'lower' ]
[2024-12-28 20:54:14,145, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:54:14,147, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:54:14,150, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:14,158, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:14,408, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:14,410, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:15,420, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:20,528, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:20,531, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:20,649, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:20,650, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:21,036, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:21,330, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:54:21,332, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:54:21,335, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:21,340, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:21,581, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:21,582, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:22,417, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:27,367, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:27,371, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:27,481, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:27,482, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:27,904, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:28,177, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:54:28,179, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:54:28,181, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:28,186, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:28,426, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:28,427, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:29,285, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:34,503, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:34,509, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:34,664, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:34,666, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:35,183, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:35,507, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:54:35,509, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:54:35,512, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:35,519, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:35,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:35,835, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:36,804, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:41,640, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:41,644, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:41,754, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:41,757, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:42,185, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:42,502, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:54:42,504, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:54:42,508, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:42,516, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:42,799, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:42,801, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:43,711, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:48,342, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:48,345, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:48,452, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:48,454, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:48,840, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:49,091, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:54:49,093, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:54:49,095, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:49,101, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:49,334, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:49,335, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:50,145, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:55,434, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:55,438, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:55,550, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:55,551, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:56,001, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:54:56,300, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:54:56,302, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:54:56,305, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:54:56,313, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:54:56,547, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:54:56,548, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:54:57,628, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:55:02,401, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:55:02,404, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:55:02,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:55:02,513, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:55:02,915, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:55:03,184, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:55:03,187, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:55:03,190, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:55:03,195, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:55:03,428, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:55:03,430, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:55:04,434, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:55:09,589, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:55:09,592, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:55:09,703, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:55:09,705, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:55:10,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:55:10,357, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:55:10,359, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:55:10,362, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:55:10,369, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:55:10,616, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:55:10,619, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:55:11,511, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:55:16,077, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:55:16,078, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:55:16,186, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:55:16,188, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:55:16,634, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:59:43,506, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:59:43,508, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:59:43,509, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:59:43,518, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:59:43,796, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:59:43,797, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:59:44,870, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:59:48,368, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:59:48,372, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:59:48,480, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:59:48,481, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:59:48,864, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:59:49,143, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:59:49,144, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:59:49,147, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:59:49,153, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:59:49,390, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:59:49,391, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:59:50,270, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:59:53,743, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:59:53,747, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:59:53,856, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:59:53,857, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:59:54,241, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:59:54,530, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 20:59:54,533, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 20:59:54,535, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:59:54,540, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:59:54,773, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:59:54,774, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:59:55,774, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 20:59:59,444, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 20:59:59,447, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 20:59:59,555, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 20:59:59,557, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 20:59:59,945, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:00,212, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:00,215, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:00,217, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:00,222, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:00,457, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:00,458, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:01,296, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:04,816, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:04,819, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:04,941, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:04,943, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:05,328, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:05,621, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:05,623, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:05,626, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:05,634, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:05,930, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:05,931, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:06,899, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:10,739, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:10,742, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:10,867, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:10,869, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:11,290, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:11,569, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:11,571, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:11,573, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:11,580, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:11,845, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:11,847, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:12,830, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:16,998, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:17,003, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:17,188, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:17,190, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:17,646, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:17,955, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:17,956, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:17,959, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:17,968, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:18,207, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:18,209, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:19,276, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:25,107, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:25,112, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:25,256, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:25,257, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:25,659, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:26,010, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:26,012, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:26,015, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:26,022, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:26,271, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:26,272, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:27,116, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:31,211, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:31,214, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:31,326, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:31,327, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:31,787, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:32,093, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:32,094, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:32,097, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:32,102, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:32,361, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:32,361, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:33,303, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:38,352, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:38,355, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:38,474, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:38,475, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:38,903, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:39,162, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:39,164, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:39,166, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:39,172, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:39,478, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:39,479, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:40,394, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:45,732, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:45,736, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:45,850, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:45,851, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:46,234, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:46,499, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:46,501, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:46,503, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:46,509, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:46,737, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:46,738, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:47,641, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:52,880, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:52,885, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:53,031, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:53,032, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:53,468, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:00:53,818, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:00:53,820, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:00:53,823, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:00:53,830, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:00:54,105, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:00:54,106, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:00:55,030, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:00,610, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:00,613, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:00,735, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:00,736, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:01,144, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:01,454, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:01,455, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:01,458, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:01,463, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:01,712, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:01,714, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:02,610, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:07,447, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:07,450, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:07,570, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:07,572, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:07,998, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:08,251, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:08,253, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:08,256, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:08,261, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:08,507, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:08,508, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:09,423, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:13,991, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:13,993, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:14,106, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:14,108, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:14,538, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:14,797, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:14,799, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:14,801, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:14,808, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:15,058, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:15,059, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:15,983, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:20,759, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:20,763, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:20,873, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:20,874, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:21,264, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:21,516, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:21,518, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:21,520, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:21,526, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:21,780, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:21,781, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:22,786, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:27,858, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:27,861, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:28,005, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:28,007, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:28,588, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:28,875, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:28,877, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:28,879, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:28,885, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:29,134, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:29,136, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:30,070, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:34,785, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:34,789, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:34,934, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:34,936, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:35,515, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:35,773, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:35,775, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:35,776, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:35,782, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:36,037, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:36,038, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:37,295, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:42,132, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:42,135, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:42,247, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:42,249, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:42,636, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:42,874, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:42,876, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:42,878, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:42,884, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:43,172, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:43,174, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:44,267, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:48,473, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:48,477, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:48,591, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:48,592, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:49,007, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:49,243, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:49,244, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:49,247, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:49,254, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:49,489, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:49,491, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:50,427, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:54,842, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:54,846, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:54,970, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:54,972, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:55,427, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:01:55,692, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:01:55,694, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:01:55,696, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:01:55,702, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:01:55,951, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:01:55,952, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:01:56,844, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:00,490, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:00,493, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:00,604, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:00,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:00,995, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:01,296, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:01,298, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:01,301, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:01,308, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:01,578, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:01,580, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:02,445, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:06,354, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:06,358, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:06,468, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:06,469, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:06,928, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:07,224, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:07,226, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:07,228, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:07,237, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:07,502, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:07,503, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:08,392, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:12,098, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:12,101, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:12,234, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:12,236, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:12,633, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:12,943, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:12,946, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:12,948, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:12,953, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:13,202, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:13,203, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:14,153, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:17,822, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:17,825, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:17,936, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:17,938, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:18,334, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:18,624, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:18,625, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:18,627, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:18,634, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:18,890, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:18,891, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:19,782, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:24,521, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:24,524, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:24,641, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:24,642, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:25,053, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:25,341, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:25,343, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:25,346, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:25,352, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:25,608, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:25,609, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:26,468, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:30,777, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:30,781, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:30,900, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:30,901, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:31,311, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:31,617, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:31,618, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:31,621, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:31,627, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:31,872, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:31,874, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:32,759, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:37,159, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:37,162, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:37,278, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:37,279, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:37,682, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:37,977, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:37,979, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:37,981, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:37,986, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:38,223, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:38,225, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:39,103, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:43,140, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:43,144, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:43,263, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:43,265, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:43,705, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:44,038, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:44,040, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:44,043, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:44,049, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:44,302, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:44,304, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:45,202, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:50,425, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:50,428, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:50,554, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:50,556, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:50,994, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:51,265, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:51,267, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:51,269, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:51,275, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:51,583, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:51,585, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:52,539, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:58,629, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:58,632, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:58,749, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:58,750, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:02:59,156, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:02:59,474, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:02:59,476, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:02:59,479, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:02:59,484, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:02:59,761, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:02:59,762, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:00,603, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:06,388, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:06,391, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:06,498, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:06,500, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:06,920, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:07,232, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:03:07,234, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:03:07,236, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:07,242, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:07,487, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:07,489, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:08,459, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:14,478, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:14,481, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:14,589, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:14,591, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:14,983, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:15,306, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:03:15,309, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:03:15,312, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:15,316, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:15,579, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:15,580, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:16,681, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:21,919, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:21,924, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:22,040, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:22,042, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:22,468, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:22,722, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:03:22,723, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:03:22,725, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:22,731, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:22,969, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:22,971, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:23,896, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:28,617, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:28,620, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:28,730, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:28,731, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:29,135, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:29,376, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:03:29,379, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:03:29,381, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:29,385, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:29,648, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:29,649, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:30,576, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:35,283, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:35,287, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:35,409, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:35,410, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:35,803, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:36,058, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:03:36,059, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:03:36,061, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:36,066, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:36,319, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:36,321, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:37,248, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:42,205, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:42,208, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:42,321, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:42,322, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:42,706, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:42,972, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:03:42,975, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:03:42,976, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:42,982, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:43,222, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:43,224, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:44,106, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:47,949, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:47,954, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:48,075, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:48,076, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:48,517, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:48,761, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:03:48,763, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:03:48,766, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:48,772, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:49,012, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:49,013, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:50,219, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:57,363, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:57,367, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:57,474, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:57,475, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:57,939, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:03:58,206, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:03:58,208, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:03:58,210, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:03:58,215, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:03:58,478, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:03:58,479, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:03:59,491, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:04,114, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:04,120, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:04,237, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:04,238, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:04,834, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:05,142, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:05,144, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:05,147, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:05,156, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:05,439, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:05,440, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:06,522, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:11,093, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:11,098, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:11,207, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:11,209, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:11,603, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:11,882, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:11,884, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:11,886, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:11,891, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:12,176, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:12,177, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:13,110, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:17,030, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:17,034, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:17,146, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:17,148, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:17,557, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:17,840, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:17,842, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:17,844, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:17,850, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:18,101, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:18,102, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:19,012, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:22,829, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:22,833, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:22,954, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:22,955, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:24,430, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:25,875, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:25,882, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:25,890, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:25,915, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:26,382, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:26,384, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:27,289, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:31,100, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:31,103, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:31,215, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:31,217, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:31,605, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:31,914, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:31,916, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:31,918, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:31,923, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:32,162, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:32,163, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:33,011, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:36,633, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:36,636, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:36,738, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:36,739, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:37,162, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:37,444, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:37,446, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:37,449, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:37,454, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:37,693, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:37,695, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:38,660, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:42,848, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:42,851, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:42,958, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:42,958, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:43,338, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:43,615, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:43,617, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:43,620, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:43,626, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:43,876, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:43,877, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:44,722, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:48,732, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:48,735, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:48,843, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:48,845, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:49,218, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:49,499, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:49,501, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:49,503, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:49,508, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:49,759, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:49,761, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:50,764, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:54,854, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:54,857, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:54,964, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:54,965, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:55,340, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:04:55,632, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:04:55,634, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:04:55,636, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:04:55,643, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:04:55,891, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:04:55,893, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:04:56,769, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:00,928, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:00,931, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:01,048, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:01,049, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:01,436, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:01,735, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:01,737, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:01,739, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:01,745, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:01,987, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:01,988, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:02,814, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:07,767, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:07,771, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:07,875, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:07,876, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:08,267, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:08,522, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:08,524, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:08,527, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:08,533, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:08,771, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:08,772, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:09,671, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:14,776, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:14,780, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:14,884, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:14,886, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:15,264, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:15,524, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:15,526, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:15,529, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:15,534, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:15,772, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:15,774, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:16,642, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:22,012, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:22,015, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:22,121, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:22,122, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:22,536, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:22,841, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:22,842, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:22,844, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:22,849, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:23,083, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:23,084, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:23,904, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:29,640, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:29,644, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:29,747, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:29,748, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:30,179, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:30,487, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:30,489, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:30,492, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:30,497, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:30,746, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:30,747, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:31,716, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:35,922, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:35,925, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:36,032, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:36,033, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:36,440, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:36,671, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:36,672, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:36,674, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:36,680, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:36,933, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:36,934, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:37,769, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:41,986, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:41,989, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:42,097, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:42,098, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:42,487, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:42,722, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:42,724, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:42,727, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:42,732, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:42,967, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:42,969, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:43,797, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:48,161, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:48,164, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:48,270, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:48,271, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:48,652, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:48,886, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:48,888, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:48,891, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:48,896, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:49,133, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:49,134, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:49,948, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:54,862, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:54,866, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:54,971, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:54,973, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:55,349, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:05:55,599, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:05:55,602, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:05:55,604, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:05:55,610, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:05:55,861, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:05:55,863, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:05:56,721, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:00,388, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:00,391, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:00,516, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:00,518, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:00,939, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:01,166, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:01,169, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:01,171, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:01,177, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:01,412, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:01,413, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:02,326, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:06,029, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:06,032, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:06,139, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:06,140, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:06,534, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:06,774, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:06,776, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:06,778, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:06,784, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:07,020, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:07,021, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:07,893, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:11,716, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:11,719, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:11,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:11,834, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:12,246, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:12,480, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:12,482, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:12,484, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:12,490, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:12,722, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:12,723, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:13,635, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:17,783, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:17,787, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:17,897, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:17,898, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:18,294, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:18,559, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:18,561, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:18,564, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:18,570, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:18,805, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:18,806, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:19,780, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:23,438, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:23,442, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:23,547, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:23,549, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:23,921, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:24,190, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:24,192, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:24,194, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:24,200, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:24,432, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:24,433, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:25,309, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:29,065, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:29,068, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:29,180, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:29,182, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:29,547, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:29,827, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:29,829, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:29,832, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:29,836, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:30,070, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:30,071, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:30,928, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:34,454, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:34,457, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:34,563, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:34,564, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:34,932, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:35,222, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:35,224, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:35,226, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:35,232, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:35,465, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:35,467, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:36,298, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:39,968, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:39,971, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:40,082, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:40,084, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:40,499, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:40,787, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:40,789, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:40,791, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:40,798, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:41,035, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:41,036, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:41,866, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:45,815, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:45,818, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:45,924, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:45,925, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:46,299, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:46,583, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:46,584, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:46,586, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:46,592, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:46,835, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:46,837, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:47,676, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:51,812, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:51,815, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:51,921, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:51,922, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:52,292, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:52,578, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:52,580, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:52,583, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:52,589, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:52,819, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:52,821, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:53,794, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:57,904, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:57,907, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:58,012, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:58,013, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:58,406, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:06:58,699, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:06:58,700, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:06:58,702, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:06:58,707, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:06:58,938, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:06:58,940, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:06:59,783, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:03,951, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:03,954, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:04,057, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:04,059, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:04,436, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:04,772, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:04,774, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:04,777, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:04,783, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:05,032, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:05,033, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:05,915, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:10,700, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:10,703, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:10,810, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:10,812, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:11,257, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:11,516, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:11,518, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:11,520, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:11,527, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:11,766, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:11,767, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:12,607, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:17,760, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:17,763, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:17,873, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:17,875, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:18,249, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:18,528, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:18,530, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:18,533, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:18,538, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:18,766, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:18,767, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:19,678, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:25,034, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:25,038, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:25,145, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:25,146, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:25,520, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:25,809, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:25,811, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:25,813, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:25,819, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:26,069, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:26,071, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:26,944, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:32,460, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:32,464, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:32,571, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:32,572, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:32,942, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:33,240, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:33,241, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:33,243, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:33,248, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:33,483, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:33,484, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:34,331, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:38,397, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:38,399, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:38,507, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:38,508, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:38,909, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:39,130, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:39,132, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:39,134, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:39,140, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:39,368, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:39,370, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:40,432, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:44,615, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:44,618, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:44,722, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:44,723, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:45,099, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:45,326, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:45,328, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:45,330, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:45,335, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:45,572, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:45,573, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:46,623, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:50,792, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:50,795, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:50,899, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:50,900, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:51,278, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:51,515, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:51,517, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:51,520, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:51,525, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:51,757, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:51,758, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:52,608, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:57,061, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:57,064, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:57,178, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:57,179, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:57,549, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:07:57,790, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:07:57,792, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:07:57,795, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:07:57,800, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:07:58,043, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:07:58,045, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:07:58,948, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:02,488, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:02,492, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:02,598, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:02,599, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:02,969, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:03,190, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:03,192, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:03,194, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:03,200, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:03,445, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:03,446, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:04,329, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:08,052, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:08,055, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:08,163, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:08,165, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:08,546, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:08,773, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:08,775, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:08,777, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:08,783, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:09,026, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:09,027, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:09,896, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:16,214, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:16,218, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:16,388, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:16,390, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:16,990, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:17,356, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:17,358, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:17,361, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:17,371, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:17,700, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:17,702, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:19,073, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:24,521, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:24,524, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:24,649, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:24,651, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:25,103, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:25,395, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:25,397, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:25,400, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:25,406, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:25,682, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:25,683, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:26,710, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:32,948, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:32,951, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:33,101, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:33,102, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:33,578, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:33,933, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:33,934, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:33,937, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:33,943, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:34,192, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:34,193, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:35,147, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:39,245, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:39,248, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:39,375, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:39,376, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:39,808, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:40,133, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:40,135, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:40,137, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:40,143, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:40,384, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:40,385, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:41,409, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:45,449, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:45,453, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:45,576, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:45,578, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:46,013, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:46,329, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:46,331, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:46,333, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:46,338, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:46,584, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:46,585, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:47,579, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:51,610, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:51,613, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:51,725, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:51,727, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:52,156, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:52,483, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:52,484, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:52,487, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:52,492, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:52,755, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:52,756, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:53,716, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:58,344, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:58,347, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:58,460, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:58,461, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:08:58,906, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:08:59,205, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:08:59,207, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:08:59,209, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:08:59,215, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:08:59,518, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:08:59,519, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:00,494, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:04,889, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:04,894, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:05,030, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:05,031, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:05,441, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:05,751, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:09:05,755, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:09:05,757, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:05,764, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:06,099, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:06,100, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:07,086, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:13,817, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:13,820, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:13,933, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:13,934, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:14,386, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:14,734, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:09:14,736, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:09:14,740, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:14,747, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:15,000, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:15,001, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:15,985, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:20,504, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:20,507, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:20,618, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:20,619, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:21,077, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:21,409, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:09:21,411, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:09:21,414, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:21,423, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:21,672, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:21,673, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:22,562, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:27,322, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:27,325, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:27,431, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:27,432, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:27,825, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:28,130, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:09:28,132, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:09:28,134, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:28,139, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:28,383, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:28,384, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:29,251, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:34,219, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:34,222, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:34,331, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:34,332, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:34,731, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:34,997, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:09:34,998, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:09:35,001, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:35,007, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:35,257, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:35,258, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:36,134, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:41,507, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:41,510, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:41,619, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:41,620, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:42,044, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:42,318, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:09:42,320, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:09:42,322, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:42,327, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:42,568, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:42,569, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:43,421, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:49,177, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:49,180, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:49,284, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:49,286, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:49,674, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:49,969, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:09:49,971, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:09:49,973, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:49,979, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:50,219, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:50,221, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:51,151, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:57,219, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:57,225, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:57,410, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:57,412, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:09:57,912, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:09:58,705, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:09:58,711, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:09:58,718, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:09:58,743, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:09:59,764, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:09:59,768, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:01,892, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:08,282, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:08,285, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:08,396, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:08,397, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:08,793, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:09,026, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:09,028, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:09,031, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:09,036, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:09,275, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:09,276, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:10,157, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:14,301, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:14,304, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:14,408, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:14,410, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:14,800, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:15,378, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:15,384, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:15,390, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:15,411, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:16,431, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:16,435, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:18,516, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:22,898, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:22,901, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:23,025, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:23,027, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:23,413, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:23,662, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:23,663, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:23,666, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:23,672, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:23,908, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:23,909, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:24,868, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:28,330, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:28,333, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:28,445, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:28,446, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:28,839, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:29,067, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:29,069, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:29,072, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:29,077, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:29,326, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:29,329, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:30,227, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:33,621, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:33,625, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:33,748, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:33,750, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:34,212, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:34,449, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:34,451, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:34,454, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:34,459, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:34,692, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:34,693, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:35,591, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:39,176, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:39,179, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:39,287, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:39,288, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:39,662, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:39,911, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:39,913, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:39,916, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:39,922, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:40,153, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:40,154, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:40,981, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:44,665, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:44,668, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:44,774, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:44,775, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:45,208, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:45,456, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:45,458, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:45,461, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:45,466, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:45,699, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:45,700, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:46,602, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:50,461, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:50,464, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:50,573, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:50,574, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:50,967, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:51,231, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:51,233, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:51,236, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:51,242, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:51,478, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:51,479, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:52,337, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:55,963, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:55,966, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:56,081, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:56,082, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:56,486, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:10:56,770, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:10:56,772, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:10:56,774, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:10:56,779, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:10:57,025, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:10:57,027, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:10:57,865, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:01,435, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:01,439, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:01,548, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:01,550, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:01,937, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:02,217, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:02,219, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:02,221, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:02,227, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:02,462, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:02,463, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:03,325, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:07,295, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:07,298, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:07,408, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:07,409, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:07,815, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:08,106, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:08,107, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:08,110, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:08,115, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:08,391, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:08,392, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:09,245, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:13,266, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:13,269, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:13,390, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:13,391, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:13,768, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:14,043, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:14,045, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:14,049, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:14,059, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:14,317, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:14,319, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:15,228, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:19,424, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:19,427, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:19,534, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:19,536, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:19,914, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:20,192, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:20,194, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:20,196, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:20,201, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:20,432, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:20,433, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:21,271, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:25,493, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:25,496, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:25,607, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:25,608, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:26,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:26,401, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:26,403, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:26,405, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:26,411, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:26,657, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:26,658, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:27,535, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:31,703, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:31,706, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:31,832, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:31,834, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:32,215, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:32,526, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:32,527, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:32,529, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:32,534, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:32,770, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:32,771, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:33,607, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:38,364, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:38,367, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:38,475, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:38,476, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:38,857, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:39,111, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:39,113, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:39,116, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:39,121, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:39,356, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:39,357, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:40,282, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:44,931, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:44,934, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:45,038, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:45,040, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:45,409, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:45,665, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:45,666, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:45,669, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:45,674, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:45,907, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:45,908, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:46,771, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:52,033, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:52,036, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:52,140, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:52,142, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:52,517, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:52,796, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:11:52,798, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:11:52,801, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:52,807, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:53,061, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:53,062, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:53,937, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:11:59,467, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:11:59,470, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:11:59,576, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:11:59,577, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:11:59,961, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:00,250, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:00,252, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:00,254, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:00,260, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:00,494, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:00,495, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:01,316, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:04,953, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:04,956, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:05,082, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:05,083, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:05,530, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:05,805, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:05,808, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:05,810, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:05,817, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:06,145, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:06,146, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:07,057, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:10,695, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:10,698, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:10,812, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:10,814, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:11,279, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:11,504, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:11,506, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:11,508, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:11,513, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:11,750, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:11,751, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:12,573, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:16,801, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:16,804, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:16,912, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:16,914, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:17,293, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:17,524, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:17,525, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:17,528, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:17,534, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:17,768, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:17,769, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:18,600, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:22,803, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:22,806, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:22,912, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:22,913, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:23,317, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:23,558, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:23,560, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:23,563, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:23,569, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:23,801, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:23,802, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:24,676, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:28,042, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:28,045, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:28,151, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:28,153, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:28,525, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:28,749, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:28,750, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:28,753, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:28,759, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:29,046, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:29,048, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:29,924, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:33,205, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:33,209, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:33,318, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:33,319, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:33,712, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:33,939, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:33,941, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:33,944, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:33,948, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:34,191, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:34,193, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:35,029, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:38,644, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:38,647, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:38,754, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:38,756, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:39,136, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:39,376, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:39,378, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:39,380, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:39,385, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:39,611, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:39,612, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:40,489, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:44,086, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:44,089, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:44,197, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:44,198, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:44,578, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:44,825, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:44,827, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:44,830, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:44,834, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:45,080, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:45,081, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:45,960, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:49,702, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:49,705, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:49,810, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:49,812, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:50,203, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:50,472, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:50,473, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:50,476, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:50,481, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:50,711, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:50,713, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:51,549, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:55,285, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:55,288, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:55,399, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:55,401, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:55,896, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:12:56,183, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:12:56,184, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:12:56,187, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:12:56,193, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:12:56,435, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:12:56,436, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:12:57,272, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:00,835, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:00,838, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:00,946, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:00,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:01,333, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:01,611, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:01,613, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:01,616, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:01,621, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:01,867, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:01,868, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:02,717, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:06,446, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:06,449, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:06,561, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:06,563, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:06,955, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:07,240, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:07,242, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:07,244, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:07,249, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:07,490, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:07,491, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:08,364, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:12,336, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:12,340, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:12,457, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:12,459, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:12,837, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:13,108, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:13,109, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:13,111, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:13,116, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:13,360, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:13,362, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:14,203, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:18,337, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:18,340, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:18,464, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:18,465, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:18,855, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:19,129, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:19,131, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:19,134, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:19,140, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:19,379, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:19,381, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:20,250, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:24,341, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:24,345, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:24,451, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:24,453, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:24,874, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:25,176, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:25,177, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:25,180, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:25,185, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:25,424, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:25,425, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:26,264, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:30,369, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:30,372, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:30,477, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:30,478, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:30,876, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:31,186, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:31,188, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:31,190, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:31,195, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:31,423, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:31,424, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:32,226, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:36,759, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:36,763, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:36,874, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:36,876, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:37,244, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:37,487, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:37,489, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:37,491, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:37,497, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:37,741, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:37,742, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:38,614, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:44,512, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:44,515, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:44,620, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:44,621, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:45,000, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:45,247, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:45,249, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:45,252, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:45,257, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:45,493, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:45,494, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:46,378, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:51,471, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:51,474, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:51,579, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:51,580, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:51,955, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:52,247, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:52,249, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:52,252, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:52,258, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:52,498, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:52,499, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:53,400, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:58,749, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:58,753, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:58,859, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:58,861, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:13:59,234, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:13:59,506, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:13:59,508, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:13:59,511, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:13:59,516, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:13:59,750, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:13:59,751, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:00,606, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:04,389, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:04,392, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:04,516, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:04,518, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:04,985, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:05,222, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:05,223, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:05,225, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:05,230, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:05,465, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:05,466, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:06,369, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:10,068, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:10,072, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:10,179, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:10,180, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:10,558, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:10,781, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:10,783, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:10,785, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:10,791, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:11,023, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:11,024, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:11,991, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:15,879, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:15,883, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:15,992, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:15,993, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:16,380, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:16,621, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:16,623, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:16,626, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:16,631, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:16,864, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:16,866, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:17,768, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:21,948, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:21,951, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:22,059, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:22,061, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:22,448, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:22,703, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:22,705, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:22,707, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:22,713, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:22,951, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:22,953, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:23,901, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:27,121, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:27,124, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:27,240, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:27,242, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:27,648, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:27,881, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:27,884, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:27,886, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:27,891, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:28,119, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:28,120, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:28,971, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:32,406, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:32,409, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:32,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:32,514, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:32,916, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:33,139, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:33,141, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:33,144, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:33,150, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:33,392, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:33,393, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:34,244, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:37,571, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:37,574, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:37,681, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:37,682, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:38,142, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:38,387, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:38,390, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:38,392, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:38,398, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:38,663, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:38,664, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:39,508, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:43,052, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:43,055, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:43,160, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:43,162, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:43,555, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:43,799, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:43,801, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:43,804, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:43,809, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:44,042, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:44,043, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:44,880, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:48,676, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:48,679, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:48,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:48,783, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:49,164, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:49,422, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:49,424, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:49,426, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:49,432, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:49,663, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:49,664, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:50,521, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:54,168, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:54,171, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:54,279, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:54,280, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:54,746, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:14:55,936, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:14:55,941, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:14:55,949, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:14:55,970, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:14:56,831, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:14:56,833, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:14:57,667, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:01,313, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:01,315, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:01,423, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:01,424, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:01,812, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:02,089, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:02,090, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:02,093, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:02,099, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:02,333, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:02,334, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:03,184, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:06,988, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:06,991, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:07,097, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:07,098, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:07,472, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:07,752, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:07,754, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:07,757, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:07,762, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:08,025, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:08,027, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:10,505, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:14,374, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:14,377, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:14,484, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:14,486, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:14,861, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:15,123, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:15,125, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:15,127, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:15,132, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:15,367, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:15,368, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:16,204, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:20,308, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:20,311, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:20,419, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:20,420, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:20,803, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:21,081, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:21,083, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:21,085, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:21,090, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:21,326, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:21,327, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:22,173, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:26,313, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:26,316, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:26,422, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:26,423, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:26,853, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:27,150, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:27,151, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:27,153, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:27,159, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:27,399, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:27,401, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:28,323, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:32,511, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:32,514, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:32,621, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:32,623, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:32,991, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:33,278, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:33,280, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:33,282, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:33,287, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:33,549, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:33,550, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:34,395, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:38,874, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:38,877, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:38,982, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:38,983, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:39,373, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:39,606, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:39,608, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:39,610, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:39,615, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:39,842, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:39,843, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:40,725, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:45,250, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:45,254, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:45,359, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:45,361, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:45,729, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:45,974, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:45,976, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:45,978, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:45,984, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:46,207, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:46,208, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:47,047, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:51,883, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:51,886, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:51,996, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:51,998, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:52,415, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:15:52,677, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:15:52,679, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:15:52,681, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:15:52,688, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:15:52,923, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:15:52,924, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:15:53,809, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:00,803, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:00,806, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:00,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:00,949, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:01,333, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:01,604, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:01,606, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:01,610, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:01,615, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:01,844, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:01,845, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:02,654, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:06,179, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:06,182, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:06,300, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:06,301, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:06,690, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:06,909, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:06,911, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:06,913, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:06,918, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:07,156, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:07,157, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:07,991, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:11,587, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:11,590, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:11,691, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:11,692, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:12,058, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:12,291, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:12,293, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:12,295, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:12,300, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:12,537, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:12,539, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:13,362, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:17,220, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:17,223, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:17,324, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:17,325, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:17,709, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:17,937, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:17,939, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:17,942, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:17,947, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:18,175, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:18,177, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:19,015, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:22,995, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:22,998, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:23,097, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:23,098, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:23,455, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:23,711, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:23,712, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:23,716, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:23,721, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:23,956, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:23,957, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:24,776, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:28,083, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:28,086, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:28,195, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:28,196, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:28,599, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:28,817, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:28,819, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:28,822, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:28,827, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:29,061, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:29,061, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:31,504, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:34,705, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:34,709, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:34,816, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:34,816, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:35,183, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:35,407, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:35,409, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:35,412, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:35,417, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:35,648, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:35,650, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:36,535, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:39,856, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:39,859, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:39,967, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:39,968, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:40,349, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:40,584, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:40,585, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:40,587, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:40,593, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:40,823, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:40,824, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:41,668, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:45,148, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:45,151, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:45,259, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:45,261, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:45,638, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:45,889, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:45,891, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:45,893, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:45,899, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:46,124, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:46,125, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:47,008, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:50,718, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:50,721, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:50,829, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:50,830, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:51,207, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:51,468, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:51,470, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:51,472, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:51,477, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:51,713, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:51,714, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:52,558, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:56,313, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:56,316, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:56,422, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:56,424, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:56,836, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:16:57,102, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:16:57,104, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:16:57,106, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:16:57,111, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:16:57,343, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:16:57,344, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:16:58,185, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:01,873, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:01,876, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:01,984, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:01,985, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:02,376, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:02,668, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:02,670, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:02,672, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:02,677, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:02,902, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:02,904, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:03,741, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:07,386, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:07,390, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:07,493, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:07,494, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:07,878, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:08,168, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:08,170, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:08,173, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:08,178, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:08,416, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:08,417, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:09,434, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:13,413, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:13,417, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:13,531, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:13,532, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:13,902, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:14,168, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:14,170, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:14,172, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:14,177, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:14,408, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:14,409, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:15,386, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:19,784, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:19,787, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:19,890, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:19,893, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:20,271, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:20,536, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:20,538, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:20,540, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:20,546, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:20,800, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:20,802, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:21,681, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:25,947, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:25,950, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:26,056, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:26,057, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:26,444, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:26,774, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:26,776, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:26,778, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:26,784, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:27,058, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:27,059, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:27,973, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:32,931, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:32,934, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:33,053, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:33,054, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:33,442, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:33,774, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:33,775, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:33,778, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:33,783, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:34,024, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:34,025, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:34,985, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:39,576, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:39,580, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:39,690, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:39,691, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:40,095, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:40,331, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:40,332, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:40,334, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:40,340, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:40,582, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:40,583, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:41,407, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:46,089, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:46,093, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:46,212, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:46,214, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:46,606, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:47,202, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:47,208, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:47,216, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:47,236, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:48,312, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:48,317, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:49,690, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:54,567, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:54,570, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:54,677, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:54,678, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:55,118, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:17:55,370, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:17:55,371, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:17:55,374, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:17:55,381, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:17:55,621, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:17:55,623, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:17:56,472, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:01,884, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:01,887, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:01,996, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:01,997, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:02,377, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:02,662, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:18:02,663, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:18:02,665, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:02,670, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:02,908, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:02,909, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:03,842, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:07,250, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:07,254, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:07,365, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:07,367, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:07,766, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:07,992, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:18:07,993, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:18:07,996, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:08,002, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:08,269, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:08,270, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:09,132, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:12,800, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:12,804, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:12,916, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:12,917, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:13,310, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:13,546, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:18:13,547, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:18:13,549, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:13,554, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:13,794, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:13,795, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:14,631, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:18,511, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:18,514, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:18,624, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:18,625, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:19,046, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:19,277, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:18:19,278, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:18:19,280, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:19,285, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:19,554, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:19,556, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:20,511, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:24,519, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:24,523, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:24,631, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:24,632, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:25,017, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:25,266, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:18:25,268, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:18:25,270, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:25,276, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:25,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:25,513, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:26,338, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:29,786, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:29,789, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:29,892, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:29,894, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:30,285, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:30,504, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:18:30,506, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:18:30,508, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:30,513, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:30,752, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:30,753, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:31,759, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:34,925, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:34,929, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:35,041, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:35,043, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:35,410, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:35,640, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:18:35,641, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:18:35,643, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:35,648, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:35,893, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:35,894, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:36,699, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:40,329, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:40,332, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:40,442, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:40,444, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:40,808, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:41,058, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 21:18:41,060, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 21:18:41,062, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:41,069, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:41,340, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:41,341, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:42,197, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 21:18:45,529, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 21:18:45,532, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 21:18:45,637, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 21:18:45,638, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 21:18:47,028, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:04:53,811, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,813, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,817, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,820, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,823, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,824, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,865, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,867, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,871, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,872, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,876, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,878, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,882, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,883, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,885, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,887, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,890, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,891, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,894, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,896, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,900, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,901, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,903, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,905, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,907, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,908, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,910, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,911, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,915, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,917, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,919, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,920, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,923, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,925, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,926, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,928, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,931, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,933, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,934, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,936, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,939, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,940, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,942, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,944, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,946, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,948, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,951, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,953, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,955, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,956, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,958, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,960, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,963, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,964, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,967, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,969, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,971, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,972, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,974, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,976, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,978, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,979, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,981, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,983, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,986, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,987, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,989, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,990, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:53,993, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:53,994, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:53,996, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:53,998, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,001, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,002, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,004, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,005, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,008, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,010, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,011, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,013, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,016, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,017, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,020, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,022, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,025, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,026, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,027, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,029, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,032, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,034, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,035, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,037, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,039, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,040, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,041, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,043, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,045, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,047, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,049, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,050, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,053, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,055, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,057, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,059, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,062, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,063, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,065, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,067, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,069, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,071, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,072, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,074, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,077, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,078, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,079, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,082, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,085, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,086, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:04:54,088, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:04:54,089, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:04:54,091, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:04:54,093, text_data_transformation, INFO, Error in preprocessing the text: 'list' object has no attribute 'map' ]
[2024-12-28 22:05:17,738, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:05:17,741, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:05:17,747, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:05:17,749, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:05:17,760, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:05:17,892, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:05:22,436, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:05:22,439, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:05:37,359, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:07:18,890, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:07:18,891, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:07:18,926, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:07:19,000, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:07:23,338, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:07:23,340, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:07:38,520, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:07:50,995, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:07:50,997, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:07:51,001, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:07:51,003, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:07:51,007, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:07:51,012, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:07:51,315, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:07:51,317, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:07:52,402, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:07:57,011, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:07:57,013, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:07:57,018, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:07:57,024, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:07:57,265, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:07:57,267, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:07:58,189, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:03,265, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:03,267, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:03,273, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:03,279, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:03,558, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:03,560, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:04,636, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:09,465, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:09,467, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:09,471, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:09,480, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:09,776, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:09,778, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:10,763, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:15,577, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:15,579, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:15,585, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:15,592, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:15,909, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:15,911, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:17,049, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:22,133, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:22,135, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:22,141, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:22,147, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:22,467, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:22,468, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:23,433, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:27,978, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:27,979, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:27,984, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:27,991, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:28,278, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:28,279, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:29,356, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:34,407, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:34,409, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:34,414, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:34,420, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:34,668, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:34,670, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:35,784, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:40,833, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:40,836, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:40,841, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:40,847, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:41,155, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:41,156, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:42,131, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:46,606, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:46,608, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:46,613, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:46,620, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:46,903, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:46,904, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:47,870, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:51,746, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:51,748, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:51,756, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:51,764, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:52,016, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:52,017, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:52,958, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:08:56,884, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:08:56,886, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:08:56,891, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:08:56,896, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:08:57,173, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:08:57,174, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:08:58,252, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:01,850, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:01,853, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:01,857, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:01,863, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:02,091, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:02,092, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:03,007, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:07,284, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:07,286, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:07,292, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:07,297, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:07,590, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:07,591, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:08,614, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:12,609, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:12,611, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:12,617, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:12,627, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:12,937, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:12,938, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:13,916, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:17,970, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:17,972, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:17,977, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:17,983, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:18,279, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:18,281, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:19,214, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:24,392, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:24,393, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:24,398, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:24,403, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:24,641, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:24,642, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:25,472, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:30,634, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:30,636, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:30,641, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:30,647, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:30,891, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:30,892, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:31,746, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:37,113, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:37,114, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:37,119, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:37,125, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:37,406, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:37,407, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:38,354, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:42,553, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:42,555, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:42,559, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:42,566, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:42,844, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:42,846, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:43,829, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:47,811, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:47,813, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:47,818, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:47,826, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:48,081, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:48,082, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:48,860, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:53,304, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:53,305, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:53,310, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:53,316, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:53,597, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:53,597, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:09:54,555, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:09:58,960, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:09:58,962, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:09:58,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:09:58,971, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:09:59,208, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:09:59,209, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:00,201, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:10:04,277, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:10:04,279, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:10:04,284, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:10:04,288, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:10:04,529, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:10:04,531, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:05,664, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:10:10,105, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:10:10,107, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:10:10,112, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:10:10,118, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:10:10,400, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:10:10,401, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:11,426, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:10:15,789, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:10:15,791, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:10:15,796, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:10:15,800, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:10:16,039, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:10:16,040, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:16,978, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:10:21,320, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:10:21,323, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:10:21,328, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:10:21,334, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:10:21,608, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:10:21,609, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:22,656, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:10:26,660, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:10:26,662, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:10:26,667, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:10:26,672, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:10:26,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:10:26,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:27,937, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:10:31,506, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:10:31,508, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:10:31,513, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:10:31,519, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:10:31,804, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:10:31,806, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:32,658, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:10:35,888, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:10:35,890, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:10:35,895, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:10:35,900, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:10:36,129, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:10:36,130, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:36,967, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:10:40,348, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:10:40,350, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:10:40,353, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:10:40,361, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:10:40,774, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:10:40,776, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:10:42,065, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:16:53,604, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:16:53,607, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:16:53,614, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:16:53,617, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:16:53,622, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:16:53,628, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:16:53,938, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:16:53,940, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:16:54,972, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:17:06,273, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:17:06,275, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:17:06,280, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:17:06,283, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:17:06,289, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:17:06,295, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:17:06,546, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:17:06,548, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:17:07,682, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:17:12,706, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:17:12,708, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:17:12,713, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:17:12,720, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:17:12,991, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:17:12,992, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:17:14,012, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:17:18,656, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:17:18,658, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:17:18,663, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:17:18,670, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:17:18,976, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:17:18,978, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:17:19,940, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:17:24,769, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:17:24,772, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:17:24,777, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:17:24,781, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:17:25,050, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:17:25,052, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:17:26,257, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:18:54,873, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:18:54,875, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:18:54,880, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:18:54,883, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:18:54,888, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:18:54,895, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:18:55,153, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:18:55,156, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:18:56,235, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:19:00,868, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:19:00,871, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:19:00,878, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:19:00,883, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:19:01,161, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:19:01,163, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:19:02,199, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:19:06,998, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:19:07,000, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:19:07,004, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:19:07,009, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:19:07,250, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:19:07,251, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:19:08,312, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:20:14,505, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:14,507, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:14,511, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:14,513, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:14,519, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:20:14,525, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:20:14,764, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:20:14,766, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:20:16,022, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:20:20,396, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:20,397, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:20,402, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:20:20,408, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:20:20,728, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:20:20,729, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:20:21,758, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:20:26,400, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:26,402, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:26,407, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:20:26,413, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:20:26,706, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:20:26,708, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:20:27,700, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:20:31,889, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:31,892, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:31,898, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:20:31,903, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:20:32,180, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:20:32,181, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:20:33,254, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:20:37,921, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:37,922, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:37,927, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:20:37,933, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:20:38,192, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:20:38,193, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:20:39,070, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:20:43,703, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:43,705, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:43,710, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:20:43,715, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:20:43,950, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:20:43,951, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:20:44,956, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:20:50,027, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:50,029, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:50,034, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:20:50,039, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:20:50,315, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:20:50,316, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:20:51,132, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:20:56,288, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:20:56,290, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:20:56,295, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:20:56,301, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:20:56,594, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:20:56,595, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:20:57,866, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:03,442, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:03,444, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:03,449, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:03,457, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:03,777, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:03,779, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:05,040, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:10,677, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:10,679, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:10,685, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:10,693, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:11,031, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:11,033, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:12,230, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:16,330, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:16,332, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:16,338, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:16,343, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:16,624, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:16,626, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:17,778, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:26,688, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:26,690, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:26,695, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:26,698, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:26,703, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:26,710, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:26,962, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:26,964, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:27,964, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:32,565, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:32,569, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:32,709, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:32,711, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:33,203, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:33,635, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:33,637, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:33,642, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:33,647, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:33,960, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:33,961, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:34,936, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:39,824, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:39,828, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:39,969, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:39,970, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:40,546, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:40,971, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:40,973, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:40,977, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:40,982, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:41,231, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:41,231, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:42,184, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:47,002, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:47,005, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:47,138, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:47,139, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:47,687, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:48,078, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:48,080, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:48,085, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:48,090, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:48,331, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:48,334, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:49,325, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:54,308, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:54,311, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:54,467, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:54,469, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:55,020, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:21:55,343, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:21:55,345, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:21:55,352, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:21:55,358, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:21:55,602, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:21:55,604, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:21:56,702, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:01,660, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:01,664, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:01,798, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:01,800, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:02,301, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:02,569, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:02,571, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:02,575, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:02,582, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:02,830, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:02,831, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:03,693, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:08,775, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:08,780, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:08,894, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:08,895, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:09,295, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:09,582, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:09,584, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:09,588, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:09,594, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:09,821, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:09,822, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:10,808, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:15,652, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:15,655, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:15,792, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:15,794, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:16,322, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:16,637, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:16,639, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:16,643, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:16,649, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:16,925, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:16,926, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:17,904, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:22,658, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:22,662, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:22,818, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:22,820, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:23,299, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:23,602, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:23,604, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:23,608, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:23,615, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:23,846, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:23,847, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:24,778, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:29,602, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:29,605, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:29,738, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:29,739, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:30,145, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:30,395, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:30,397, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:30,401, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:30,406, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:30,640, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:30,641, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:31,649, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:35,424, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:35,429, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:35,566, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:35,567, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:36,064, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:36,370, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:36,371, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:36,377, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:36,383, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:36,669, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:36,670, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:37,609, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:41,356, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:41,359, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:41,494, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:41,495, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:41,961, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:42,234, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:42,236, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:42,241, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:42,247, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:42,509, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:42,511, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:43,460, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:47,252, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:47,255, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:47,376, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:47,377, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:47,830, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:48,129, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:48,131, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:48,135, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:48,141, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:48,415, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:48,416, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:49,528, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:53,639, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:53,643, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:53,778, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:53,779, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:54,264, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:22:54,635, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:22:54,637, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:22:54,641, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:22:54,647, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:22:54,936, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:22:54,937, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:22:55,861, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:00,023, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:00,027, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:00,162, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:00,163, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:00,573, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:00,877, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:00,879, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:00,883, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:00,889, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:01,137, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:01,139, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:02,086, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:06,395, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:06,398, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:06,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:06,513, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:06,929, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:07,228, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:07,230, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:07,234, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:07,240, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:07,474, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:07,475, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:08,380, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:13,984, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:13,988, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:14,108, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:14,109, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:14,639, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:14,998, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:14,999, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:15,004, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:15,010, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:15,288, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:15,290, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:16,249, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:21,854, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:21,858, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:21,992, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:21,994, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:22,547, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:22,901, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:22,903, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:22,907, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:22,913, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:23,189, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:23,190, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:24,203, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:29,835, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:29,839, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:29,967, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:29,968, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:30,464, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:30,817, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:30,819, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:30,824, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:30,830, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:31,132, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:31,134, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:32,130, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:36,429, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:36,432, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:36,572, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:36,573, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:37,007, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:37,272, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:37,274, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:37,278, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:37,284, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:37,532, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:37,534, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:38,402, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:42,856, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:42,859, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:42,998, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:43,000, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:43,483, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:43,808, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:43,810, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:43,814, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:43,820, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:44,093, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:44,094, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:45,023, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:49,253, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:49,256, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:49,379, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:49,379, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:49,885, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:50,210, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:50,212, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:50,217, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:50,223, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:50,500, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:50,501, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:51,456, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:55,879, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:55,883, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:56,026, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:56,027, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:56,556, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:23:56,861, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:23:56,862, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:23:56,867, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:23:56,872, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:23:57,158, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:23:57,159, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:23:58,074, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:02,760, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:02,764, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:02,907, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:02,909, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:03,477, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:03,836, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:24:03,837, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:24:03,842, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:03,848, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:04,140, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:04,141, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:04,992, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:09,460, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:09,464, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:09,595, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:09,597, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:10,003, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:10,366, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:24:10,369, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:24:10,373, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:10,379, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:10,655, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:10,657, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:11,617, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:16,295, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:16,299, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:16,435, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:16,436, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:16,836, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:17,091, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:24:17,093, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:24:17,098, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:17,103, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:17,328, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:17,330, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:18,237, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:24,174, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:24,178, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:24,316, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:24,318, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:24,804, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:25,132, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:24:25,134, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:24:25,140, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:25,146, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:25,448, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:25,449, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:26,301, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:30,795, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:30,798, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:30,915, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:30,917, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:31,316, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:31,582, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:24:31,584, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:24:31,588, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:31,594, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:31,822, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:31,823, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:32,678, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:36,224, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:36,228, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:36,343, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:36,344, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:36,741, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:37,001, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:24:37,003, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:24:37,007, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:37,013, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:37,289, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:37,290, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:38,281, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:41,664, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:41,667, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:41,802, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:41,804, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:42,270, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:42,519, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:24:42,521, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:24:42,525, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:42,531, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:42,815, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:42,817, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:43,639, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:47,122, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:47,125, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:47,264, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:47,265, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:47,842, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:24:48,148, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:24:48,150, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:24:48,153, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:24:48,161, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:24:48,551, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:24:48,552, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:24:52,029, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:19,996, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:26:19,998, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:26:20,002, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:26:20,005, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:26:20,010, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:20,016, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:20,267, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:20,268, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:21,217, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:25,642, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:25,646, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:25,782, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:25,784, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:26,230, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:26,580, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:26:26,582, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:26:26,586, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:26,591, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:26,847, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:26,848, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:27,677, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:32,089, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:32,093, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:32,207, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:32,209, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:32,626, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:33,013, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:26:33,015, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:26:33,019, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:33,024, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:33,271, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:33,272, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:34,239, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:38,896, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:38,899, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:39,039, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:39,041, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:39,557, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:39,925, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:26:39,927, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:26:39,932, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:39,938, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:40,167, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:40,167, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:41,133, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:46,063, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:46,067, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:46,181, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:46,183, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:46,587, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:46,854, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:26:46,857, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:26:46,862, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:46,869, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:47,100, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:47,101, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:48,051, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:53,124, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:53,129, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:53,264, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:53,265, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:53,692, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:26:54,009, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:26:54,011, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:26:54,015, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:26:54,021, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:26:54,333, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:26:54,334, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:26:55,422, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:00,471, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:00,475, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:00,622, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:00,624, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:01,064, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:01,345, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:01,348, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:01,352, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:01,357, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:01,591, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:01,592, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:02,505, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:07,495, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:07,499, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:07,662, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:07,663, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:08,280, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:08,612, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:08,614, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:08,619, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:08,625, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:08,930, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:08,930, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:09,982, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:14,591, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:14,595, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:14,728, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:14,730, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:15,152, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:15,402, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:15,403, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:15,408, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:15,414, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:15,634, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:15,635, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:16,725, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:21,314, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:21,317, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:21,454, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:21,455, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:21,937, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:22,239, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:22,241, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:22,245, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:22,251, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:22,529, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:22,530, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:23,575, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:27,109, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:27,112, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:27,249, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:27,250, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:27,727, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:28,025, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:28,027, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:28,032, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:28,037, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:28,314, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:28,315, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:29,139, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:32,770, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:32,774, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:32,915, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:32,917, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:33,376, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:33,623, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:33,625, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:33,629, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:33,633, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:33,902, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:33,903, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:34,820, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:38,297, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:38,301, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:38,426, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:38,427, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:38,843, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:39,091, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:39,093, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:39,097, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:39,102, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:39,346, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:39,347, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:40,318, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:44,240, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:44,243, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:44,361, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:44,362, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:44,838, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:45,201, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:45,204, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:45,209, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:45,215, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:45,497, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:45,498, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:46,467, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:50,609, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:50,612, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:50,747, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:50,748, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:51,234, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:51,587, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:51,589, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:51,593, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:51,598, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:51,825, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:51,826, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:52,664, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:56,898, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:56,902, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:57,036, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:57,038, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:57,523, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:27:57,886, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:27:57,888, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:27:57,892, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:27:57,898, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:27:58,199, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:27:58,200, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:27:59,182, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:04,605, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:04,608, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:04,751, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:04,752, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:05,242, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:05,528, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:05,530, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:05,535, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:05,540, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:05,787, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:05,789, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:06,722, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:12,346, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:12,349, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:12,484, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:12,485, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:12,900, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:13,237, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:13,239, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:13,243, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:13,249, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:13,538, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:13,539, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:14,525, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:19,985, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:19,989, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:20,125, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:20,126, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:20,611, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:20,954, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:20,955, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:20,960, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:20,966, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:21,240, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:21,241, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:22,166, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:26,153, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:26,156, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:26,275, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:26,277, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:26,848, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:27,174, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:27,176, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:27,181, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:27,186, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:27,462, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:27,463, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:28,432, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:32,700, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:32,703, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:32,816, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:32,817, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:33,282, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:33,619, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:33,620, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:33,625, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:33,631, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:33,918, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:33,919, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:35,133, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:39,083, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:39,086, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:39,241, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:39,242, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:39,755, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:40,064, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:40,066, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:40,069, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:40,074, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:40,301, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:40,302, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:41,136, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:45,611, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:45,615, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:45,751, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:45,753, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:46,231, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:46,583, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:46,584, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:46,589, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:46,595, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:46,873, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:46,873, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:47,794, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:52,277, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:52,281, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:52,421, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:52,422, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:52,897, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:53,266, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:53,268, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:53,273, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:53,279, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:53,556, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:53,558, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:54,468, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:58,847, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:58,850, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:58,964, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:58,965, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:28:59,363, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:28:59,645, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:28:59,646, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:28:59,650, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:28:59,655, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:28:59,878, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:28:59,880, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:00,680, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:04,691, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:04,695, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:04,848, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:04,850, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:05,340, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:05,649, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:29:05,651, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:29:05,656, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:05,663, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:05,952, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:05,954, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:06,889, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:11,150, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:11,155, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:11,307, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:11,308, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:11,784, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:12,089, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:29:12,092, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:29:12,096, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:12,101, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:12,378, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:12,379, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:13,475, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:17,682, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:17,686, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:17,802, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:17,803, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:18,257, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:18,520, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:29:18,522, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:29:18,527, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:18,534, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:18,765, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:18,766, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:19,655, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:23,164, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:23,167, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:23,295, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:23,296, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:23,730, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:24,035, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:29:24,037, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:29:24,042, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:24,048, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:24,326, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:24,328, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:25,345, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:28,856, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:28,860, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:28,981, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:28,982, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:29,460, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:29,764, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:29:29,766, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:29:29,770, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:29,777, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:30,053, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:30,055, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:31,129, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:34,907, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:34,910, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:35,052, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:35,054, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:35,572, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:29:35,880, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:29:35,882, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:29:35,884, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:29:35,893, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:29:36,303, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:29:36,304, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:29:37,594, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:17,279, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,282, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,296, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,299, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,302, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,304, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,308, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,311, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,314, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,317, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,319, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,322, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,326, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,329, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,332, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,333, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,336, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,338, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,429, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,432, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,435, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,449, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,452, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,455, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,459, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,462, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,468, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,471, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,474, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,479, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,502, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,505, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:17,509, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:17,513, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:32,094, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:32,094, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:32,094, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:32,094, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:32,095, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:32,095, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:32,096, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:32,097, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:32,105, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:32,106, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:32,107, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:32,107, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:32,107, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:32,108, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:32,110, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:32,683, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:32,683, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:32,692, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:32,692, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:32,693, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:32,694, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:32,695, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:32,696, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:32,698, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:32,699, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:32,699, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:32,700, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:32,718, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:32,719, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:32,721, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:32,722, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:43,899, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:43,928, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:43,957, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:44,097, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:44,120, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:44,128, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:44,227, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:44,240, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:53,395, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:53,398, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:53,549, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:53,552, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:53,554, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:53,555, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:53,578, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:53,581, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:53,714, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:53,714, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:53,741, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:53,742, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:54,022, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:54,025, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:54,102, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:54,106, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:54,152, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:54,156, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:54,172, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:54,173, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:54,200, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:54,255, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:54,256, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:54,313, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:54,314, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:54,347, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:54,383, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:54,640, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:54,645, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:54,649, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:54,648, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:54,651, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:54,655, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:54,654, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:54,657, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:54,661, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:54,664, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:54,667, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:54,670, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:54,675, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:54,678, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:54,681, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:54,684, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:54,688, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:33:54,691, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:33:54,788, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:54,791, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:54,799, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:54,803, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:54,811, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:54,846, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:54,846, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:54,858, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:54,946, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:54,947, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:54,994, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:55,005, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:55,005, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:55,007, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:55,017, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:55,413, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:55,423, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:55,434, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:55,434, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:55,452, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:55,461, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:55,498, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:55,568, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:55,586, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:55,596, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:55,620, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:55,621, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:55,624, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:55,625, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:56,062, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:56,062, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:56,114, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:56,117, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:56,118, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:56,125, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:56,190, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:33:56,201, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:33:56,244, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:56,245, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:56,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:56,783, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:56,834, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:33:56,835, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:33:57,910, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:58,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:58,227, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:58,560, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:58,580, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:58,679, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:59,276, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:33:59,316, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:07,447, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:07,449, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:07,610, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:07,611, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:07,820, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:07,823, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:07,941, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:07,944, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:07,987, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:07,988, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:08,098, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:08,099, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:08,282, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:08,295, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:08,298, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:08,445, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:08,448, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:08,451, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:08,454, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:08,455, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:08,455, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:08,607, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:08,608, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:08,612, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:08,622, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:08,623, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:08,701, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:08,742, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:08,756, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:08,745, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:08,760, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:08,763, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:08,766, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:08,769, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:08,772, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:08,774, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:08,775, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:08,779, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:08,781, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:08,785, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:08,788, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:08,791, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:08,793, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:08,797, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:08,800, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:09,080, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:09,091, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:09,101, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:09,193, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:09,203, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:09,308, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:09,322, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:09,362, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:09,365, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:09,438, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:09,439, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:09,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:09,513, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:09,571, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:09,581, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:09,723, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:09,726, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:09,728, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:09,729, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:09,780, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:09,791, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:09,793, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:09,803, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:09,820, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:09,820, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:09,882, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:09,883, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:10,158, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:10,226, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:10,226, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:10,424, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:10,425, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:10,451, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:10,452, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:10,524, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:10,650, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:10,661, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:11,085, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:11,096, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:11,348, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:11,349, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:11,742, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:11,743, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:12,005, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:12,252, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:12,292, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:12,726, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:12,976, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:13,037, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:13,883, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:14,262, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:22,350, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:22,353, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:22,446, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:22,449, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:22,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:22,513, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:22,582, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:22,585, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:22,610, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:22,611, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:22,743, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:22,744, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:23,031, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:23,034, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:23,149, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:23,198, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:23,199, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:23,237, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:23,241, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:23,265, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:23,388, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:23,404, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:23,405, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:23,412, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:23,415, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:23,578, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:23,579, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:23,622, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:23,625, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:23,628, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:23,642, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:23,642, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:23,645, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:23,648, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:23,651, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:23,654, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:23,656, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:23,657, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:23,660, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:23,663, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:23,665, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:23,668, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:23,672, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:23,675, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:23,677, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:23,798, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:23,808, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:23,861, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:23,913, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:23,923, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:24,082, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:24,271, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:24,325, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:24,326, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:24,367, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:24,378, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:24,392, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:24,395, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:24,444, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:24,444, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:24,541, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:24,541, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:24,555, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:24,556, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:24,595, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:24,606, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:24,752, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:24,755, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:24,776, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:24,787, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:24,933, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:24,934, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:25,025, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:25,026, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:25,187, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:25,250, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:25,251, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:25,426, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:25,427, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:25,615, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:25,699, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:25,710, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:26,132, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:26,142, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:26,369, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:26,370, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:26,781, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:26,781, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:26,947, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:26,997, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:27,113, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:27,510, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:27,830, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:28,002, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:29,042, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:29,427, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:37,513, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:37,516, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:37,676, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:37,677, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:38,024, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:38,027, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:38,190, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:38,191, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:38,211, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:38,214, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:38,329, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:38,379, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:38,379, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:38,552, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:38,555, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:38,717, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:38,717, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:38,792, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:38,807, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:38,795, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:38,811, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:38,813, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:38,817, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:38,820, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:38,823, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:38,824, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:38,826, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:38,829, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:38,832, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:38,836, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:38,839, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:38,842, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:38,845, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:38,848, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:38,851, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:38,861, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:38,864, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:38,891, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:39,053, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:39,054, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:39,063, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:39,276, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:39,279, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:39,368, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:39,380, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:39,387, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:39,449, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:39,450, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:39,501, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:39,501, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:39,554, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:39,565, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:39,639, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:39,642, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:39,769, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:39,805, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:39,806, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:39,857, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:39,861, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:39,887, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:39,898, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:40,043, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:40,044, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:40,052, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:40,052, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:40,096, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:40,258, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:40,259, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:40,282, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:40,294, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:40,460, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:40,551, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:40,552, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:40,604, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:40,616, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:40,728, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:40,959, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:40,960, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:40,983, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:40,994, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:41,262, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:41,273, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:41,299, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:41,300, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:41,649, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:41,650, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:41,926, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:41,927, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:42,124, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:42,613, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:42,889, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:43,094, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:43,559, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:43,927, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:44,168, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:44,619, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:52,117, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:52,120, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:52,281, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:52,282, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:52,696, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:52,700, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:52,724, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:52,727, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:52,861, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:52,863, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:52,909, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:52,910, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:52,941, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:53,006, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:53,009, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:53,179, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:53,183, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:53,186, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:53,187, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:53,236, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:53,239, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:53,337, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:53,347, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:53,348, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:53,352, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:53,340, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:53,356, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:34:53,359, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:34:53,367, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:53,421, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:53,421, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:53,531, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:53,582, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:53,596, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:53,599, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:53,709, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:53,711, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:53,771, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:53,772, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:53,907, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:53,908, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:53,920, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:54,078, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:54,079, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:54,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:54,104, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:54,115, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:54,116, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:54,128, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:54,174, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:54,371, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:54,384, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:54,502, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:54,520, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:54,532, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:54,578, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:54,708, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:54,718, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:54,782, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:54,782, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:54,808, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:54,808, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:54,995, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:55,006, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:55,009, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:34:55,020, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:34:55,059, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:55,060, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:55,187, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:55,188, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:55,370, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:55,371, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:55,681, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:55,682, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:55,691, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:34:55,691, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:34:56,683, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:57,363, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:57,477, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:57,699, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:57,843, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:57,928, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:58,345, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:34:58,401, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:07,334, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:07,337, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:07,502, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:07,502, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:07,770, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:07,774, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:07,938, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:07,939, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:07,949, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:07,952, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:08,117, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:08,118, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:08,155, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:08,326, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:08,329, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:08,384, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:08,387, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:08,502, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:08,503, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:08,560, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:08,561, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:08,584, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:08,656, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:08,666, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:08,698, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:08,701, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:08,785, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:08,866, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:08,867, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:09,004, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:09,007, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:09,075, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:09,086, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:09,111, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:09,115, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:09,176, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:09,177, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:09,203, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:09,223, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:09,281, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:09,281, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:09,319, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:09,319, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:09,520, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:09,727, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:09,728, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:09,823, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:09,935, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:11,331, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:11,590, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:16,506, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:16,508, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:16,593, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:16,594, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:16,703, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:16,705, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:16,784, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:16,785, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:16,900, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:17,088, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:35:17,306, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:35:17,307, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:35:17,310, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:35:17,319, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:35:17,724, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:35:17,725, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:35:19,189, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:16,895, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,898, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,905, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,908, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,910, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,913, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,914, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,917, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,919, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,921, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,924, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,927, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,929, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,931, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,933, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,936, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,940, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:16,942, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:16,972, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:16,981, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:17,046, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:17,026, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:17,052, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:17,056, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:17,058, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:17,061, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:17,064, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:17,067, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:17,072, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:17,076, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:17,079, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:17,082, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:17,086, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:17,090, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:17,093, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:17,096, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:17,099, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:17,102, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:17,762, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:17,763, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:17,826, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:17,827, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:21,148, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:21,236, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:25,083, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:25,096, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:25,151, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:25,156, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:25,164, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:25,167, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:25,178, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:25,191, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:25,193, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:25,206, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:25,223, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:25,234, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:25,729, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:25,729, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:25,771, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:25,771, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:25,773, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:25,773, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:25,815, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:25,816, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:25,842, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:25,843, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:25,859, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:25,860, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:34,299, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:34,302, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:34,464, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:34,465, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:34,478, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:34,481, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:34,641, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:34,641, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:35,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:35,289, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:35,586, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:35,589, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:35,593, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:35,605, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:35,606, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:35,610, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:35,612, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:35,616, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:35,619, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:35,622, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:35,622, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:35,625, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:35,629, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:35,631, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:35,635, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:35,638, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:35,641, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:35,644, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:35,881, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:35,892, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:36,243, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:36,244, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:36,546, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:36,547, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:36,792, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:36,814, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:36,862, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:36,878, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:37,034, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:37,097, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:38,719, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:39,136, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:48,862, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:48,865, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:48,934, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:48,937, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:49,014, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:49,014, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:49,098, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:49,098, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:49,327, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:49,330, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:49,495, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:49,496, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:49,655, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:49,720, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:50,171, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:50,209, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:50,212, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:50,239, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:50,250, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:50,301, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:50,312, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:50,408, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:50,408, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:50,512, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:50,515, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:50,602, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:50,605, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:50,671, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:50,672, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:50,765, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:50,768, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:50,859, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:50,870, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:50,919, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:50,920, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:50,938, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:50,939, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:51,037, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:51,343, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:51,447, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:51,532, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:51,533, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:51,582, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:51,594, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:51,901, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:51,912, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:52,015, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:52,026, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:52,111, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:52,114, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:52,235, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:52,235, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:52,281, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:52,282, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:52,591, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:52,592, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:52,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:52,668, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:52,934, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:53,146, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:53,149, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:53,305, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:53,305, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:53,362, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:53,365, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:53,380, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:53,380, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:53,383, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:53,386, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:53,389, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:53,390, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:53,392, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:53,395, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:53,399, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:53,402, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:53,405, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:53,408, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:53,412, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:53,414, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:53,417, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:40:53,420, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:40:53,517, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:53,552, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:53,975, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:54,031, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:54,032, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:54,216, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:54,432, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:40:54,446, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:40:54,758, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:55,098, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:40:55,098, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:40:55,235, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:55,326, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:56,626, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:40:57,811, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:04,141, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:04,144, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:04,271, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:04,274, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:04,310, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:04,311, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:04,433, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:04,434, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:04,979, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:05,088, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:05,090, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:05,093, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:05,277, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:05,278, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:05,427, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:05,430, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:05,509, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:05,521, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:05,599, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:05,600, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:05,604, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:05,618, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:05,932, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:06,155, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:06,155, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:06,247, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:06,262, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:06,262, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:06,333, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:06,336, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:06,460, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:06,472, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:06,507, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:06,507, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:06,784, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:06,795, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:07,154, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:07,155, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:07,158, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:07,454, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:07,455, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:07,692, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:07,705, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:08,392, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:08,393, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:08,474, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:08,477, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:08,640, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:08,641, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:08,684, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:08,880, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:09,315, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:09,579, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:09,583, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:09,750, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:09,751, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:09,828, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:09,838, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:09,900, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:09,986, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:10,409, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:10,529, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:10,530, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:10,791, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:10,794, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:10,859, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:10,874, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:10,862, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:10,878, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:10,881, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:10,884, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:10,887, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:10,891, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:10,891, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:10,894, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:10,897, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:10,900, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:10,903, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:10,907, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:10,910, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:10,913, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:10,917, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:10,920, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:11,037, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:11,038, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:11,128, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:11,600, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:11,600, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:11,685, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:12,220, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:12,232, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:12,889, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:12,889, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:13,363, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:14,293, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:15,583, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:21,495, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:21,498, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:21,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:21,668, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:21,832, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:21,835, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:21,993, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:21,994, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:22,362, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:22,460, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:22,464, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:22,631, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:22,632, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:22,644, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:22,714, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:22,717, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:22,880, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:22,881, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:22,883, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:22,894, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:23,122, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:23,133, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:23,294, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:23,561, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:23,562, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:23,581, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:23,703, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:23,707, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:23,785, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:23,786, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:23,804, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:23,816, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:23,874, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:23,875, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:24,149, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:24,160, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:24,481, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:24,482, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:24,515, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:24,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:24,833, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:25,013, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:25,023, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:25,651, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:25,651, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:26,193, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:26,197, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:26,331, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:26,382, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:26,383, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:26,419, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:27,023, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:27,043, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:27,102, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:27,105, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:27,271, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:27,272, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:27,567, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:27,570, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:27,583, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:27,593, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:27,603, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:27,739, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:27,740, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:27,894, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:28,214, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:28,235, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:28,235, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:28,373, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:28,375, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:28,378, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:28,392, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:28,392, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:28,396, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:28,400, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:28,399, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:28,403, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:28,406, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:28,409, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:28,412, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:28,415, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:28,416, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:28,419, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:28,422, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:28,425, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:28,429, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:28,431, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:28,908, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:28,919, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:29,046, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:29,047, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:29,615, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:29,616, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:30,947, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:31,659, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:32,327, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:38,244, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:38,247, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:38,397, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:38,398, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:38,444, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:38,447, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:38,604, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:38,607, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:38,607, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:38,608, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:38,776, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:38,777, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:39,037, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:39,330, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:39,431, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:39,496, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:39,508, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:39,755, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:39,758, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:39,840, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:39,853, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:39,918, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:39,919, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:39,926, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:39,936, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:39,940, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:39,945, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:40,110, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:40,111, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:40,141, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:40,142, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:40,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:40,513, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:40,583, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:40,584, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:40,588, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:40,748, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:41,064, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:41,074, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:41,175, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:41,187, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:41,769, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:41,769, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:41,855, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:41,856, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:42,717, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:43,113, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:43,116, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:43,175, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:43,286, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:43,287, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:43,311, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:43,667, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:43,670, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:43,823, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:43,824, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:44,022, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:44,438, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:44,445, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:44,455, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:44,473, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:44,511, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:44,514, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:44,603, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:44,674, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:44,675, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:44,848, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:44,851, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:44,866, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:44,866, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:41:44,869, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:41:44,883, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:45,138, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:45,138, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:45,436, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:45,552, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:45,553, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:45,873, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:45,883, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:46,538, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:46,539, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:47,900, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:48,215, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:49,220, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:54,601, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:54,604, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:54,763, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:54,763, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:55,396, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:55,601, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:55,604, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:55,771, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:55,772, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:55,773, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:55,774, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:55,816, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:55,828, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:55,944, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:55,945, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:56,478, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:56,488, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:56,488, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:56,564, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:56,567, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:56,647, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:56,739, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:56,740, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:56,819, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:56,823, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:56,991, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:56,993, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:57,008, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:57,019, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:57,219, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:57,230, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:57,402, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:57,633, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:41:57,682, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:57,683, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:57,887, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:57,888, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:57,953, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:57,965, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:58,191, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:41:58,203, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:41:58,642, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:58,643, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:58,868, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:41:58,869, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:41:59,131, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:00,370, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:00,483, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:00,486, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:00,641, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:00,642, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:00,671, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:00,674, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:00,718, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:00,839, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:00,839, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:01,261, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:01,320, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:01,503, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:01,518, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:01,851, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:01,863, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:01,867, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:01,871, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:01,894, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:01,904, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:02,037, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:02,038, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:02,536, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:02,537, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:02,544, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:02,545, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:02,729, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:03,146, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:03,157, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:03,831, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:03,831, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:05,202, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:05,307, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:06,498, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:11,404, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:11,409, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:11,578, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:11,579, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:11,686, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:11,689, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:11,774, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:11,777, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:11,853, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:11,854, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:11,939, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:11,940, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:12,219, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:12,547, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:12,607, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:13,062, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:13,065, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:13,234, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:13,235, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:13,547, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:13,550, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:13,704, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:13,705, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:13,867, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:14,165, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:14,641, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:14,643, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:14,756, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:14,756, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:14,901, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:14,903, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:14,993, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:14,994, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:15,107, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:15,327, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:15,414, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:15,416, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:15,499, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:15,499, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:15,806, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:42:16,069, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:42:16,071, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:42:16,074, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:42:16,082, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:42:16,536, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:42:16,537, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:42:17,966, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:24,869, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,872, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,879, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,881, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,884, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,886, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,889, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,891, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,893, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,895, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,898, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,901, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,904, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,906, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,909, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,911, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,913, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,915, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,917, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,944, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:24,932, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,946, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,949, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,951, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,953, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:24,953, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,957, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,960, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,963, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,966, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,969, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,972, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,975, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,977, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:24,980, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:24,983, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:25,316, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:25,316, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:27,691, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:31,848, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:31,864, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:32,810, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:32,811, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:33,635, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:33,647, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:34,428, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:34,428, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:34,726, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:34,743, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:35,545, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:35,560, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:35,593, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:35,593, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:36,522, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:36,523, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:36,906, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:36,924, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:37,562, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:37,563, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:37,650, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:37,663, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:38,291, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:38,291, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:38,349, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:38,362, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:39,031, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:39,036, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:40,565, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:40,568, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:40,744, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:40,744, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:41,401, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:41,905, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:41,908, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:41,923, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:50:41,925, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:41,927, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:41,931, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:41,934, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:41,936, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:50:41,937, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:41,939, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:41,942, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:41,945, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:41,948, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:41,950, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:41,954, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:41,956, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:41,959, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:50:41,962, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:50:42,820, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:50:42,820, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:50:48,123, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:48,492, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:50,297, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:51,301, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:51,805, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:53,712, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:54,651, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:50:56,264, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:02,470, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:02,474, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:02,709, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:02,710, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:03,933, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:04,706, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:04,709, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:04,792, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:04,807, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:04,962, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:04,963, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:05,386, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:05,390, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:05,625, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:05,626, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:05,664, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:05,664, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:05,821, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:06,374, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:06,377, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:06,567, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:06,581, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:06,590, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:06,591, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:06,628, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:07,372, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:07,386, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:07,407, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:07,427, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:07,427, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:08,288, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:08,303, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:08,342, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:08,342, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:08,430, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:08,433, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:08,653, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:08,655, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:09,236, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:09,237, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:09,243, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:09,653, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:10,202, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:10,217, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:10,939, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:11,073, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:11,074, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:11,689, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:12,700, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:13,018, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:13,022, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:13,228, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:13,229, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:13,687, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:13,691, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:13,936, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:13,937, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:14,188, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:14,935, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:15,076, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:15,091, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:15,229, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:15,936, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:15,951, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:15,977, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:15,978, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:15,996, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:16,000, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:16,204, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:16,205, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:16,837, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:16,838, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:17,088, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:17,587, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:17,590, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:17,594, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:17,598, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:17,602, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:17,606, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:17,610, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:17,614, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:17,618, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:17,622, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:17,627, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:17,630, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:17,635, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:17,638, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:17,642, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:17,646, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:17,764, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:17,777, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:18,734, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:18,735, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:19,670, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:20,563, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:22,365, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:27,834, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:27,838, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:28,046, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:28,047, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:28,552, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:28,556, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:28,603, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:28,607, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:28,801, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:28,802, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:28,827, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:28,830, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:29,062, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:29,665, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:29,670, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:29,718, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:29,822, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:29,835, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:29,867, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:29,909, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:29,910, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:30,412, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:30,426, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:30,486, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:30,499, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:30,927, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:30,928, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:31,031, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:31,238, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:31,239, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:31,343, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:31,343, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:31,755, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:31,759, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:31,800, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:31,814, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:31,976, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:31,977, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:32,615, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:32,616, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:32,806, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:33,575, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:33,588, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:33,951, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:33,954, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:34,153, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:34,154, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:34,367, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:34,396, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:34,397, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:34,525, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:34,688, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:34,934, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:35,512, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:35,525, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:35,847, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:35,851, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:36,058, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:36,064, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:36,065, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:36,151, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:36,154, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:36,360, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:36,362, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:36,366, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:36,367, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:36,938, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:37,253, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:37,528, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:37,541, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:37,698, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:37,919, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:37,922, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:37,942, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:37,926, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:37,944, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:37,948, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:37,951, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:37,956, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:37,955, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:37,959, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:37,963, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:37,966, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:37,969, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:37,972, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:37,976, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:37,979, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:37,983, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:37,986, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:38,365, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:38,366, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:38,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:38,834, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:39,780, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:41,806, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:42,457, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:47,619, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:47,623, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:47,904, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:47,909, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:47,911, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:47,913, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:47,986, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:47,989, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:48,160, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:48,162, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:48,237, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:48,238, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:48,296, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:48,301, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:48,537, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:48,538, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:48,938, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:49,069, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:49,114, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:49,424, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:49,536, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:49,551, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:49,568, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:49,583, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:49,684, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:49,697, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:50,029, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:50,042, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:50,087, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:50,091, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:50,310, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:50,311, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:50,355, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:50,356, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:50,387, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:50,388, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:50,510, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:50,511, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:50,837, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:50,837, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:51,170, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:51,721, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:51,736, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:51,921, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:51,928, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:52,160, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:52,161, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:52,685, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:52,686, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:53,107, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:53,649, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:53,663, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:53,816, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:53,818, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:53,948, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:54,210, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:54,214, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:54,333, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:54,417, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:54,418, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:54,534, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:54,535, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:55,207, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:55,212, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:55,316, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:55,430, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:55,431, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:55,854, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:55,868, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:56,185, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:56,276, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:51:56,735, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:56,735, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:56,773, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:56,777, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:56,797, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:51:56,781, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:56,799, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:56,803, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:56,806, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:56,811, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:56,813, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:51:56,814, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:56,818, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:56,822, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:56,826, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:56,829, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:56,833, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:56,837, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:56,842, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:51:56,845, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:51:57,705, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:51:57,706, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:51:58,086, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:00,801, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:01,917, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:11,705, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:11,711, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:11,728, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:11,731, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:11,848, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:11,851, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:11,927, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:11,928, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:11,938, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:11,939, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:12,059, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:12,060, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:12,120, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:12,124, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:12,345, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:12,346, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:12,786, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:12,808, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:12,883, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:13,235, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:13,371, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:13,391, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:13,425, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:13,441, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:13,481, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:13,496, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:13,877, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:13,890, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:14,237, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:14,239, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:14,333, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:14,334, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:14,359, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:14,360, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:14,399, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:14,403, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:14,630, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:14,631, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:14,655, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:14,658, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:14,745, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:14,746, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:14,887, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:14,889, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:15,474, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:15,693, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:16,220, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:16,259, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:16,492, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:16,513, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:17,285, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:17,290, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:17,319, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:17,319, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:17,432, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:17,433, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:17,506, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:17,506, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:17,902, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:17,965, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:18,133, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:18,366, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:18,387, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:18,503, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:18,509, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:18,717, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:18,718, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:19,059, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:19,083, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:19,539, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:19,960, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:19,961, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:20,143, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:52:20,163, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:20,146, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:52:20,165, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:52:20,169, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:52:20,178, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:20,890, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:20,910, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:21,109, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:21,110, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:23,656, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:24,631, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:33,961, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:33,965, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:34,004, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:34,007, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:34,165, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:34,166, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:34,205, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:34,206, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:34,295, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:34,299, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:34,515, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:34,516, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:34,664, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:34,668, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:34,929, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:34,930, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:35,025, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:35,102, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:35,372, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:35,648, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:35,661, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:35,728, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:35,741, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:35,796, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:36,032, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:36,047, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:36,470, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:36,484, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:36,525, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:36,527, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:36,763, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:36,764, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:36,800, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:36,807, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:36,897, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:36,898, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:37,031, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:37,032, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:37,060, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:37,064, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:37,289, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:37,290, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:37,359, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:37,359, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:37,923, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:38,519, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:39,066, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:39,082, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:39,719, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:39,732, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:39,878, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:39,881, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:40,014, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:40,087, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:40,088, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:40,163, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:40,164, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:40,369, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:40,374, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:40,374, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:40,592, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:40,592, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:40,604, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:40,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:40,787, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:40,911, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:41,112, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:41,531, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:41,597, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:41,612, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:42,191, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:42,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:42,484, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:42,485, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:43,063, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:43,064, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:43,628, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:44,060, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:45,948, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:46,496, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:53,875, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:53,879, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:54,081, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:54,082, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:54,264, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:54,268, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:54,472, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:54,473, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:54,799, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:54,803, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:54,913, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:54,993, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:54,994, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:55,103, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:55,106, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:55,292, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:55,306, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:55,307, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:55,513, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:55,526, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:55,759, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:56,091, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:56,364, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:56,364, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:56,667, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:56,671, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:56,857, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:56,858, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:57,261, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:57,263, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:57,411, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:57,412, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:57,536, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:57,947, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:58,457, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:58,459, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:58,559, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:58,560, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:58,591, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:58,675, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:52:58,677, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:52:58,795, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:52:58,796, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:52:58,944, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:52:59,255, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:53:03,294, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:53:03,296, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:53:03,368, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:53:03,369, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:53:03,616, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:53:03,818, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:53:03,820, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:53:03,822, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:53:03,829, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:53:04,183, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:53:04,185, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:53:05,546, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:28,239, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,241, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,246, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,249, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,251, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,253, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,256, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,258, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,261, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,265, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,267, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,269, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,273, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,275, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,277, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,279, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,281, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,283, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,288, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,298, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,306, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:28,299, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,312, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:28,317, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:28,323, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:28,317, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,330, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:28,337, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:28,338, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:28,348, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:28,329, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,355, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:28,365, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:28,366, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:28,353, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,376, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:28,381, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:28,371, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,391, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:28,388, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,393, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,395, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,397, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:28,398, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,402, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,407, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:28,406, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,421, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,425, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:28,429, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:28,913, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:28,914, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:28,952, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:28,953, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:28,964, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:28,964, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:28,966, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:28,967, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:28,983, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:28,984, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:28,993, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:28,993, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:29,003, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:29,004, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:29,014, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:29,015, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:31,441, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:31,443, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:31,459, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:31,470, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:31,501, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:31,512, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:31,543, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:31,549, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:41,970, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:41,973, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:42,009, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:42,012, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:42,071, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:42,074, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:42,128, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:42,129, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:42,129, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:42,130, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:42,131, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:42,133, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:42,170, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:42,171, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:42,249, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:42,249, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:42,287, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:42,287, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:42,289, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:42,290, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:42,364, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:42,367, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:42,410, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:42,413, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:42,527, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:42,528, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:42,566, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:42,567, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:42,669, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:42,672, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:42,773, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:42,823, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:42,839, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:42,840, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:42,871, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:42,891, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:42,923, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:43,152, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:43,162, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:43,165, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:43,178, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:43,182, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:43,181, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:43,184, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:43,186, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:43,190, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:43,189, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:43,192, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:43,195, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:43,195, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:43,197, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:43,200, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:43,214, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:43,203, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:43,217, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:43,220, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:43,223, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:43,225, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:43,225, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:43,258, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:43,268, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:43,271, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:43,281, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:43,339, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:43,349, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:43,487, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:43,527, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:43,540, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:43,624, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:43,655, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:43,860, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:43,871, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:43,874, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:43,875, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:43,882, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:43,882, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:43,896, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:43,896, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:43,928, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:43,928, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:44,022, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:44,022, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:44,163, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:44,163, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:44,321, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:44,322, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:44,511, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:44,512, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:46,238, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:46,253, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:46,256, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:46,298, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:46,423, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:46,539, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:46,695, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:46,906, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:55,683, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:55,686, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:55,722, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:55,725, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:55,751, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:55,753, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:55,834, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:55,835, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:55,871, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:55,871, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:55,877, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:55,880, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:55,899, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:55,900, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:55,911, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:55,914, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:55,986, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:55,989, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:56,048, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:56,048, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:56,086, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:56,087, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:56,138, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:56,139, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:56,288, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:56,291, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:56,444, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:56,445, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:56,447, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:56,499, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:56,503, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:56,675, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:56,729, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:56,730, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:56,882, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:56,885, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:56,899, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:56,888, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:56,902, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:56,905, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:56,907, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:56,910, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:56,913, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:56,914, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:56,916, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:56,918, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:56,921, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:56,924, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:56,927, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:56,929, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:56,932, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:54:56,934, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:54:56,960, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:56,970, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:56,984, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:56,994, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:57,045, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:57,070, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:57,080, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:57,146, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:57,157, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:57,183, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:57,193, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:57,495, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:57,506, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:57,506, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:57,506, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:57,570, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:57,571, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:57,593, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:57,594, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:57,689, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:57,692, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:57,725, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:57,726, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:57,802, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:57,803, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:57,813, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:57,813, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:57,858, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:57,858, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:58,215, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:58,215, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:54:58,648, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:54:59,198, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:54:59,209, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:54:59,897, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:54:59,898, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:00,269, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:00,269, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:00,423, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:00,581, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:00,595, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:00,599, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:00,895, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:02,666, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:14,245, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:14,255, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:14,508, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:14,512, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:14,541, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:14,542, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:14,701, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:14,702, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:14,824, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:14,827, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:14,828, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:14,831, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:15,040, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:15,041, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:15,067, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:15,068, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:15,105, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:15,109, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:15,326, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:15,327, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:15,497, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:15,513, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:15,819, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:15,823, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:15,895, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:15,899, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:15,930, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:16,046, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:16,047, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:16,059, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:16,138, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:16,139, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:16,153, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:16,162, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:16,173, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:16,157, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:16,191, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:16,177, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:16,196, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:16,199, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:16,200, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:16,204, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:16,204, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:16,208, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:16,211, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:16,215, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:16,218, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:16,223, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:16,226, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:16,230, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:16,233, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:16,237, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:16,240, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:16,592, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:16,605, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:16,815, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:16,829, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:16,862, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:16,875, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:17,052, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:17,076, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:17,087, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:17,088, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:17,131, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:17,132, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:17,454, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:17,459, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:17,492, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:17,493, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:17,680, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:17,684, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:17,726, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:17,726, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:17,728, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:17,728, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:17,776, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:17,789, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:17,803, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:17,817, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:18,599, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:18,604, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:18,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:18,629, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:18,630, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:19,423, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:19,446, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:20,398, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:20,399, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:20,438, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:20,794, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:21,008, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:21,133, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:21,302, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:22,030, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:22,136, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:24,268, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:36,124, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:36,127, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:36,233, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:36,237, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:36,319, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:36,323, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:36,383, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:36,384, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:36,456, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:36,456, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:36,536, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:36,537, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:37,234, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:37,272, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:37,427, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:37,774, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:37,778, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:37,782, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:37,801, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:37,805, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:37,809, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:37,812, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:37,814, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:37,819, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:37,823, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:37,826, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:37,831, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:37,831, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:37,835, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:37,840, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:37,844, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:37,850, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:37,854, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:37,900, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:37,903, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:37,986, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:38,000, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:38,114, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:38,115, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:38,165, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:38,177, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:38,682, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:38,683, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:38,690, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:38,694, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:38,845, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:38,851, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:38,854, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:38,855, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:38,925, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:38,926, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:38,948, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:39,117, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:39,118, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:39,119, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:39,120, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:39,353, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:39,356, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:39,574, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:39,575, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:39,636, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:39,649, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:39,800, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:39,991, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:40,274, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:40,278, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:40,406, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:40,455, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:40,455, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:40,482, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:40,494, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:40,495, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:40,498, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:40,698, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:40,712, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:41,079, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:41,093, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:41,290, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:41,291, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:41,335, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:41,500, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:41,500, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:41,922, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:41,923, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:41,940, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:41,954, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:42,092, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:42,142, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:42,421, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:42,832, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:42,833, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:43,816, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:44,589, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:44,790, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:45,185, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:46,120, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:55,868, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:55,872, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:55,993, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:55,997, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:56,087, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:56,088, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:56,164, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:56,167, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:56,194, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:56,195, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:56,379, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:56,380, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:56,959, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:57,022, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:57,081, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:57,085, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:57,183, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:57,259, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:57,260, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:57,458, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:57,473, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:57,461, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:57,477, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:55:57,480, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:55:57,495, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:57,537, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:57,542, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:57,544, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:57,546, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:57,644, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:57,655, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:57,699, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:57,700, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:57,814, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:57,817, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:57,908, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:57,930, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:57,934, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:57,963, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:57,964, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:58,102, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:58,102, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:58,114, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:58,114, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:58,173, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:58,173, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:58,283, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:58,284, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:58,334, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:58,445, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:58,455, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:58,599, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:58,756, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:58,815, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:58,818, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:58,835, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:58,846, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:58,983, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:58,984, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:59,116, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:59,117, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:59,117, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:59,128, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:59,279, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:55:59,290, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:55:59,481, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:59,482, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:59,630, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:55:59,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:59,784, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:55:59,946, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:55:59,947, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:00,130, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:00,140, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:00,706, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:00,789, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:00,789, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:00,830, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:00,870, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:01,717, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:02,038, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:02,405, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:02,455, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:03,339, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:13,448, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:13,451, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:13,615, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:13,616, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:13,633, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:13,636, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:13,642, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:13,645, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:13,807, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:13,807, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:13,808, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:13,808, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:14,254, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:14,429, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:14,480, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:14,604, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:14,607, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:14,711, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:14,714, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:14,776, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:14,776, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:14,854, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:14,867, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:14,877, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:14,878, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:15,017, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:15,029, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:15,454, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:15,528, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:15,542, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:15,543, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:15,678, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:15,678, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:16,284, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:16,287, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:16,330, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:16,332, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:16,400, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:16,401, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:16,453, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:16,453, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:16,743, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:16,745, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:16,849, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:16,856, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:16,857, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:16,872, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:17,246, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:17,274, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:17,458, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:23,474, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:23,476, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:23,559, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:23,560, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:23,755, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:23,757, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:23,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:23,833, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:23,855, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:24,089, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-28 22:56:24,279, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-28 22:56:24,281, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-28 22:56:24,284, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-28 22:56:24,292, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-28 22:56:24,651, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-28 22:56:24,652, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-28 22:56:25,889, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:32:55,734, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:32:55,748, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:32:56,131, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:32:56,133, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:32:57,486, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:42:09,339, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:42:09,353, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,796, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,798, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,804, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,807, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,810, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,812, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,815, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,817, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,820, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,822, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,827, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,829, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,831, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,832, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,835, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,839, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,842, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,844, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,912, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,916, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,918, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,920, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,922, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,924, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,927, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,929, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,934, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,937, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,941, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,944, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,950, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,953, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:37,957, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:43:37,961, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:43:51,446, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:43:51,446, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:43:51,446, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:43:51,447, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:43:51,447, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:43:51,448, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:43:51,450, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:43:51,450, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:43:51,458, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:43:51,458, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:43:51,458, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:43:51,459, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:43:51,461, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:43:51,462, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:43:51,462, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:43:52,069, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:43:52,070, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:43:52,075, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:43:52,075, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:43:52,076, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:43:52,076, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:43:52,077, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:43:52,078, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:43:52,078, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:43:52,081, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:43:52,084, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:43:52,085, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:43:52,085, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:43:52,086, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:43:52,105, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:43:52,105, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:02,828, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:02,884, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:02,885, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:02,923, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:03,008, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:03,012, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:03,028, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:03,051, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:17,037, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:17,041, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:17,052, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:17,055, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:17,079, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:17,083, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:17,225, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:17,229, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:17,236, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:17,237, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:17,242, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:17,244, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:17,256, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:17,257, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:17,260, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:17,261, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:17,264, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:17,264, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:17,270, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:17,273, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:17,336, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:17,340, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:17,425, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:17,426, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:17,457, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:17,458, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:17,458, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:17,458, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:17,473, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:17,474, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:17,541, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:17,541, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:18,015, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:18,036, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:18,037, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:18,185, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:18,240, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:18,241, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:18,260, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:18,279, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:18,578, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:18,581, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:18,585, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:18,589, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:18,593, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:18,597, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:18,600, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:18,605, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:18,610, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:18,613, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:18,618, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:18,621, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:18,626, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:18,630, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:18,634, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:18,638, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:18,740, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:18,754, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:18,770, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:18,779, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:18,784, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:18,792, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:18,961, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:18,978, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:18,991, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:19,005, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:19,008, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:19,021, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:19,028, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:19,045, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:19,046, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:19,058, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:19,536, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:19,537, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:19,537, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:19,537, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:19,542, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:19,543, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:19,754, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:19,754, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:19,767, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:19,768, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:19,778, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:19,778, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:19,811, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:19,811, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:19,814, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:19,814, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:22,629, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:22,658, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:22,825, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:22,873, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:22,889, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:22,923, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:22,951, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:23,092, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:32,967, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:32,970, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:33,124, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:33,124, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:33,168, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:33,171, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:33,319, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:33,321, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:33,322, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:33,322, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:33,462, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:33,463, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:33,745, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:33,934, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:34,065, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:34,171, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:34,184, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:34,174, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:34,189, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:34,192, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:34,196, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:34,195, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:34,198, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:34,201, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:34,204, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:34,207, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:34,210, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:34,213, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:34,216, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:34,219, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:34,222, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:34,224, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:34,227, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:34,372, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:34,382, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:34,545, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:34,555, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:34,798, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:34,799, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:34,957, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:34,957, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:35,136, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:35,138, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:35,150, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:35,151, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:35,189, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:35,192, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:35,306, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:35,307, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:35,369, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:35,370, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:35,506, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:35,509, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:35,685, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:35,686, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:35,705, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:35,708, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:35,889, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:35,890, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:35,945, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:35,948, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:35,988, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:36,042, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:36,183, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:36,185, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:36,428, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:36,440, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:36,454, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:36,464, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:36,472, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:36,548, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:36,809, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:36,898, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:36,912, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:36,946, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:36,960, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:37,084, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:37,085, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:37,088, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:37,088, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:37,282, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:37,292, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:37,471, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:37,651, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:37,652, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:37,680, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:37,681, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:37,779, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:37,931, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:37,932, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:38,023, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:39,606, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:39,619, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:40,149, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:40,309, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:40,478, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:47,425, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:47,428, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:47,575, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:47,575, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:47,843, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:47,845, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:47,991, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:47,992, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:48,102, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:48,105, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:48,178, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:48,259, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:48,260, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:48,581, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:48,584, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:48,599, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:48,601, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:48,605, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:48,608, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:48,611, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:48,611, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:48,615, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:48,618, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:48,619, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:48,621, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:48,624, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:48,627, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:48,629, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:48,630, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:48,634, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:48,637, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:44:48,640, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:44:48,943, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:49,058, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:49,068, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:49,160, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:49,162, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:49,237, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:49,238, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:49,312, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:49,313, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:49,371, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:49,382, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:49,452, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:49,454, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:49,603, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:49,604, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:49,666, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:49,666, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:49,921, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:49,939, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:49,942, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:49,978, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:49,979, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:49,999, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:50,003, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:50,091, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:50,092, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:50,151, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:50,151, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:50,198, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:50,201, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:50,207, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:50,358, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:50,359, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:50,417, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:50,429, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:50,666, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:50,666, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:50,676, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:50,764, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:50,985, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:51,041, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:51,042, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:51,119, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:51,132, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:51,227, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:51,237, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:51,293, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:51,293, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:51,470, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:44:51,480, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:44:51,644, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:51,739, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:51,740, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:51,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:51,833, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:52,030, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:52,083, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:44:52,083, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:44:52,409, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:53,380, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:53,698, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:54,122, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:54,275, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:44:54,512, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:01,137, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:01,139, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:01,291, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:01,292, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:01,406, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:01,409, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:01,554, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:01,555, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:01,827, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:01,831, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:01,932, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:01,986, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:01,987, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:02,141, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:02,366, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:02,369, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:02,384, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:02,372, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:02,386, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:02,390, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:02,392, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:02,396, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:02,399, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:02,401, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:02,402, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:02,406, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:02,408, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:02,411, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:02,416, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:02,419, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:02,422, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:02,424, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:02,581, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:02,583, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:02,630, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:02,640, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:02,685, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:02,733, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:02,734, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:02,949, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:02,952, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:03,035, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:03,036, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:03,100, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:03,101, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:03,145, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:03,159, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:03,236, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:03,237, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:03,379, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:03,550, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:03,553, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:03,720, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:03,720, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:03,755, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:03,775, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:03,775, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:03,842, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:03,853, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:04,214, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:04,224, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:04,307, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:04,380, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:04,383, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:04,456, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:04,457, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:04,531, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:04,532, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:04,692, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:04,696, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:04,752, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:04,764, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:04,823, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:04,824, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:04,847, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:04,848, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:05,127, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:05,368, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:05,368, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:05,397, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:05,449, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:05,565, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:05,574, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:05,624, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:05,904, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:05,915, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:06,157, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:06,157, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:06,163, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:06,523, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:06,523, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:06,866, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:07,202, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:07,741, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:08,617, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:08,984, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:15,541, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:15,546, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:15,649, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:15,653, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:15,701, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:15,701, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:15,805, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:15,806, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:16,293, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:16,296, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:16,309, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:16,440, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:16,441, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:16,463, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:16,724, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:16,728, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:16,731, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:16,746, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:16,749, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:16,759, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:16,915, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:16,924, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:17,033, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:17,335, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:17,338, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:17,360, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:17,360, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:17,448, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:17,458, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:17,491, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:17,492, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:17,526, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:17,527, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:17,608, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:17,611, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:17,765, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:17,766, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:18,061, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:18,062, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:18,119, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:18,280, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:18,283, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:18,381, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:18,440, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:18,441, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:18,617, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:18,628, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:18,885, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:18,896, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:19,039, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:19,240, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:19,240, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:19,288, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:19,291, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:19,461, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:19,461, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:19,514, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:19,515, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:19,573, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:19,575, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:19,596, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:19,607, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:19,695, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:19,744, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:19,745, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:19,894, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:20,084, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:20,211, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:20,212, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:20,335, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:20,464, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:20,602, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:20,613, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:20,835, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:20,846, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:21,220, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:21,221, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:21,450, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:21,451, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:21,670, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:21,892, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:22,585, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:23,637, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:23,831, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:29,110, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:29,112, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:29,132, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:29,135, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:29,259, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:29,260, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:29,287, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:29,288, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:29,864, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:29,878, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:29,881, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:29,905, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:30,035, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:30,035, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:30,309, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:30,319, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:30,350, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:30,360, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:30,638, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:30,919, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:30,920, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:30,985, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:30,985, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:31,181, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:31,184, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:31,339, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:31,340, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:31,414, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:31,416, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:31,562, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:31,562, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:31,976, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:32,158, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:32,347, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:32,350, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:32,500, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:32,501, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:32,994, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:33,161, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:33,186, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:33,189, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:33,229, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:33,293, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:33,293, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:33,311, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:33,312, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:33,396, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:33,396, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:33,620, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:33,722, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:38,199, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:38,201, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:38,296, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:38,296, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:38,300, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:38,301, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:38,379, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:38,380, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:38,589, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:38,665, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:45:38,887, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:45:38,889, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:45:38,893, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:45:38,899, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:45:39,270, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:45:39,271, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:45:40,619, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:46:03,671, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:46:03,682, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:46:04,028, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:46:04,030, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:46:05,538, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:08,313, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,315, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,320, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,323, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,327, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,329, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,332, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,334, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,336, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,339, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,342, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,345, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,348, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,351, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,352, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,356, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,358, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,360, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,362, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,395, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:08,406, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:08,421, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:08,434, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:08,440, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:08,405, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,451, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:08,462, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:08,450, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,473, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:08,474, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:08,484, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:08,504, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:08,515, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:08,517, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:08,527, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:08,492, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,532, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:08,533, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,536, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,539, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,542, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:08,542, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,549, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,551, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,559, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,562, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,585, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,589, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:08,594, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:08,596, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:09,066, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:09,066, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:09,070, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:09,070, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:09,095, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:09,095, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:09,126, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:09,127, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:09,136, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:09,138, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:09,149, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:09,150, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:09,156, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:09,158, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:09,172, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:09,173, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:11,413, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:11,436, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:11,446, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:11,449, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:11,470, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:11,480, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:11,517, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:11,519, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:20,906, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:20,908, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:20,933, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:20,936, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:20,989, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:20,993, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:21,057, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:21,057, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:21,082, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:21,083, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:21,142, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:21,143, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:21,203, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:21,206, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:21,225, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:21,229, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:21,276, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:21,279, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:21,283, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:21,287, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:21,327, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:21,329, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:21,347, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:21,348, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:21,370, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:21,371, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:21,425, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:21,426, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:21,430, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:21,431, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:21,466, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:21,467, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:21,642, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:21,662, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:21,742, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:21,903, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:21,930, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:21,995, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:22,012, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:22,023, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:22,061, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:22,064, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:22,079, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:22,079, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:22,081, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:22,084, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:22,087, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:22,090, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:22,090, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:22,092, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:22,095, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:22,099, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:22,102, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:22,117, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:22,105, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:22,119, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:22,122, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:22,125, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:22,127, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:22,129, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:22,178, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:22,187, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:22,364, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:22,374, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:22,394, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:22,404, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:22,448, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:22,458, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:22,462, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:22,473, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:22,476, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:22,486, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:22,719, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:22,720, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:22,738, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:22,738, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:22,782, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:22,782, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:23,066, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:23,066, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:23,088, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:23,089, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:23,121, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:23,122, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:23,183, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:23,184, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:23,192, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:23,193, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:25,131, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:25,222, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:25,492, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:25,727, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:26,331, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:26,336, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:26,341, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:26,363, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:34,824, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:34,827, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:34,969, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:34,970, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:35,392, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:35,395, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:35,541, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:35,549, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:35,550, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:35,947, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:35,952, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:35,955, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:35,961, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:35,950, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:35,965, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:35,968, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:35,970, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:35,972, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:35,973, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:35,976, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:35,978, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:35,981, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:35,984, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:35,987, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:35,989, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:35,992, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:35,995, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:35,998, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:36,000, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:36,114, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:36,115, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:36,230, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:36,563, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:36,563, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:36,673, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:36,684, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:36,708, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:37,106, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:37,109, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:37,134, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:37,143, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:37,258, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:37,259, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:37,286, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:37,286, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:37,552, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:37,555, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:37,704, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:37,705, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:37,716, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:37,718, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:37,823, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:38,095, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:38,098, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:38,204, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:38,205, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:38,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:38,214, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:38,252, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:38,253, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:38,290, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:38,293, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:38,307, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:38,358, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:38,358, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:38,450, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:38,450, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:38,696, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:38,706, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:38,845, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:38,852, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:38,853, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:38,881, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:38,940, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:39,045, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:39,227, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:39,240, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:39,306, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:39,306, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:39,315, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:39,340, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:39,435, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:39,444, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:39,783, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:39,844, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:39,844, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:39,934, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:39,934, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:40,037, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:40,038, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:40,060, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:41,206, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:41,729, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:42,130, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:42,242, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:42,302, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:48,441, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:48,444, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:48,596, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:48,597, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:49,203, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:49,389, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:49,392, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:49,542, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:49,542, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:49,618, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:49,620, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:49,629, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:49,632, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:49,635, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:49,649, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:49,652, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:49,655, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:49,655, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:49,658, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:49,660, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:49,663, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:49,666, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:49,666, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:49,669, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:49,671, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:49,674, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:49,676, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:49,679, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:48:49,682, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:48:49,781, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:49,782, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:50,202, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:50,313, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:50,313, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:50,425, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:50,635, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:50,645, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:50,877, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:50,887, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:51,001, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:51,004, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:51,150, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:51,151, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:51,261, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:51,261, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:51,489, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:51,489, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:51,532, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:51,535, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:51,684, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:51,685, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:51,734, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:51,917, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:51,920, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:51,976, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:51,979, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:52,072, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:52,073, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:52,102, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:52,105, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:52,128, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:52,128, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:52,169, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:52,181, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:52,258, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:52,259, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:52,276, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:52,681, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:52,691, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:52,702, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:52,731, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:52,741, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:52,785, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:52,785, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:52,863, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:53,130, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:53,140, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:53,159, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:53,170, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:53,310, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:48:53,320, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:48:53,328, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:53,328, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:53,632, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:53,747, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:53,747, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:53,760, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:53,761, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:53,828, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:53,920, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:48:53,921, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:48:55,142, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:55,777, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:56,129, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:56,167, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:48:56,308, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:02,569, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:02,572, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:02,718, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:02,719, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:03,302, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:03,462, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:03,466, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:03,591, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:03,593, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:03,622, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:03,623, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:03,724, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:03,739, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:03,726, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:03,741, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:03,745, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:03,744, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:03,747, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:03,747, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:03,749, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:03,753, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:03,753, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:03,755, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:03,758, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:03,760, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:03,763, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:03,766, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:03,769, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:03,771, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:03,774, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:03,776, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:04,260, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:04,367, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:04,369, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:04,369, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:04,698, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:04,708, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:04,762, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:04,766, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:04,801, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:04,811, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:04,918, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:04,919, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:05,304, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:05,305, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:05,416, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:05,417, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:05,460, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:05,463, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:05,512, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:05,619, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:05,620, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:05,908, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:05,911, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:05,960, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:05,971, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:06,059, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:06,060, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:06,217, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:06,220, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:06,243, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:06,370, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:06,372, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:06,373, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:06,373, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:06,527, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:06,528, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:06,588, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:06,589, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:06,627, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:06,705, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:06,715, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:06,741, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:06,979, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:07,078, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:07,089, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:07,143, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:07,347, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:07,347, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:07,452, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:07,462, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:07,590, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:07,600, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:07,720, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:07,721, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:07,729, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:07,824, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:08,060, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:08,061, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:08,208, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:08,208, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:08,954, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:09,772, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:10,122, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:10,427, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:10,596, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:16,889, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:16,892, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:17,043, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:17,044, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:17,640, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:17,784, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:17,787, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:17,942, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:17,943, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:17,980, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:17,983, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:18,041, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:18,044, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:18,059, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:18,047, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:18,061, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:18,123, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:18,176, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:18,176, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:18,639, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:18,738, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:18,738, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:18,769, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:19,064, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:19,073, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:19,198, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:19,208, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:19,690, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:19,691, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:19,795, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:19,798, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:19,830, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:19,830, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:19,964, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:19,966, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:20,570, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:20,795, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:20,798, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:20,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:20,947, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:21,036, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:21,047, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:21,149, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:21,176, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:21,181, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:21,322, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:21,323, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:21,411, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:21,415, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:21,555, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:21,574, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:21,575, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:21,635, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:21,636, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:21,814, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:21,817, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:21,910, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:21,965, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:21,966, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:22,030, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:22,039, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:22,092, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:22,203, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:22,233, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:22,390, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:22,402, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:22,551, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:22,644, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:22,644, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:22,701, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:22,713, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:23,001, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:23,001, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:23,009, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:23,020, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:23,319, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:23,320, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:23,629, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:23,629, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:24,012, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:25,150, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:25,417, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:25,736, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:26,096, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:31,184, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:31,187, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:31,370, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:31,371, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:31,974, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:31,978, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:31,980, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:32,135, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:32,136, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:32,369, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:32,372, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:32,422, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:32,432, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:32,524, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:32,525, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:32,787, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:33,032, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:33,033, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:33,134, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:33,235, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:33,245, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:33,843, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:33,843, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:34,215, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:34,218, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:34,382, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:34,382, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:34,986, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:35,030, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:35,034, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:35,187, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:35,188, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:35,296, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:35,299, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:35,379, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:35,381, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:35,402, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:35,452, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:35,452, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:35,550, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:35,550, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:35,638, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:35,641, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:35,795, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:35,795, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:35,839, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:36,062, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:36,145, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:36,256, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:36,269, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:40,262, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:40,263, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:40,435, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:40,436, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:41,568, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:41,785, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:41,794, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:42,080, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:42,081, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:42,539, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:42,721, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:49:42,723, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:49:42,725, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:42,733, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:43,066, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:43,068, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:44,254, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:49:50,862, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:49:50,869, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:49:51,233, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:49:51,234, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:49:52,423, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:17,694, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,696, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,701, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,704, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,707, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,710, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,713, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,716, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,719, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,722, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,724, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,727, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,730, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,733, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,734, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,738, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,740, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,743, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,748, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,800, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:17,810, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:17,813, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:17,781, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,822, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:17,825, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:17,833, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:17,841, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:17,851, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:17,817, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,858, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:17,867, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:17,868, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:17,858, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,872, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,874, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,878, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:17,877, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,880, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:17,879, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,888, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:17,904, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:17,882, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,906, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,909, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,912, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,914, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:17,914, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,942, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:17,951, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:17,953, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:18,456, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:18,456, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:18,484, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:18,486, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:18,492, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:18,492, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:18,512, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:18,512, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:18,518, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:18,519, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:18,519, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:18,520, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:18,526, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:18,527, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:18,535, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:18,536, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:21,045, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:21,127, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:21,244, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:21,368, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:21,591, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:21,658, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:21,697, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:21,761, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:30,574, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:30,577, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:30,645, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:30,648, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:30,722, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:30,723, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:30,791, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:30,794, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:30,801, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:30,802, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:30,842, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:30,845, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:30,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:30,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:30,990, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:30,991, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:31,303, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:31,306, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:31,337, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:31,415, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:31,452, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:31,453, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:31,469, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:31,471, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:31,531, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:31,562, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:31,565, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:31,580, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:31,613, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:31,614, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:31,707, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:31,710, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:31,714, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:31,715, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:31,778, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:31,781, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:31,783, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:31,797, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:31,801, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:31,804, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:31,807, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:31,810, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:31,814, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:31,813, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:31,815, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:31,818, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:31,821, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:31,825, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:31,827, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:31,830, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:31,832, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:31,833, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:31,876, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:31,877, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:31,883, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:31,894, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:31,983, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:31,993, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:32,042, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:32,046, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:32,053, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:32,240, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:32,340, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:32,441, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:32,442, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:32,467, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:32,491, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:32,492, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:32,500, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:32,511, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:32,590, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:32,591, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:32,639, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:32,639, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:32,684, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:32,693, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:32,778, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:32,787, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:32,908, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:32,919, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:33,109, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:33,110, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:33,288, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:33,289, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:33,404, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:33,404, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:33,508, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:33,509, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:34,877, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:34,908, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:34,919, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:34,969, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:35,466, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:35,661, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:35,823, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:35,839, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:44,704, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:44,707, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:44,815, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:44,818, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:44,868, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:44,869, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:44,973, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:44,974, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:45,460, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:45,574, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:45,577, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:45,715, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:45,750, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:45,751, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:45,947, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:45,950, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:45,955, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:45,969, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:45,970, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:45,973, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:45,976, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:45,979, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:45,982, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:45,986, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:45,987, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:45,989, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:45,994, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:45,996, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:46,000, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:46,003, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:46,006, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:54:46,010, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:54:46,326, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:46,338, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:46,542, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:46,653, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:46,653, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:46,960, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:46,961, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:46,990, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:46,999, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:47,369, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:47,372, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:47,372, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:47,375, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:47,521, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:47,521, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:47,522, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:47,523, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:47,610, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:47,611, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:47,803, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:47,804, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:47,807, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:47,807, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:47,959, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:47,959, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:47,962, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:47,963, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:48,130, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:48,144, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:48,516, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:48,528, text_data_transformation, INFO, Tokenizing and removing stopwords ]
ercase ]
[2024-12-29 13:54:48,539, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:48,559, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:48,573, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:48,702, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:48,705, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:48,900, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:48,901, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:49,002, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:49,013, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:49,021, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:49,031, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:49,131, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:49,225, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:49,225, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:49,297, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:49,298, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:49,634, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:49,661, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:49,661, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:49,700, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:49,702, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:49,703, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:50,274, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:50,287, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:50,568, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:50,989, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:50,990, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:54:51,827, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:52,171, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:52,244, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:52,292, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:53,405, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:54:59,483, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:54:59,488, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:54:59,660, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:54:59,661, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:00,377, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:00,624, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:00,627, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:00,822, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:00,823, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:00,859, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:00,873, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:00,869, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:00,877, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:00,880, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:00,883, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:00,887, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:00,888, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:00,890, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:00,893, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:00,896, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:00,899, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:00,903, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:00,906, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:00,909, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:00,912, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:00,915, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:00,918, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:01,551, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:01,554, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:01,570, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:01,573, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:01,574, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:01,714, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:01,715, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:02,022, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:02,032, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:02,198, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:02,201, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:02,354, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:02,355, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:02,379, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:02,653, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:02,654, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:02,836, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:02,846, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:02,916, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:02,919, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:02,962, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:03,062, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:03,065, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:03,074, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:03,076, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:03,077, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:03,077, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:03,213, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:03,214, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:03,226, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:03,226, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:03,473, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:03,485, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:03,528, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:03,529, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:03,750, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:03,875, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:03,875, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:03,908, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:03,911, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:04,068, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:04,069, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:04,073, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:04,094, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:04,094, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:04,238, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:04,254, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:04,358, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:04,370, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:04,385, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:04,395, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:04,743, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:04,958, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:04,958, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:04,992, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:04,992, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:05,015, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:05,015, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:05,255, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:05,266, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:05,320, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:05,976, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:05,977, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:06,425, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:06,928, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:07,724, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:07,959, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:08,029, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:08,919, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:15,011, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:15,015, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:15,169, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:15,170, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:15,779, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:16,112, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:16,115, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:16,218, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:16,221, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:16,234, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:16,224, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:16,238, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:16,241, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:16,244, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:16,248, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:16,250, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:16,251, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:16,254, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:16,257, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:16,260, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:16,263, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:16,267, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:16,270, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:16,273, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:16,276, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:16,289, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:16,290, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:16,907, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:16,907, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:16,935, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:17,091, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:17,094, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:17,255, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:17,255, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:17,417, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:17,426, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:17,469, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:17,472, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:17,636, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:17,637, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:17,912, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:18,051, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:18,051, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:18,266, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:18,311, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:18,315, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:18,358, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:18,362, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:18,388, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:18,399, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:18,484, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:18,485, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:18,527, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:18,529, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:18,556, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:18,559, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:18,710, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:18,710, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:18,738, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:18,750, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:19,034, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:19,034, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:19,116, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:19,139, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:19,339, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:19,375, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:19,384, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:19,384, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:19,593, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:19,604, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:19,623, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:19,636, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:19,786, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:19,789, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:19,823, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:19,834, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:19,951, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:19,951, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:20,229, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:20,230, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:20,255, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:20,256, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:20,471, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:20,472, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:20,482, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:20,580, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:21,066, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:21,076, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:21,622, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:21,713, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:21,714, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:21,892, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:22,743, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:22,751, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:23,000, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:24,333, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:29,875, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:29,879, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:30,041, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:30,042, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:30,648, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:30,870, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:30,874, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:31,035, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:31,035, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:31,092, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:31,095, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:31,111, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:31,098, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:31,113, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:31,122, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:31,688, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:31,750, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:31,750, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:32,160, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:32,170, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:32,306, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:32,310, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:32,481, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:32,482, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:32,811, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:32,812, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:32,916, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:32,919, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:33,080, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:33,081, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:33,171, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:33,610, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:33,613, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:33,631, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:33,643, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:33,722, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:33,740, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:33,743, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:33,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:33,783, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:33,908, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:33,909, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:34,196, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:34,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:34,212, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:34,215, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:34,251, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:34,310, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:34,310, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:34,377, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:34,378, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:34,427, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:34,501, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:34,849, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:34,849, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:34,903, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:34,916, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:34,969, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:34,982, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:35,038, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:35,279, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:35,528, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:35,538, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:35,544, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:35,544, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:35,605, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:35,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:35,646, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:35,649, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:35,811, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:35,812, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:36,185, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:36,185, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:36,446, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:36,924, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:36,927, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:36,938, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:37,362, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:37,595, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:37,595, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:38,177, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:38,185, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:38,849, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:40,412, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:44,898, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:44,901, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:45,078, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:45,078, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:45,649, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:45,653, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:45,730, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:45,817, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:45,817, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:46,213, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:46,223, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:46,449, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:46,860, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:46,861, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:46,898, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:46,910, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:47,543, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:47,544, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:47,996, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:47,998, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:48,108, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:48,111, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:48,153, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:48,154, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:48,269, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:48,269, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:48,343, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:48,346, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:48,525, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:48,526, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:48,527, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:48,536, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:48,752, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:48,752, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:48,829, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:49,086, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:49,252, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:49,416, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:49,424, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:49,427, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:49,596, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:49,597, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:49,619, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:49,941, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:49,974, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:50,380, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:50,381, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:50,485, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:50,486, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:50,786, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:54,348, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:54,350, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:54,420, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:54,421, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:54,513, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:54,515, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:54,584, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:54,584, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:54,678, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:54,838, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:55:55,025, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:55:55,027, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:55:55,030, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:55:55,037, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:55:55,389, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:55:55,390, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:55:56,650, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:56:02,086, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:56:02,094, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:56:02,451, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:56:02,452, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:56:03,683, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:56:23,930, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:56:23,940, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:56:24,353, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:56:24,354, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:56:26,880, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:58:58,943, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,945, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,951, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,954, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,957, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,960, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,962, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,965, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,968, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,971, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,975, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,978, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,980, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,984, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,987, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,989, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,991, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:58,994, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:58,998, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:59,009, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:59,019, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:58:59,019, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:59,026, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:58:59,029, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:58:59,035, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:58:59,042, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:58:59,052, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:58:59,031, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:59,055, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:58:59,063, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:58:59,069, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:58:59,077, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:58:59,081, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:58:59,058, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:59,083, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:59,091, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:58:59,089, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:59,093, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:59,097, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:58:59,106, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:58:59,117, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:58:59,095, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:59,122, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:59,124, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:59,126, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:58:59,127, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:59,130, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:59,157, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:59,160, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:58:59,162, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:58:59,613, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:58:59,614, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:58:59,626, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:58:59,627, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:58:59,630, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:58:59,631, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:58:59,642, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:58:59,642, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:58:59,681, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:58:59,682, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:58:59,685, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:58:59,686, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:58:59,688, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:58:59,689, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:58:59,700, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:58:59,701, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:02,173, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:02,180, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:02,226, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:02,235, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:02,238, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:02,240, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:02,266, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:02,323, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:12,307, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:12,309, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:12,457, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:12,457, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:12,574, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:12,575, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:12,577, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:12,578, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:12,596, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:12,599, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:12,608, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:12,611, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:12,724, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:12,725, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:12,725, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:12,726, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:12,747, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:12,748, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:12,750, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:12,751, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:12,928, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:12,932, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:12,954, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:12,957, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:13,044, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:13,085, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:13,086, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:13,115, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:13,116, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:13,129, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:13,132, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:13,280, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:13,282, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:13,311, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:13,316, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:13,332, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:13,339, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:13,467, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:13,469, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:13,485, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:13,484, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:13,487, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:13,490, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:13,492, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:13,495, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:13,498, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:13,500, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:13,501, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:13,503, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:13,506, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:13,509, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:13,512, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:13,515, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:13,518, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:13,520, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:13,706, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:13,738, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:13,754, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:13,765, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:13,765, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:13,776, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:13,802, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:13,812, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:13,816, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:13,826, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:13,899, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:14,093, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:14,104, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:14,111, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:14,111, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:14,123, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:14,134, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:14,286, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:14,297, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:14,396, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:14,397, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:14,397, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:14,407, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:14,431, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:14,432, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:14,471, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:14,472, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:14,773, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:14,774, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:14,801, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:14,801, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:14,948, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:14,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:16,603, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:16,840, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:16,855, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:16,931, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:16,958, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:17,146, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:17,221, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:17,308, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:26,791, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:26,794, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:26,834, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:26,837, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:26,936, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:26,939, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:26,942, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:26,943, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:26,983, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:26,984, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:27,042, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:27,045, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:27,090, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:27,091, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:27,124, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:27,126, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:27,193, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:27,196, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:27,208, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:27,209, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:27,220, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:27,223, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:27,278, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:27,279, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:27,365, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:27,365, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:27,384, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:27,385, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:27,555, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:27,585, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:27,628, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:27,631, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:27,733, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:27,786, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:27,787, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:27,868, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:27,883, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:27,984, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:28,007, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:28,023, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:28,010, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:28,033, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:28,033, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:28,024, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:28,038, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:28,039, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:28,041, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:28,044, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:28,047, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:28,049, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:28,052, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:28,054, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:28,055, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:28,057, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:28,060, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:28,063, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:28,066, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:28,068, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:28,071, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:28,197, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:28,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:28,342, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:28,354, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:28,354, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:28,365, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:28,403, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:28,419, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:28,429, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:28,504, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:28,514, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:28,629, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:28,630, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:28,662, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:28,663, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:28,779, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:28,788, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:28,805, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:28,806, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:28,972, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:28,972, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:28,974, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:28,974, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:29,031, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:29,032, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:29,129, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:29,130, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:29,396, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:29,397, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:31,012, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:31,081, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:31,248, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:31,369, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:31,432, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:31,436, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:31,538, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:31,783, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:41,223, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:41,228, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:41,257, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:41,261, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:41,412, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:41,413, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:41,473, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:41,474, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:41,537, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:41,542, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:41,674, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:41,677, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:41,725, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:41,726, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:41,823, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:41,827, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:41,865, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:41,868, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:41,901, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:41,901, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:42,040, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:42,041, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:42,041, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:42,042, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:42,131, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:42,156, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:42,159, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:42,170, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:42,173, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:42,325, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:42,351, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:42,352, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:42,407, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:42,408, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:42,437, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:42,733, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:42,736, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:42,739, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:42,742, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:42,746, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:42,750, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:42,766, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:42,767, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:42,770, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:42,774, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:42,776, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:42,780, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:42,784, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:42,787, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:42,790, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:42,792, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:42,796, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:42,798, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:42,873, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:42,909, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:42,988, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:43,000, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:43,003, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:43,149, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:43,158, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:43,201, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:43,382, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:43,417, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:43,428, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:43,525, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:43,538, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:43,547, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:43,557, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:43,614, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:43,615, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:43,704, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:43,705, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:43,757, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:43,768, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:43,977, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:43,978, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:44,007, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:44,020, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:44,147, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:44,148, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:44,288, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:44,289, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:44,334, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:44,334, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:44,483, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:44,484, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:44,776, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:44,777, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:46,315, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:46,690, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:46,809, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:46,948, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:47,001, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:47,175, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:47,555, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:47,590, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:55,732, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:55,734, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:55,895, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:55,896, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:55,961, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:55,964, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:56,131, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:56,132, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:56,532, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:56,627, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:56,630, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:56,765, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:56,789, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:56,790, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:56,900, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:56,914, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:56,903, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:56,918, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:56,921, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:56,925, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:56,929, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:56,932, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:56,935, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:56,938, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:56,940, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:56,944, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:56,950, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:56,949, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:56,952, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:56,954, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:56,957, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 13:59:56,960, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 13:59:57,185, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:57,196, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:57,286, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:57,290, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:57,409, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:57,412, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:57,484, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:57,485, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:57,515, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:57,518, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:57,559, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:57,574, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:57,575, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:57,672, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:57,673, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:57,744, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:57,744, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:57,836, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:57,838, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:57,933, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:57,934, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:58,003, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:58,004, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:58,039, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:58,050, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:58,118, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:58,197, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:58,204, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:58,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:58,313, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:58,371, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:58,372, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:58,599, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:58,609, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:58,634, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:58,681, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:58,690, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:58,703, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:58,704, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:58,806, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:58,817, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:59,060, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 13:59:59,121, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:59,132, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:59,255, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:59,256, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:59,330, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:59,330, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:59,473, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:59,473, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 13:59:59,595, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 13:59:59,606, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 13:59:59,793, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 13:59:59,794, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:00,295, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:00,296, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:00,376, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:00,487, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:01,219, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:01,742, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:01,847, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:02,054, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:02,329, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:02,920, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:08,905, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:08,909, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:08,973, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:08,977, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:09,080, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:09,081, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:09,134, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:09,135, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:09,649, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:09,652, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:09,756, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:09,783, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:09,815, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:09,816, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:10,133, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:00:10,136, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:00:10,139, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:00:10,154, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:00:10,157, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:10,171, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:10,185, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:10,196, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:10,467, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:10,822, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:10,823, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:10,838, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:10,848, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:10,886, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:10,886, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:11,516, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:11,516, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:12,702, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:12,705, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:12,826, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:12,829, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:12,863, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:12,864, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:12,993, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:12,994, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:13,265, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:13,268, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:13,381, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:13,408, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:13,411, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:13,434, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:13,435, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:13,492, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:13,500, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:13,581, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:13,582, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:13,620, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:14,041, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:14,055, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:14,101, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:14,171, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:14,185, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:14,255, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:14,469, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:14,533, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:14,537, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:14,758, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:14,759, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:14,777, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:14,778, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:14,866, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:14,866, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:14,964, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:14,976, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:15,103, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:15,115, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:15,493, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:15,637, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:15,637, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:15,794, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:15,795, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:16,015, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:16,026, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:16,701, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:16,702, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:17,437, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:17,610, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:18,584, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:18,699, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:19,688, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:24,784, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:24,788, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:24,950, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:24,951, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:25,358, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:25,362, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:25,526, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:25,526, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:25,596, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:25,781, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:25,784, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:25,944, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:25,945, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:26,058, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:26,069, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:26,183, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:26,626, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:26,669, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:26,679, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:26,719, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:26,719, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:27,318, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:27,318, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:28,501, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:28,503, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:28,662, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:28,662, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:28,756, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:28,758, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:28,908, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:28,909, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:29,050, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:29,228, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:29,231, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:29,272, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:29,311, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:29,314, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:29,386, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:29,387, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:29,444, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:29,471, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:29,472, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:29,702, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:29,939, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:29,983, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:30,139, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:30,142, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:30,274, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:30,275, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:30,615, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:34,793, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:34,795, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:34,878, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:34,878, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:35,122, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:35,124, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:35,160, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:35,198, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:35,199, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:35,465, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:35,680, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:00:35,682, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:00:35,684, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:35,692, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:36,104, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:36,105, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:37,644, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:00:44,961, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:00:44,971, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:00:45,361, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:00:45,363, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:00:46,809, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:01:56,071, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,073, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,079, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,082, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,085, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,088, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,091, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,095, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,098, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,100, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,104, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,106, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,108, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,112, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,115, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,118, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,120, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,124, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,127, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,132, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,150, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:01:56,143, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,156, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:01:56,162, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:01:56,170, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:01:56,171, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:01:56,180, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:01:56,163, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,186, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:01:56,195, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:01:56,197, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:01:56,205, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:01:56,209, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:01:56,218, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:01:56,221, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:01:56,229, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:01:56,186, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,241, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:01:56,241, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,248, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,250, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,275, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,278, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,281, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,283, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,302, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,305, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,307, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:01:56,309, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:01:56,766, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:01:56,767, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:01:56,767, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:01:56,767, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:01:56,770, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:01:56,771, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:01:56,795, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:01:56,796, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:01:56,834, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:01:56,835, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:01:56,837, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:01:56,838, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:01:56,841, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:01:56,842, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:01:56,864, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:01:56,865, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:01:59,308, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:01:59,336, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:01:59,347, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:01:59,378, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:01:59,544, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:01:59,552, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:01:59,579, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:01:59,603, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:10,153, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:10,160, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:10,161, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:10,164, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:10,334, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:10,335, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:10,383, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:10,384, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:11,041, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:11,045, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:11,117, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:11,120, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:11,157, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:11,209, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:11,210, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:11,223, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:11,245, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:11,248, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:11,279, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:11,280, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:11,324, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:11,328, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:11,385, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:11,388, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:11,413, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:11,414, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:11,496, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:11,497, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:11,557, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:11,559, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:11,632, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:11,635, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:11,650, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:11,651, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:11,653, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:11,656, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:11,659, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:11,663, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:11,663, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:11,665, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:11,668, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:11,670, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:11,673, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:11,676, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:11,680, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:11,682, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:11,685, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:11,688, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:11,736, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:11,750, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:11,900, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:11,930, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:12,040, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:12,044, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:12,088, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:12,148, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:12,198, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:12,199, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:12,216, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:12,311, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:12,312, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:12,392, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:12,402, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:12,414, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:12,415, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:12,431, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:12,443, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:12,532, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:12,543, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:12,613, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:12,623, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:12,664, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:12,676, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:12,843, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:13,121, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:13,122, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:13,128, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:13,129, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:13,298, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:13,299, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:13,336, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:13,337, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:13,406, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:13,409, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:13,410, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:13,416, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:14,068, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:14,068, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:15,134, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:15,575, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:16,113, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:16,139, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:16,272, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:16,274, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:16,382, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:16,985, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:26,270, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:26,274, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:26,437, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:26,438, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:26,774, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:26,777, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:26,896, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:26,899, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:26,900, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:26,903, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:26,936, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:26,937, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:26,938, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:26,941, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:27,066, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:27,067, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:27,088, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:27,090, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:27,112, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:27,116, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:27,151, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:27,163, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:27,164, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:27,165, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:27,169, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:27,293, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:27,294, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:27,344, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:27,345, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:27,380, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:27,383, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:27,545, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:27,546, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:27,621, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:27,658, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:27,660, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:27,663, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:27,677, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:27,678, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:27,681, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:27,684, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:27,687, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:27,691, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:27,690, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:27,693, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:27,696, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:27,699, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:27,701, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:27,704, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:27,707, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:27,710, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:27,713, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:27,722, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:27,774, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:27,906, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:27,998, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:28,013, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:28,165, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:28,174, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:28,242, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:28,253, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:28,324, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:28,358, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:28,369, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:28,392, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:28,402, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:28,447, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:28,448, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:28,474, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:28,488, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:28,497, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:28,499, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:28,807, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:28,817, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:28,829, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:28,830, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:28,880, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:28,881, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:29,014, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:29,014, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:29,043, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:29,045, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:29,131, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:29,131, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:29,136, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:29,137, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:29,483, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:29,484, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:30,959, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:31,346, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:31,382, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:31,548, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:31,565, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:31,589, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:31,629, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:32,037, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:42,541, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:42,544, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:42,616, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:42,619, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:42,698, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:42,701, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:42,707, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:42,707, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:42,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:42,784, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:42,867, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:42,868, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:42,879, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:42,882, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:43,045, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:43,045, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:43,208, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:43,210, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:43,356, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:43,375, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:43,376, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:43,454, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:43,491, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:43,549, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:43,551, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:43,561, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:43,564, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:43,667, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:43,670, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:43,689, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:43,710, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:43,711, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:43,729, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:43,730, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:43,824, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:43,826, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:43,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:43,834, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:43,842, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:43,829, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:43,845, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:43,848, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:43,851, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:43,855, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:43,859, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:43,863, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:43,863, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:43,865, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:43,869, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:43,872, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:43,876, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:43,879, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:43,882, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:43,885, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:44,000, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:44,011, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:44,018, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:44,029, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:44,030, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:44,254, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:44,266, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:44,395, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:44,435, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:44,533, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:44,540, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:44,543, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:44,554, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:44,555, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:44,654, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:44,655, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:44,671, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:44,671, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:44,888, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:44,900, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:44,911, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:44,912, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:44,924, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:44,935, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:45,050, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:45,060, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:45,172, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:45,172, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:45,569, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:45,571, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:45,599, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:45,600, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:45,748, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:45,748, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:47,087, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:47,167, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:47,267, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:47,564, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:47,747, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:48,107, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:48,109, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:48,399, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:57,167, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:57,171, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:57,341, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:57,342, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:57,388, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:57,391, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:57,554, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:57,555, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:57,590, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:57,593, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:57,720, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:57,723, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:57,743, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:57,746, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:57,750, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:57,752, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:57,894, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:57,895, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:57,912, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:57,913, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:57,994, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:58,179, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:58,182, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:58,208, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:58,345, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:58,346, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:58,385, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:58,470, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:58,485, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:58,473, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:58,489, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:58,491, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:58,495, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:58,498, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:58,501, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:58,501, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:58,505, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:58,508, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:58,510, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:58,513, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:58,516, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:58,520, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:58,524, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:58,527, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:02:58,530, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:02:58,589, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:58,595, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:58,746, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:58,757, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:58,903, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:58,914, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:58,998, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:02:59,063, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:59,074, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:59,116, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:59,126, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:59,177, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:59,178, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:59,261, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:59,264, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:59,401, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:59,401, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:59,428, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:59,429, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:59,487, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:59,498, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:02:59,552, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:59,552, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:59,744, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:59,745, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:59,789, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:02:59,790, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:02:59,875, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:02:59,878, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:00,064, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:00,065, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:00,076, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:00,176, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:00,176, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:00,556, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:00,568, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:00,748, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:01,211, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:01,212, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:01,282, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:01,293, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:01,775, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:01,975, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:01,976, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:02,005, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:02,195, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:02,343, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:02,358, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:02,779, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:03,756, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:04,681, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:12,914, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:12,919, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:12,973, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:12,976, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:13,081, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:13,081, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:13,085, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:13,088, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:13,138, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:13,139, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:13,181, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:13,184, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:13,248, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:13,250, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:13,331, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:13,334, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:13,337, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:13,338, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:13,497, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:13,498, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:13,737, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:13,762, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:13,765, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:13,787, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:13,923, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:13,924, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:13,924, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:13,984, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:14,173, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:14,205, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:03:14,208, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:03:14,221, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:14,211, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:03:14,226, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:03:14,237, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:14,271, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:14,282, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:14,373, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:14,376, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:14,433, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:14,444, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:14,483, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:14,494, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:14,557, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:14,558, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:14,584, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:14,684, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:14,695, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:14,910, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:14,911, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:14,956, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:14,956, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:15,057, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:15,069, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:15,104, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:15,105, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:15,146, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:15,147, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:15,230, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:15,343, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:15,344, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:15,646, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:15,650, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:15,725, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:15,725, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:15,746, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:15,759, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:15,809, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:15,810, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:16,416, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:16,417, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:16,452, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:16,947, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:16,958, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:17,546, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:17,558, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:17,687, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:17,687, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:17,726, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:17,743, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:17,916, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:18,329, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:18,982, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:20,406, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:28,003, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:28,005, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:28,028, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:28,032, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:28,171, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:28,171, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:28,201, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:28,202, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:28,261, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:28,266, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:28,431, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:28,432, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:28,503, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:28,505, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:28,560, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:28,564, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:28,670, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:28,671, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:28,735, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:28,736, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:28,804, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:28,873, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:29,095, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:29,147, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:29,151, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:29,285, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:29,295, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:29,312, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:29,312, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:29,313, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:29,367, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:29,379, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:29,389, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:29,696, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:29,699, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:29,856, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:29,857, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:29,946, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:29,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:29,983, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:30,030, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:30,030, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:30,562, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:31,177, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:31,180, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:31,284, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:31,285, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:31,700, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:32,224, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:32,410, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:39,751, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:39,754, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:39,795, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:39,798, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:39,841, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:39,842, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:39,897, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:39,897, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:40,263, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:40,322, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:40,570, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:03:40,572, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:03:40,575, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:40,585, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:41,010, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:41,012, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:42,484, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:03:48,903, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:03:48,916, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:03:49,372, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:03:49,374, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:03:50,890, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:13:52,330, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,333, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,340, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,343, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,347, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,349, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,352, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,355, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,357, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,360, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,363, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,366, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,370, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,373, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,375, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,378, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,381, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,384, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,455, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,459, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,463, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,471, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,475, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,478, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,481, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,484, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,488, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,491, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,494, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,497, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,500, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,503, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:13:52,508, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:13:52,511, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:01,024, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:01,038, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:01,038, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:01,051, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:01,074, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:01,083, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:01,085, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:01,093, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:01,096, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:01,098, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:01,134, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:01,146, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:01,250, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:01,261, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:01,274, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:01,286, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:01,643, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:01,644, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:01,650, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:01,650, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:01,697, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:01,699, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:01,699, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:01,700, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:01,704, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:01,704, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:01,749, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:01,749, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:01,863, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:01,864, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:01,892, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:01,893, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:13,375, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:13,426, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:13,529, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:13,681, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:13,769, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:14,039, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:14,043, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:14,167, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:24,586, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:24,589, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:24,592, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:24,594, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:24,598, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:24,601, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:24,604, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:24,607, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:24,611, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:24,614, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:24,617, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:24,620, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:24,623, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:24,626, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:24,630, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:24,633, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:24,723, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:24,735, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:24,819, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:24,829, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:25,206, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:25,218, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:25,231, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:25,242, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:25,361, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:25,363, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:25,453, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:25,463, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:25,479, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:25,480, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:25,878, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:25,879, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:25,905, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:25,906, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:26,143, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:26,144, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:26,436, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:26,447, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:27,021, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:27,033, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:27,095, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:27,096, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:27,405, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:27,417, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:27,694, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:27,695, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:27,898, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:28,078, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:28,079, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:28,079, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:28,465, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:28,475, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:28,692, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:29,608, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:30,244, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:30,660, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:39,038, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:39,041, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:39,045, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:39,059, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:39,063, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:39,066, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:39,070, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:39,073, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:39,079, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:39,077, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:39,081, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:39,084, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:39,087, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:39,090, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:39,093, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:39,097, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:39,098, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:39,100, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:39,309, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:39,320, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:39,469, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:39,481, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:39,505, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:39,516, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:39,580, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:39,590, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:39,748, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:39,749, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:39,995, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:39,996, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:40,117, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:40,117, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:40,152, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:40,152, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:40,241, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:40,241, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:40,669, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:40,680, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:41,148, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:41,158, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:41,338, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:41,339, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:41,828, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:41,839, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:41,843, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:41,844, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:42,357, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:42,518, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:42,518, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:42,613, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:42,744, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:42,749, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:42,862, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:43,917, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:44,438, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:45,133, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:53,004, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:53,007, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:53,021, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:53,011, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:53,026, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:53,029, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:53,032, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:53,036, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:53,039, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:53,042, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:53,046, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:53,049, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:53,052, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:53,056, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:53,055, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:53,058, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:53,063, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:14:53,066, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:14:53,253, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:53,264, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:53,366, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:53,376, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:53,437, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:53,448, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:53,570, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:53,582, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:53,732, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:53,733, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:53,918, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:53,919, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:54,023, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:54,024, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:54,089, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:54,090, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:54,226, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:54,227, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:54,603, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:54,618, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:55,136, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:55,147, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:55,285, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:55,286, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:55,740, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:14:55,751, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:14:55,822, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:55,822, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:56,368, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:56,399, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:14:56,400, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:14:56,482, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:56,643, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:56,648, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:56,761, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:57,978, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:58,438, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:14:59,065, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:06,770, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:06,773, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:06,789, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:06,792, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:06,796, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:06,800, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:06,800, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:06,803, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:06,806, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:06,810, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:06,814, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:06,813, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:06,817, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:06,820, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:06,823, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:06,826, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:06,830, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:06,833, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:06,918, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:06,936, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:06,938, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:06,949, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:07,088, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:07,100, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:07,101, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:07,112, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:07,235, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:07,246, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:07,498, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:07,499, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:07,584, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:07,584, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:07,633, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:07,634, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:07,718, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:07,740, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:07,747, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:07,748, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:07,772, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:07,775, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:07,957, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:07,958, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:08,420, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:08,421, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:08,585, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:08,596, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:09,336, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:09,337, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:10,181, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:10,301, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:10,307, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:10,348, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:10,394, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:10,682, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:11,074, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:11,933, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:18,749, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:18,752, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:18,768, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:18,768, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:18,771, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:18,782, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:18,972, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:18,983, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:19,013, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:19,026, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:19,467, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:19,467, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:19,663, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:19,664, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:19,692, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:19,692, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:21,640, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:21,651, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:21,840, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:21,851, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:22,050, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:22,062, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:22,225, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:22,348, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:22,348, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:22,449, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:22,453, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:22,490, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:22,501, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:22,536, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:22,536, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:22,753, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:22,754, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:23,180, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:23,181, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:23,497, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:23,508, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:24,248, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:24,249, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:25,003, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:25,243, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:25,400, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:25,789, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:27,002, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:33,318, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:33,329, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:33,624, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:33,635, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:34,003, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:34,004, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:34,336, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:34,337, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:36,270, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:36,476, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:15:41,688, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:15:41,689, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:15:41,692, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:15:41,700, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:15:42,111, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:15:42,112, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:15:43,662, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:17:18,645, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,648, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,658, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,661, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,663, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,665, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,668, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,671, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,673, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,676, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,678, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,682, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,689, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,691, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,694, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,707, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,709, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,712, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,716, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,722, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,849, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,851, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,853, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,857, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,861, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,863, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,865, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,868, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,873, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,876, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:18,977, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:17:18,878, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:18,999, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:19,001, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:17:19,003, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:17:19,080, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:17:19,146, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:17:19,298, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:17:19,333, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:17:19,492, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:17:19,531, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:17:19,704, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:17:19,738, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:17:19,933, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:17:19,964, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:17:20,168, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:17:20,179, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:17:20,462, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:17:20,706, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:17:21,484, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:17:32,109, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:17:32,112, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:17:32,126, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:17:32,128, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:17:32,475, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:17:32,478, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:17:32,782, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:17:32,784, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:17:32,910, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:17:32,912, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:17:33,199, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:17:33,202, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:17:33,444, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:17:33,447, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:17:33,725, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:17:33,728, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:18:13,653, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:18:13,663, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:18:14,276, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:18:14,293, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:18:14,517, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:18:14,540, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:18:14,706, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:18:15,505, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:22:30,762, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:30,793, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:30,804, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:30,839, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:31,137, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:31,181, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:31,218, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:31,262, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:32,949, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:32,989, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:33,430, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:22:33,443, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:22:33,475, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:22:33,485, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:22:33,859, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:22:33,868, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:22:33,895, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:22:33,905, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:22:35,704, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:22:35,716, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:22:44,383, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:22:44,469, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:22:44,475, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:22:44,568, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:22:46,527, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:22:54,551, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:54,627, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:54,709, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:54,785, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:54,800, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:54,920, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:54,960, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:55,099, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:22:56,872, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:22:57,069, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:23:02,437, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:23:02,481, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:23:05,140, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:05,153, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:05,273, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:05,278, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:05,313, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:05,319, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:05,618, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:05,624, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:05,733, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:05,739, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:05,787, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:23:05,828, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:23:07,724, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:07,733, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:07,736, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:23:07,780, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:23:08,526, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:08,540, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:10,536, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:10,547, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:15,783, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:23:19,131, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:23:21,127, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:23:23,984, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:23:24,146, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:23:27,435, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:23:27,601, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:23:29,416, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:23:29,586, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:23:35,100, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:35,106, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:38,551, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:38,558, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:40,585, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:23:40,591, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:23:47,711, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:23:48,021, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:23:48,039, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:23:48,127, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:23:49,912, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:24:17,791, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:24:22,193, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:24:23,458, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:24:24,848, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:24:24,852, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:24:24,856, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:24:24,859, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:24:24,862, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:24:24,865, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:24:24,869, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:24:24,872, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:24:24,875, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:24:24,878, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:24:24,881, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:24:24,884, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:24:24,888, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:24:24,891, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:24:24,894, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:24:24,897, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:24:25,094, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:24:25,289, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:24:25,480, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:24:25,682, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:24:25,782, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:24:26,010, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:24:26,089, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:24:26,281, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:24:27,747, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:24:27,936, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:24:35,809, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:24:35,812, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:24:36,193, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:24:36,195, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:24:36,725, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:24:36,728, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:24:36,940, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:24:36,943, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:24:38,476, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:24:38,478, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:24:50,001, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:24:50,188, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:24:55,173, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:24:55,364, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:24:56,756, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:24:56,951, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:25:00,892, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:25:00,895, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:25:06,091, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:25:06,094, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:25:07,722, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:25:07,725, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:25:17,954, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:25:18,554, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:25:18,994, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:25:19,213, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:25:20,462, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:25:43,050, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:25:48,835, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:25:50,196, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:29:46,579, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:29:46,621, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:29:47,974, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:29:48,017, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:29:49,381, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:29:49,389, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:29:50,777, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:29:50,785, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:29:51,881, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:29:51,923, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:29:54,663, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:29:54,671, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:00,515, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:30:01,823, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:30:05,545, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:30:09,788, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:09,831, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:11,083, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:11,246, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:12,322, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:12,352, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:12,366, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:12,518, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:12,549, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:12,562, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:12,655, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:12,698, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:15,098, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:15,106, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:15,362, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:15,373, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:16,468, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:16,642, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:18,563, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:18,607, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:21,325, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:21,332, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:21,706, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:21,712, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:21,870, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:21,915, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:23,155, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:23,163, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:23,411, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:30:24,704, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:24,711, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:25,821, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:30:25,889, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:30:27,463, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:27,470, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:31,659, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:31,831, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:32,056, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:30:34,105, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:34,275, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:35,488, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:30:36,843, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:37,016, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:42,722, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:42,730, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:42,905, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:43,071, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:45,101, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:45,106, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:46,177, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:30:46,351, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:30:47,764, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:47,771, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:54,029, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:54,036, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:30:57,312, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:30:57,318, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:31:04,371, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:31:06,178, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:31:10,164, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:31:25,343, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:31:27,832, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:31:30,332, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:31:37,378, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:31:39,831, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:31:43,017, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:31:43,020, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:31:43,023, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:31:43,026, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:31:43,030, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:31:43,033, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:31:43,036, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:31:43,039, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:31:43,042, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:31:43,045, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:31:43,048, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:31:43,051, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:31:43,054, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:31:43,057, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:31:43,061, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:31:43,063, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:31:43,300, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:31:43,493, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:31:45,313, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:31:45,504, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:31:49,099, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:31:49,287, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:31:54,352, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:31:54,355, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:31:56,362, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:31:56,365, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:31:58,049, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:31:58,243, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:31:59,970, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:32:00,144, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:32:00,146, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:32:00,180, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:32:08,434, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:32:08,625, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:32:09,114, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:32:09,116, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:32:11,034, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:32:11,036, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:32:16,940, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:32:17,133, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:32:19,270, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:32:19,460, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:32:19,542, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:32:19,545, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:32:28,373, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:32:28,375, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:32:30,494, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:32:30,497, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:32:37,340, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:32:39,514, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:32:42,729, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:32:52,230, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:32:53,478, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:33:02,245, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:33:12,308, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:33:14,456, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:06,976, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:07,018, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:07,571, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:07,613, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:09,757, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:09,763, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:10,314, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:10,324, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:15,821, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:15,862, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:18,547, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:18,588, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:18,677, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:18,687, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:20,800, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:20,960, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:21,001, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:21,283, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:21,327, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:21,334, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:23,703, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:23,712, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:26,443, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:26,487, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:29,270, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:29,277, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:29,654, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:31,456, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:31,501, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:31,629, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:31,669, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:32,210, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:34,432, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:38,421, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:38,463, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:40,160, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:40,514, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:40,682, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:41,106, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:41,142, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:41,149, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:41,149, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:42,350, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:42,358, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:42,477, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:42,482, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:42,709, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:42,873, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:43,956, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:43,967, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:44,862, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:45,034, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:51,108, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:37:51,280, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:37:51,757, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:51,763, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:52,113, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:53,907, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:53,913, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:37:54,817, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:37:56,050, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:37:56,057, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:38:02,321, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:38:02,327, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:38:03,228, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:38:03,399, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:38:07,060, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:38:07,267, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:38:17,180, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:38:17,188, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:38:20,266, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:38:20,272, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:38:29,271, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:38:30,090, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:38:38,720, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:38:40,693, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:38:43,109, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:38:49,689, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:39:03,704, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:39:06,221, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:39:09,923, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:39:09,926, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:39:09,929, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:39:09,932, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:39:09,936, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:39:09,938, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:39:09,943, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:39:09,946, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:39:09,949, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:39:09,953, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:39:09,956, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:39:09,959, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:39:09,963, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:39:09,965, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:39:09,970, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:39:09,973, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:39:10,213, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:39:10,414, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:39:11,013, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:39:11,206, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:39:18,680, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:39:18,874, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:39:19,919, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:39:20,130, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:39:21,912, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:39:21,915, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:39:22,585, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:39:22,587, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:39:23,825, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:39:24,017, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:39:30,169, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:39:30,319, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:39:30,321, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:39:30,374, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:39:31,490, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:39:31,493, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:39:35,333, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:39:35,336, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:39:41,784, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:39:41,787, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:39:44,609, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:39:44,804, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:39:47,498, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:39:47,689, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:39:56,144, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:39:56,147, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:39:59,170, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:39:59,172, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:40:06,922, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:40:07,231, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:40:14,858, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:40:15,168, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:40:18,959, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:40:26,127, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:40:41,493, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:40:42,609, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:43:58,160, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:43:58,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:00,599, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:00,639, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:00,991, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:01,000, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:03,473, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:03,484, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:12,639, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:44:14,831, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:44:20,441, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:20,603, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:22,672, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:22,834, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:26,293, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:26,332, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:29,025, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:29,035, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:31,318, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:31,361, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:31,471, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:31,477, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:31,893, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:31,934, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:33,738, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:33,743, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:34,118, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:34,129, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:34,698, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:34,708, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:34,914, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:34,958, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:37,739, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:37,746, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:37,756, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:37,781, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:39,964, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:44:40,612, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:40,627, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:45,040, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:44:45,846, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:44:48,275, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:48,318, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:48,577, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:44:50,549, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:50,723, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:51,059, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:44:51,069, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:44:51,581, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:44:55,011, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:55,172, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:56,016, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:56,188, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:44:59,078, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:44:59,258, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:45:01,639, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:45:01,645, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:45:01,967, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:45:02,626, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:45:02,819, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:45:05,922, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:45:05,929, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:45:07,048, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:45:07,057, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:45:10,201, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:45:10,207, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:45:12,936, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:45:13,116, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:45:14,146, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:45:14,152, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:45:16,085, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:45:17,952, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:45:24,334, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:45:24,342, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:45:44,941, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:45:48,248, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:45:48,251, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:45:48,254, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:45:48,257, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:45:48,260, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:45:48,263, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:45:48,267, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:45:48,269, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:45:48,273, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:45:48,276, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:45:48,279, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:45:48,282, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:45:48,285, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:45:48,288, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:45:48,292, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:45:48,296, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:45:48,581, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:45:48,773, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:45:49,444, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:45:50,973, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:45:51,118, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:45:51,168, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:45:55,267, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:45:58,417, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:45:59,808, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:45:59,811, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:46:02,374, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:46:02,377, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:46:07,895, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:46:23,064, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:46:23,262, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:46:26,112, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:46:26,307, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:46:28,480, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:46:28,696, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:46:33,773, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:46:33,965, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:46:34,326, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:46:34,328, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:46:36,867, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:46:37,052, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:46:37,275, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:46:37,277, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:46:39,748, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:46:39,751, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:46:44,251, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:46:45,195, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:46:45,198, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:46:46,986, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:46:47,276, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:46:47,487, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:46:48,219, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:46:48,222, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:46:58,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:46:58,835, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:47:17,491, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:47:21,294, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:47:22,830, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:47:29,573, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:47:31,144, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:47:44,194, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:50:09,246, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:50:09,289, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:50:11,175, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:50:11,218, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:50:12,121, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:50:12,132, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:50:14,104, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:50:14,113, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:50:23,772, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:50:25,593, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:50:31,542, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:50:31,710, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:50:33,446, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:50:33,626, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:50:34,916, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:50:34,958, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:50:37,706, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:50:37,716, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:50:42,801, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:50:42,807, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:50:44,812, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:50:44,819, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:50:48,700, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:50:56,270, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:50:56,454, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:51:07,597, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:51:07,604, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:51:28,756, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:51:30,104, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:51:53,362, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:51:59,058, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:51:59,062, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:51:59,065, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 14:51:59,068, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 14:51:59,327, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:51:59,517, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:01,174, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:01,374, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:10,460, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:10,463, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:12,218, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:12,221, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:14,605, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:14,646, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:17,268, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:17,312, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:17,378, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:17,388, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:17,836, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:17,882, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:17,896, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:17,942, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:20,011, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:20,024, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:20,678, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:20,690, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:20,809, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:20,821, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:25,203, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:25,401, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:28,775, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:52:31,057, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:52:31,771, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:52:32,204, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:52:36,432, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:36,436, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:39,551, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:39,718, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:41,142, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:41,186, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:42,144, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:42,299, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:42,719, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:42,889, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:43,827, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:52:43,986, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:43,996, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:52:43,998, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:50,784, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:50,791, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:53,538, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:53,545, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:54,127, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:54,134, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:54,760, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:52:55,099, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:52:55,107, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:52:55,442, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:52:56,093, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:53:06,199, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:53:06,387, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:53:17,744, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:53:17,750, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:53:20,902, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:53:33,804, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:53:37,003, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:53:37,857, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:53:39,046, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:54:03,254, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:54:12,853, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:54:13,048, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:54:16,775, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:54:17,001, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:54:17,122, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:54:17,312, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:54:18,652, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:54:18,842, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:54:24,052, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:54:24,055, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:54:28,112, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:54:28,115, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:54:28,498, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:54:28,501, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:54:29,865, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:54:29,868, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:54:43,900, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:54:44,100, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:54:55,188, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:54:55,190, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:55:07,817, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:55:13,293, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:55:13,780, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:55:14,162, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:55:38,290, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:57:33,603, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:57:33,647, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:57:35,143, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:57:35,188, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:57:36,446, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:57:36,459, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:57:37,991, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:57:38,002, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:57:48,192, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:57:49,341, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:57:58,949, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:57:59,127, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:57:59,403, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:57:59,444, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:57:59,581, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:57:59,753, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:58:02,157, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:58:02,165, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:58:10,453, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:58:10,459, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:58:10,786, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:58:10,792, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:58:13,097, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:58:23,845, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:58:24,017, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:58:35,064, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:58:35,071, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:58:54,710, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:58:56,920, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:59:20,722, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:59:31,959, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:59:32,163, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:59:36,392, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:59:36,574, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:59:39,567, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:59:39,612, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:59:42,408, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:59:42,418, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:59:43,166, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:59:43,169, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:59:43,206, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:59:43,247, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:59:45,979, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:59:45,989, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:59:46,543, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:59:46,585, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:59:47,690, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:59:47,693, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:59:49,513, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:59:49,523, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:59:53,003, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 14:59:53,043, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 14:59:53,408, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 14:59:55,755, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 14:59:55,764, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 14:59:57,391, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:00:00,610, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:00:04,269, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:00:04,436, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:00:06,593, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:00:06,639, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:00:06,754, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:00:08,526, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:00:08,693, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:00:09,364, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:00:09,371, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:00:11,542, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:00:11,713, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:00:15,602, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:00:15,609, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:00:18,159, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:00:18,322, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:00:19,768, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:00:19,773, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:00:20,233, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:00:22,504, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:00:22,508, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:00:25,881, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:00:28,490, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:00:28,499, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:00:30,058, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:00:30,215, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:00:31,083, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:00:40,190, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:00:40,196, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:00:55,166, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:00:58,832, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:01:01,773, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:01:08,822, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:01:20,484, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:03:07,945, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:03:07,968, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:03:09,281, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:03:09,285, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:03:10,905, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:03:10,924, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:03:12,196, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:03:12,200, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:03:14,466, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:03:16,971, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:03:19,398, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:03:19,478, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:03:21,863, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:03:21,945, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:03:25,037, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:03:25,041, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:03:27,262, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:03:27,265, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:03:44,160, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:03:46,756, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:04:04,651, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:04:04,654, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:04:04,659, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:04:04,813, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:04:11,030, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:04:11,033, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:04:33,450, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:07:23,808, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:07:23,923, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:07:32,315, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:07:32,325, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:07:55,355, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:14:57,584, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:14:57,686, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:15:04,103, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:15:04,113, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:15:28,096, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:19:36,920, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,923, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:36,929, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,932, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:36,935, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,938, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:36,941, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,944, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:36,947, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,949, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:36,951, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,954, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:36,956, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,959, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:36,961, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,963, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:36,966, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:36,968, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:37,023, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:37,029, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:37,033, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:37,036, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:37,038, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:37,041, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:37,045, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:37,047, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:37,051, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:37,054, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:37,057, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:37,060, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:37,063, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:37,066, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:37,070, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:19:37,075, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:19:45,900, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:19:45,915, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:19:45,984, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:19:45,997, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:19:46,014, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:19:46,029, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:19:46,037, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:19:46,045, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:19:46,051, text_data_transformation, INFO, Tokenizing and removing stopwords ]
ercase ]
[2024-12-29 15:19:46,060, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:19:46,063, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:19:46,070, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:19:46,083, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:19:46,530, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:19:46,543, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:19:46,709, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:19:46,710, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:19:46,781, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:19:46,782, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:19:46,786, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:19:46,787, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:19:46,803, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:19:46,804, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:19:46,890, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:19:46,890, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:19:46,961, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:19:46,962, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:19:46,975, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:19:46,976, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:19:47,296, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:19:47,297, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:19:58,889, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:19:59,167, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:19:59,392, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:19:59,562, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:19:59,853, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:19:59,883, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:00,027, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:00,341, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:09,977, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:09,980, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:09,983, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:09,986, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:09,989, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:09,993, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:09,996, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:09,999, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:10,003, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:10,006, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:10,010, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:10,013, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:10,016, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:10,020, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:10,024, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:10,027, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:10,134, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:10,145, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:10,149, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:10,157, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:10,878, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:10,879, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:10,930, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:10,930, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:11,085, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:11,096, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:11,241, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:11,251, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:11,756, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:11,756, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:11,933, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:11,933, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:12,048, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:12,062, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:12,063, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:12,073, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:12,294, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:12,306, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:12,756, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:12,757, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:12,782, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:12,782, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:13,135, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:13,135, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:13,145, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:13,156, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:13,791, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:13,834, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:13,874, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:13,875, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:14,844, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:15,026, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:15,528, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:15,606, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:16,145, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:16,947, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:27,019, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:27,022, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:27,037, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:27,038, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:27,041, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:27,045, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:27,049, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:27,047, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:27,051, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:27,054, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:27,058, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:27,061, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:27,064, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:27,067, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:27,071, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:27,074, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:27,078, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:27,080, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:27,248, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:27,260, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:27,309, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:27,320, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:27,593, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:27,611, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:27,957, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:27,957, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:28,026, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:28,027, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:28,147, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:28,159, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:28,193, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:28,194, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:28,448, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:28,459, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:28,501, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:28,502, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:28,891, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:28,892, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:28,992, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:29,005, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:29,264, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:29,265, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:29,718, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:29,719, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:29,983, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:29,995, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:30,834, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:30,835, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:30,956, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:31,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:31,272, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:31,708, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:31,739, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:32,311, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:32,598, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:33,764, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:42,821, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:42,824, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:42,838, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:42,840, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:42,843, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:42,847, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:42,850, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:42,856, text_data_transformation, INFO, Tokenizing and removing stopwords ]
r English language ]
[2024-12-29 15:20:42,857, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:42,861, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:42,864, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:42,868, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:42,872, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:42,876, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:42,878, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:42,882, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:42,885, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:43,036, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:43,047, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:43,349, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:43,361, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:43,476, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:43,488, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:43,553, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:43,554, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:43,721, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:43,722, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:43,982, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:43,993, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:44,060, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:44,061, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:44,170, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:44,171, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:44,321, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:44,333, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:44,338, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:44,349, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:44,669, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:44,670, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:45,020, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:45,020, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:45,063, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:45,065, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:45,871, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:45,882, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:46,257, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:46,425, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:46,584, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:46,584, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:46,854, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:46,861, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:47,276, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:47,749, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:47,799, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:49,359, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:20:56,794, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:56,797, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:56,800, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:56,816, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:56,820, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:56,824, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:56,824, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:56,827, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:56,830, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:56,834, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:56,837, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:56,841, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:56,844, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:56,847, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:56,847, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:56,850, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:56,854, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:20:56,857, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:20:57,202, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:57,213, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:57,419, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:57,430, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:57,510, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:57,521, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:57,558, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:57,559, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:57,722, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:57,734, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:57,889, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:57,890, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:58,024, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:58,036, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:58,107, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:58,108, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:58,215, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:58,216, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:58,430, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:58,431, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:58,570, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:58,582, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:58,721, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:58,721, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:58,888, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:20:58,907, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:20:59,279, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:59,280, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:20:59,600, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:20:59,601, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:00,340, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:00,500, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:00,776, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:00,906, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:01,160, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:01,382, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:02,030, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:02,354, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:09,423, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:21:09,426, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:21:09,430, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:21:09,447, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:21:09,450, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:09,461, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:09,579, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:09,590, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:09,793, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:09,805, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:10,150, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:10,150, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:10,279, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:10,280, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:10,506, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:10,507, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:12,583, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:12,595, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:12,899, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:12,912, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:12,967, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:13,024, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:13,298, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:13,299, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:13,312, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:13,330, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:13,341, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:13,633, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:13,633, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:14,009, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:14,009, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:14,091, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:14,102, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:14,189, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:14,200, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:14,800, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:14,801, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:14,880, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:14,881, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:15,949, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:16,357, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:16,782, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:17,601, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:17,643, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:24,504, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:24,515, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:24,711, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:24,723, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:25,235, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:25,236, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:25,409, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:25,410, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:27,916, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:28,026, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:21:33,681, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:21:33,682, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:21:33,685, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:21:33,693, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:21:34,128, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:21:34,130, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:21:35,861, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:05,716, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,720, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,727, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,729, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,733, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,735, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,738, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,742, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,745, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,748, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,750, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,752, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,756, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,759, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,763, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,798, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,801, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,805, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,810, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,843, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:05,854, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:05,861, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:05,834, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,870, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:05,878, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:05,889, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:05,891, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:05,866, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,901, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:05,903, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:05,912, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:05,918, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:05,929, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:05,931, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:05,892, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,941, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:05,940, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,942, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:05,943, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,952, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:05,945, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,960, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,963, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,965, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,968, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,971, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,975, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,977, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:05,980, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:05,982, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:06,474, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:06,475, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:06,475, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:06,475, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:06,520, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:06,521, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:06,539, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:06,540, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:06,560, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:06,561, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:06,567, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:06,567, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:06,568, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:06,569, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:06,582, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:06,583, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:09,339, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:09,341, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:09,383, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:09,394, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:09,474, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:09,552, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:09,647, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:09,673, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:20,612, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:20,615, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:20,821, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:20,821, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:21,269, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:21,273, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:21,453, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:21,454, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:21,509, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:21,512, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:21,515, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:21,519, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:21,591, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:21,594, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:21,615, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:21,618, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:21,686, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:21,689, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:21,695, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:21,696, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:21,697, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:22:21,722, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:22:21.697161', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:21,760, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:21,761, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:21,769, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:21,772, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:21,794, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:21,795, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:21,817, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:21,828, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:21,924, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:21,924, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:22,025, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:22,026, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:22,110, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:22,113, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:22,234, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:22,246, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:22,295, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:22,295, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:22,428, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:22:22,450, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:22:22.428205', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:22,478, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:22,489, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:22,491, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:22,499, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:22,549, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:22,561, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:22,580, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:22,590, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:22,591, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:22,592, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:22,619, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:22,630, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:22,678, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:22:22,678, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:22:22,715, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:22:22.679534', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:22,717, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:22:22.678535', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:22,724, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:22,734, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:22,780, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:22:22,811, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:22,811, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:22,817, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:22:22.780263', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:22,817, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:22:22,822, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:22,822, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:22,851, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:22:22.817164', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:22,916, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:22,924, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:22:22,926, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:22,945, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:22,951, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:22:22.925873', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:22,956, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:23,003, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:23,026, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:23,050, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:23,063, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:23,214, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:22:23,308, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:23,308, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:23,578, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:22:23.215100', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:24,027, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:24,030, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:24,190, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:24,245, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:24,295, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:24,300, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:25,093, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:25,095, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:25,221, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:25,222, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:25,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:25,785, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:26,141, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:26,141, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:27,263, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:27,311, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:27,890, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:27,935, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:28,019, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:22:28,019, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:22:28.019248', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:28,043, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:28,092, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:28,129, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:28,180, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:28,307, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:28,329, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:28,350, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:28,380, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:28,419, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:28,422, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:28,425, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:28,440, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:28,444, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:28,447, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:28,450, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:28,453, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:28,456, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:28,458, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:28,460, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:28,461, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:28,464, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:28,467, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:28,471, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:28,475, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:28,478, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:28,478, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:28,486, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:28,568, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:28,664, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:22:28,665, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:22:28.665517', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:28,829, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:22:28,830, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:22:28.830077', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:28,863, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:28,909, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:28,923, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:22:28,923, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:22:28.923829', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:29,074, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:29,085, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:22:29,086, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:29,086, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:22:29.086392', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:29,099, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:22:29,099, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:22:29.099356', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:29,181, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:29,181, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:29,242, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:29,245, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:22:29,245, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:22:29.245964', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:29,254, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:29,354, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:29,364, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:29,509, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:29,522, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:29,539, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:29,552, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:29,586, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:22:29,586, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:22:29.586056', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:29,658, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:29,669, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:29,865, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:29,865, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:29,925, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:29,925, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:30,038, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:30,050, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:30,090, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:30,090, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:30,225, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:30,226, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:30,286, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:30,286, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:30,393, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:30,394, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:30,738, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:30,738, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:32,035, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:32,709, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:32,733, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:32,875, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:32,931, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:32,958, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:33,240, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:33,454, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:44,077, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:44,080, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:44,204, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:44,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:44,219, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:44,222, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:44,262, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:44,263, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:44,373, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:44,374, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:44,405, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:44,406, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:44,514, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:44,517, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:44,657, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:44,661, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:44,693, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:44,694, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:44,822, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:44,826, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:44,830, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:44,831, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:44,911, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:44,917, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:44,967, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:44,970, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:45,027, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:45,039, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:45,061, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:45,062, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:45,083, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:45,094, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:45,100, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:45,101, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:45,144, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:45,145, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:45,146, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:45,160, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:45,272, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:22:45,273, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:22:45.273082', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:45,353, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:22:45,353, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:22:45.353866', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:45,409, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:22:45,409, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:22:45.409717', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:45,411, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:45,423, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:45,463, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:45,474, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:45,542, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:45,558, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:45,579, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:45,591, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:45,604, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:45,615, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:45,796, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:22:45,797, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:22:45.797678', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:45,830, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:22:45,830, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:22:45.830590', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:45,873, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:45,878, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:45,889, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:45,891, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:45,910, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:45,921, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:45,960, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:45,972, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:45,974, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:45,989, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:46,073, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:22:46,073, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:22:46.073940', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:46,091, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:22:46,091, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:22:46.091891', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:46,150, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:22:46,151, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:22:46.151732', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:46,202, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:46,217, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:46,263, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:46,276, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:46,277, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:46,286, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:46,290, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:46,287, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:46,406, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:46,406, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:46,478, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:46,479, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:46,650, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:46,650, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:46,733, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:46,734, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:46,974, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:46,974, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:47,077, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:47,078, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:47,193, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:47,194, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:49,404, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:49,471, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:49,490, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:49,541, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:49,692, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:49,734, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:49,753, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:49,793, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:49,860, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:49,928, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:50,278, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:22:50,279, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:22:50.279686', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:50,507, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:22:50,507, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:22:50.507078', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:50,570, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:22:50,571, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:22:50.570908', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:50,586, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:50,652, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:50,664, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:22:50,665, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:22:50.665654', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:50,695, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:50,696, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:22:50,697, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:22:50.697569', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:50,722, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:50,743, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:50,777, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:22:50,782, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:50,786, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:50,789, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:50,808, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:50,811, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:50,813, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:50,813, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:50,818, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:50,820, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:50,825, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:50,827, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:50,831, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:50,835, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:50,838, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:50,839, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:50,842, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:50,846, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:22:50,850, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:22:51,130, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:51,155, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:51,157, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:51,168, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:51,197, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:51,205, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:51,209, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:51,222, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:51,537, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:22:51,538, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:22:51.538319', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:51,574, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:22:51,575, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:22:51.575220', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:51,689, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:51,689, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:51,732, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:22:51,733, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:22:51.732799', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:22:51,889, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:51,889, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:52,011, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:52,025, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:52,034, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:52,046, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:52,142, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:52,142, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:52,206, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:22:52,217, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:52,220, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:22:52,223, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:52,324, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:52,325, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:52,767, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:52,768, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:52,778, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:52,778, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:52,944, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:22:52,945, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:22:55,293, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:55,494, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:55,746, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:55,772, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:55,993, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:56,298, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:56,441, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:22:56,658, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:08,409, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:08,412, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:08,597, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:08,598, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:08,603, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:08,606, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:08,633, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:08,636, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:08,697, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:08,700, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:08,775, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:08,776, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:08,813, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:08,814, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:08,881, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:08,884, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:08,884, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:08,885, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:09,064, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:09,065, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:09,237, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:09,241, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:09,290, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:09,303, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:09,412, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:09,413, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:09,485, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:09,488, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:09,507, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:23:09,508, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:23:09.508239', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:09,558, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:09,572, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:09,611, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:09,628, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:09,638, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:09,652, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:09,653, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:09,665, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:09,717, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:09,718, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:09,782, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:23:09,782, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:23:09.782505', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:09,785, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:09,796, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:09,821, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:23:09,822, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:23:09.821402', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:09,857, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:23:09,857, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:23:09.857307', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:09,985, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:09,996, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:10,025, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:10,037, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:10,098, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:23:10,098, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:23:10.098660', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:10,124, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:10,128, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:10,137, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:10,144, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:10,219, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:10,238, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:10,286, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:10,300, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:10,412, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:10,424, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:10,512, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:23:10,513, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:23:10.512552', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:10,544, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:10,544, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:10,654, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:10,668, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:10,817, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:10,835, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:10,856, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:10,856, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:10,936, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:10,936, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:11,092, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:23:11,092, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:23:11.092001', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:11,134, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:11,135, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:11,191, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:11,192, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:11,217, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:11,231, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:11,489, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:11,502, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:11,531, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:11,532, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:11,695, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:23:11,695, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:23:11.695387', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:11,849, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:11,868, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:12,024, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:12,025, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:12,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:12,668, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:13,906, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:13,958, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:14,157, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:14,234, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:14,466, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:14,540, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:14,703, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:14,722, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:14,762, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:14,776, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:14,815, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:23:14,816, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:23:14.816038', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:14,971, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:15,021, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:15,320, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:15,323, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:15,327, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:15,339, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:23:15,340, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:23:15.340634', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:15,334, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:15,353, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:15,356, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:15,355, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:15,359, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:15,363, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:15,366, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:15,370, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:15,374, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:15,376, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:15,382, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:15,386, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:15,389, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:15,392, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:15,401, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:15,459, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:15,503, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:15,510, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:23:15,511, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:23:15.511178', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:15,827, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:23:15,827, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:23:15.827332', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:15,872, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:23:15,872, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:23:15.872212', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:16,001, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:16,007, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:23:16,007, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:23:16.007849', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:16,022, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:16,073, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:16,102, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:16,339, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:16,351, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:16,368, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:16,369, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:16,431, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:23:16,431, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:23:16.431715', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:16,485, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:16,501, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:16,523, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:16,537, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:16,625, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:16,680, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:16,863, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:16,864, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:16,874, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:16,885, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:16,936, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:16,937, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:17,066, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:17,066, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:17,180, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:17,180, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:17,243, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:17,244, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:17,415, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:23:17,415, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:23:17.415084', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:17,580, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:17,581, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:17,849, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:17,860, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:18,637, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:18,637, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:19,236, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:19,700, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:19,751, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:19,850, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:19,876, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:20,032, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:20,354, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:21,534, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:29,343, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:29,346, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:29,513, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:29,514, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:30,235, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:30,246, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:30,418, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:23:30,419, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:23:30.419289', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:30,523, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:30,533, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:30,744, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:30,747, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:30,817, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:30,821, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:30,881, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:30,885, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:30,919, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:30,921, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:30,922, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:30,922, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:30,995, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:30,996, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:30,999, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:31,002, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:31,056, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:31,057, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:31,062, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:31,065, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:31,099, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:31,100, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:31,130, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:31,133, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:31,172, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:31,173, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:31,200, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:31,201, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:31,239, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:31,240, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:31,312, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:31,312, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:31,630, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:31,641, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:31,684, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:31,695, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:31,812, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:31,812, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:23:31,813, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:23:31.813559', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:31,823, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:31,839, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:31,849, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:23:31,850, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:23:31.849463', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:31,850, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:31,900, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:31,911, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:31,914, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:31,922, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:31,926, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:31,934, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:31,946, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:31,957, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:31,994, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:23:31,994, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:23:31.994077', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:32,019, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:32,030, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:32,037, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:23:32,037, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:23:32.037959', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:32,081, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:23:32,082, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:23:32.082838', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:32,099, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:23:32,099, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:23:32.099794', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:32,099, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:32,111, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:32,147, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:32,158, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:32,182, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:32,195, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:32,203, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:23:32,203, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:23:32.203516', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:32,205, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:32,216, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:32,313, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:32,325, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:32,608, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:32,609, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:32,645, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:32,646, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:32,790, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:32,790, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:32,859, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:32,860, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:32,890, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:32,890, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:32,906, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:32,906, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:33,034, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:33,035, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:33,883, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:33,939, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:34,647, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:23:34,647, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:23:34.647976', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:35,031, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:35,034, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:35,051, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:35,037, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:35,053, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:35,056, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:35,059, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:35,062, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:35,065, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:35,065, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:35,068, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:35,071, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:35,074, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:35,077, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:35,081, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:35,084, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:35,087, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:35,090, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:35,312, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:35,366, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:35,379, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:35,422, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:35,656, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:35,702, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:35,709, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:35,753, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:35,760, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:35,764, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:35,767, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:35,768, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:35,803, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:35,809, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:35,916, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:35,971, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:36,034, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:23:36,034, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:23:36.034266', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:36,104, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:23:36,104, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:23:36.104080', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:36,420, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:23:36,420, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:23:36.420234', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:36,447, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:36,456, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:23:36,456, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:23:36.456137', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:36,459, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:36,497, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:23:36,497, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:23:36.497029', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:36,500, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:23:36,500, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:23:36.500021', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:36,517, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:36,529, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:36,693, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:23:36,694, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:23:36.694501', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:36,872, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:36,882, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:36,896, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:36,909, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:36,921, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:36,933, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:36,944, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:36,955, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:37,159, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:37,171, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:37,178, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:37,179, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:37,258, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:37,259, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:37,577, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:37,578, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:37,615, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:37,616, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:37,643, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:37,644, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:37,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:37,667, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:37,856, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:37,857, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:38,572, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:39,862, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:40,004, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:40,325, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:40,463, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:40,482, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:40,489, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:40,653, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:48,000, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:48,003, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:48,243, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:48,245, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:49,120, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:49,133, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:49,315, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:23:49,316, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:23:49.316729', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:49,373, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:49,377, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:49,414, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:49,425, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:49,523, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:49,527, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:49,564, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:49,565, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:49,700, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:49,701, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:50,156, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:50,157, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:50,329, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:50,340, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:50,405, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:50,418, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:50,606, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:23:50,607, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:23:50.607275', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:50,650, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:23:50,650, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:23:50.650161', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:50,741, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:50,752, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:50,776, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:50,789, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:51,639, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:51,640, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:51,723, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:51,725, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:53,132, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:53,139, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:53,331, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:53,332, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:53,591, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:53,636, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:53,642, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:53,645, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:53,817, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:53,818, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:53,978, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:53,981, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:54,064, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:54,078, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:54,158, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:54,162, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:54,199, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:54,202, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:54,208, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:54,209, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:54,313, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:23:54,314, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:23:54.313360', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:54,400, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:54,400, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:54,520, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:23:54,520, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:23:54.520804', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:54,529, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:54,555, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:54,558, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:54,561, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:54,858, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:54,869, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:54,972, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:54,976, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:54,990, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:54,979, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:23:54,998, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:54,998, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:23:55,004, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:55,047, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:55,051, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:23:55,052, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:23:55.052382', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:55,162, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:55,176, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:55,177, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:55,178, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:55,187, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:55,189, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:55,286, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:55,295, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:55,305, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:55,307, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:55,307, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:55,330, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:55,366, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:23:55,367, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:23:55.367538', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:55,381, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:23:55,382, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:23:55.382499', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:55,470, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:55,482, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:55,493, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:23:55,493, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:23:55.493203', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:55,496, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:55,507, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:55,592, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:55,603, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:55,702, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:55,703, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:55,786, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:23:55,787, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:23:55.786418', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:55,872, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:55,873, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:56,043, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:23:56,043, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:23:56.043729', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:56,180, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:56,181, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:56,186, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:56,186, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:56,207, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:56,218, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:56,309, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:56,309, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:56,458, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:56,469, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:56,915, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:56,916, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:57,170, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:23:57,170, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:23:58,164, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:58,221, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:58,667, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:58,982, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:23:58,982, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:58,982, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:23:58.982865', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:59,030, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:59,092, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:59,137, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:59,331, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:59,377, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:59,435, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:23:59,448, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:23:59,504, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:23:59,553, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:23:59,723, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:23:59,724, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:23:59.724881', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:23:59,811, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:23:59,811, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:23:59.811648', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:00,015, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:00,067, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:24:00,067, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:24:00.067962', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:00,148, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:00,149, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:00,150, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:00,163, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:00,176, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:00,230, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:00,242, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:00,269, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:24:00,269, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:24:00.269424', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:00,500, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:00,511, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:00,738, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:00,751, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:00,866, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:00,866, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:00,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:00,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:01,202, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:01,203, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:01,486, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:01,486, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:02,952, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:03,665, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:03,701, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:03,921, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:04,410, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:10,552, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:10,555, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:10,739, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:10,740, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:11,459, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:11,469, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:11,639, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:24:11,639, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:24:11.639004', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:11,748, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:11,759, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:11,841, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:11,844, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:11,982, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:11,986, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:12,023, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:12,024, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:12,168, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:12,169, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:12,430, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:12,431, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:12,702, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:12,713, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:12,876, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:24:12,877, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:24:12.877689', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:12,911, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:12,922, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:12,974, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:12,986, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:13,095, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:24:13,096, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:24:13.096105', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:13,196, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:13,206, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:13,687, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:13,688, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:13,924, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:13,924, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:14,592, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:14,595, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:14,772, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:14,773, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:15,006, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:15,009, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:15,074, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:15,077, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:15,181, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:15,182, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:15,197, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:15,243, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:15,244, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:15,245, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:15,391, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:15,394, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:15,475, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:15,494, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:15,564, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:15,565, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:15,670, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:24:15,670, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:24:15.670217', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:15,774, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:15,785, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:15,897, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:15,908, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:15,908, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:15,919, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:15,932, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:24:15,933, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:24:15.933513', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:16,047, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:16,051, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:16,076, dictionary, INFO, built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions) ]
[2024-12-29 15:24:16,076, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7259 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 1125 documents (total 51758 corpus positions)", 'datetime': '2024-12-29T15:24:16.076131', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:16,085, dictionary, INFO, built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions) ]
[2024-12-29 15:24:16,086, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7507 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 1125 documents (total 52010 corpus positions)", 'datetime': '2024-12-29T15:24:16.085107', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:16,179, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:16,191, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:16,196, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:16,206, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:16,221, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:16,222, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:16,234, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:16,245, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:16,338, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:16,350, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:16,415, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:24:16,416, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:24:16.416221', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:16,425, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:16,472, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:16,484, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:16,484, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:16,531, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:16,543, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:16,833, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:16,882, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:16,885, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:16,892, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:16,912, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:16,913, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:16,927, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:16,928, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:17,035, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:17,036, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:17,050, dictionary, INFO, built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions) ]
[2024-12-29 15:24:17,051, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7273 unique tokens: ['almost', 'also', 'ambiance', 'awesome', 'biryani']...> from 1125 documents (total 50605 corpus positions)", 'datetime': '2024-12-29T15:24:17.051521', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:17,154, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:17,165, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:17,183, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:24:17,184, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:24:17.184167', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:17,236, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:17,236, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:17,591, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:24:17,591, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:24:17.591078', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:17,603, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:17,616, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:17,873, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:17,874, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:18,307, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:18,307, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:19,254, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:19,298, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:19,563, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:19,608, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:19,610, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:19,628, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:19,649, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:19,898, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:19,926, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:24:19,927, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:24:19.927827', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:19,936, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:20,242, dictionary, INFO, built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions) ]
[2024-12-29 15:24:20,242, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16047 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 205347 corpus positions)", 'datetime': '2024-12-29T15:24:20.242984', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:20,278, dictionary, INFO, built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions) ]
[2024-12-29 15:24:20,278, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16147 unique tokens: ['also', 'amazed', 'amazing', 'blended', 'blowing']...> from 4500 documents (total 205599 corpus positions)", 'datetime': '2024-12-29T15:24:20.278887', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:20,478, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:20,498, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:24:20,498, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:24:20.498299', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:20,513, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:20,664, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:20,910, dictionary, INFO, built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions) ]
[2024-12-29 15:24:20,910, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16109 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 206752 corpus positions)", 'datetime': '2024-12-29T15:24:20.910197', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:25,193, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:25,194, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:25,292, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:25,292, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:25,573, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:25,579, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:25,651, dictionary, INFO, built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions) ]
[2024-12-29 15:24:25,651, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7529 unique tokens: ['delicious', 'enjoy', 'enjoyed', 'food', 'fulca']...> from 1125 documents (total 53055 corpus positions)", 'datetime': '2024-12-29T15:24:25.651548', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:25,698, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:25,704, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:25,753, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:25,754, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:25,824, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:25,825, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:26,015, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:26,015, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:26,091, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:26,097, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:26,168, dictionary, INFO, built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions) ]
[2024-12-29 15:24:26,168, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7389 unique tokens: ['ambiencealso', 'availablefast', 'away', 'delicious', 'deliverygood']...> from 1125 documents (total 49929 corpus positions)", 'datetime': '2024-12-29T15:24:26.168129', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:26,212, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:26,217, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:26,533, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:26,533, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:27,124, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:27,145, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:27,421, dictionary, INFO, built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions) ]
[2024-12-29 15:24:27,421, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16039 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 204302 corpus positions)", 'datetime': '2024-12-29T15:24:27.421813', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:27,697, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:24:27,719, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:24:28,033, dictionary, INFO, built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions) ]
[2024-12-29 15:24:28,033, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<16024 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 4500 documents (total 207428 corpus positions)", 'datetime': '2024-12-29T15:24:28.033139', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:24:28,205, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:24:28,207, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:24:28,210, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:24:28,217, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:24:28,586, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:24:28,587, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:24:29,884, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:28:50,384, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:28:50,397, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:28:50,816, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:28:50,818, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:28:52,530, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:28:52,571, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:28:52,995, dictionary, INFO, built Dictionary<18237 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 5625 documents (total 257357 corpus positions) ]
[2024-12-29 15:28:52,996, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<18237 unique tokens: ['add', 'along', 'amazing', 'ambience', 'charm']...> from 5625 documents (total 257357 corpus positions)", 'datetime': '2024-12-29T15:28:52.996836', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:29:31,872, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:29:32,432, dictionary, INFO, built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions) ]
[2024-12-29 15:29:32,434, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions)", 'datetime': '2024-12-29T15:29:32.434915', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:30:14,595, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:30:15,200, dictionary, INFO, built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions) ]
[2024-12-29 15:30:15,201, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions)", 'datetime': '2024-12-29T15:30:15.201068', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:30:37,385, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:30:37,986, dictionary, INFO, built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions) ]
[2024-12-29 15:30:37,989, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions)", 'datetime': '2024-12-29T15:30:37.989764', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:30:38,351, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:30:38,361, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:30:38,765, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:30:38,766, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:30:40,107, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:30:47,484, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:31:00,535, text_analysis, INFO, 61 batches submitted to accumulate stats from 3904 documents (-93340 virtual) ]
[2024-12-29 15:31:01,034, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:31:01,047, text_analysis, INFO, accumulated word occurrence stats for 113005 virtual documents ]
[2024-12-29 15:32:20,610, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:32:20,613, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:32:20,619, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:32:20,621, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:32:20,628, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:32:20,631, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:32:20,637, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:32:20,652, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:32:20,926, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:32:20,928, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:32:21,845, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:32:27,434, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:32:27,436, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:32:27,442, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:32:27,447, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:32:27,738, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:32:27,739, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:32:28,704, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:35:18,427, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:35:18,429, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:35:18,435, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:35:18,438, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:35:18,443, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:35:18,448, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:35:18,742, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:35:18,743, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:35:19,658, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:35:25,104, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:35:25,272, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:35:25,273, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:35:25.273238', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:35:25,374, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:35:25,377, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:35:25,500, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:35:25,502, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:35:25,920, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:35:28,493, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:35:41,668, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:35:41,714, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:35:42,590, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:35:42,592, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:35:42,597, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:35:42,603, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:35:42,891, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:35:42,892, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:35:43,785, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:35:48,905, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:35:49,077, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:35:49,078, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:35:49.078518', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:35:49,179, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:35:49,183, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:35:49,299, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:35:49,300, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:35:49,714, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:35:52,150, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:36:05,099, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:36:05,127, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:36:06,032, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:36:06,034, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:36:06,038, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:36:06,044, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:36:06,316, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:36:06,318, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:36:07,412, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:36:12,846, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:36:13,018, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:36:13,019, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:36:13.019510', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:36:13,120, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:36:13,124, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:36:13,240, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:36:13,241, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:36:13,653, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:36:16,061, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:36:30,361, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:36:30,382, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:36:31,258, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:36:31,260, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:36:31,266, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:36:31,275, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:36:31,587, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:36:31,588, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:36:32,700, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:36:44,047, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:36:44,252, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:36:44,253, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:36:44.253091', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:36:44,369, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:36:44,373, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:36:44,509, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:36:44,511, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:36:44,993, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:36:50,266, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:37:03,960, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:37:03,997, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:37:04,983, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:37:04,985, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:37:04,990, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:37:04,997, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:37:05,332, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:37:05,333, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:37:06,531, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:37:18,271, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:37:18,443, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:37:18,444, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:37:18.444003', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:37:18,546, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:37:18,549, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:37:18,684, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:37:18,686, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:37:19,204, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:37:24,311, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:37:37,926, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:37:37,958, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:37:38,791, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:37:38,793, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:37:38,797, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:37:38,803, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:37:39,085, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:37:39,087, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:37:40,036, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:37:51,820, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:37:51,999, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:37:52,000, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:37:52.000715', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:37:52,105, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:37:52,108, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:37:52,225, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:37:52,227, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:37:52,702, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:37:57,947, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:38:11,690, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:38:11,711, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:38:12,643, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:38:12,646, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:38:12,650, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:38:12,655, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:38:12,929, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:38:12,931, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:38:13,981, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:38:27,525, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:38:27,731, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:38:27,732, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:38:27.732325', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:38:27,859, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:38:27,863, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:38:27,981, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:38:27,982, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:38:28,430, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:38:34,929, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:38:48,529, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:38:48,555, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:38:49,132, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:38:49,134, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:38:49,138, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:38:49,144, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:38:49,370, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:38:49,372, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:38:50,407, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:39:02,769, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:39:02,938, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:39:02,939, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:39:02.939302', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:39:03,038, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:39:03,041, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:39:03,154, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:39:03,156, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:39:03,569, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:39:09,597, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:39:22,038, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:39:22,075, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:39:22,635, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:39:22,638, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:39:22,641, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:39:22,646, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:39:22,883, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:39:22,883, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:39:23,735, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:39:36,957, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:39:37,160, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:39:37,160, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:39:37.160755', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:39:37,278, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:39:37,282, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:39:37,418, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:39:37,419, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:39:37,905, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:39:43,895, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:39:57,849, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:39:57,870, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:39:58,540, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:39:58,542, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:39:58,546, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:39:58,553, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:39:58,829, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:39:58,830, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:39:59,711, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:40:11,428, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:40:11,603, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:40:11,604, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:40:11.604099', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:40:11,722, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:40:11,725, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:40:11,843, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:40:11,844, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:40:12,290, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:40:17,417, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:40:31,931, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:40:31,967, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:40:32,942, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:40:32,944, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:40:32,952, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:40:32,962, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:40:33,252, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:40:33,253, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:40:34,204, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:40:47,591, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:40:47,808, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:40:47,810, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:40:47.810049', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:40:47,928, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:40:47,932, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:40:48,065, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:40:48,066, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:40:48,535, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:40:54,031, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:41:06,993, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:41:07,038, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:41:07,894, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:41:07,896, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:41:07,900, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:41:07,906, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:41:08,175, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:41:08,176, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:41:09,143, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:41:20,964, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:41:21,171, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:41:21,172, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:41:21.172350', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:41:21,284, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:41:21,288, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:41:21,427, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:41:21,428, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:41:21,910, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:41:27,266, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:41:41,357, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:41:41,385, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:41:42,249, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:41:42,251, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:41:42,256, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:41:42,262, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:41:42,531, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:41:42,532, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:41:43,471, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:41:50,979, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:41:51,189, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:41:51,190, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:41:51.190587', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:41:51,310, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:41:51,314, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:41:51,447, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:41:51,450, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:41:51,928, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:41:55,136, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:42:07,803, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:42:07,828, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:42:08,230, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:42:08,231, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:42:08,236, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:42:08,242, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:42:08,495, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:42:08,496, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:42:09,616, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:42:17,168, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:42:17,370, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:42:17,372, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:42:17.372081', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:42:17,491, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:42:17,495, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:42:17,628, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:42:17,629, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:42:18,105, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:42:21,339, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:42:34,038, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:42:34,079, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:42:34,541, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:42:34,543, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:42:34,548, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:42:34,554, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:42:34,828, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:42:34,829, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:42:35,781, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:42:43,311, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:42:43,564, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:42:43,565, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:42:43.565683', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:42:43,691, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:42:43,694, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:42:43,831, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:42:43,832, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:42:44,306, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:42:47,338, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:43:01,061, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:43:01,108, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:43:01,561, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:43:01,563, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:43:01,567, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:43:01,573, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:43:01,850, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:43:01,852, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:43:02,816, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:43:06,610, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:43:06,803, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:43:06,805, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:43:06.805608', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:43:06,924, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:43:06,928, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:43:07,052, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:43:07,053, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:43:07,491, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:43:09,322, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:43:24,380, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:43:24,409, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:43:24,884, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:43:24,886, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:43:24,891, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:43:24,898, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:43:25,179, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:43:25,180, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:43:26,153, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:43:29,785, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:43:29,988, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:43:29,990, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:43:29.990870', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:43:30,113, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:43:30,116, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:43:30,251, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:43:30,252, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:43:30,741, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:43:32,343, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:43:45,323, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:43:45,346, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:43:45,796, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:43:45,798, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:43:45,802, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:43:45,808, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:43:46,080, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:43:46,081, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:43:46,944, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:43:50,671, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:43:50,860, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:43:50,861, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:43:50.861751', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:43:50,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:43:50,969, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:43:51,082, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:43:51,083, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:43:51,485, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:43:52,988, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:44:05,707, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:44:05,734, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:44:06,166, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:44:06,168, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:44:06,173, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:44:06,178, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:44:06,448, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:44:06,449, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:44:07,408, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:44:10,679, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:44:10,863, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:44:10,864, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:44:10.864346', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:44:10,973, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:44:10,977, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:44:11,093, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:44:11,095, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:44:12,570, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:44:14,647, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:44:26,887, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:44:26,903, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:44:27,153, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:44:27,155, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:44:27,161, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:44:27,167, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:44:27,457, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:44:27,458, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:44:28,465, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:44:31,948, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:44:32,159, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:44:32,160, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:44:32.160864', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:44:32,279, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:44:32,283, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:44:32,443, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:44:32,444, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:44:33,004, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:44:34,518, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:44:48,539, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:44:48,557, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:44:48,761, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:44:48,763, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:44:48,768, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:44:48,773, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:44:49,001, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:44:49,003, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:44:49,866, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:44:53,409, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:44:53,603, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:44:53,604, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:44:53.604702', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:44:53,728, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:44:53,732, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:44:53,870, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:44:53,870, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:44:54,342, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:44:56,067, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:45:08,543, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:45:08,560, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:45:08,800, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:45:08,802, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:45:08,806, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:45:08,812, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:45:09,116, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:45:09,117, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:45:10,089, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:45:15,118, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:45:15,326, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:45:15,328, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:45:15.327584', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:45:15,453, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:45:15,457, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:45:15,593, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:45:15,594, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:45:16,078, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:45:18,017, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:45:32,046, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:45:32,063, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:45:32,304, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:45:32,306, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:45:32,310, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:45:32,317, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:45:32,591, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:45:32,592, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:45:33,561, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:45:38,434, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:45:38,638, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:45:38,640, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:45:38.640001', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:45:38,760, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:45:38,764, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:45:38,882, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:45:38,883, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:45:39,279, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:45:41,372, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:45:55,418, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:45:55,435, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:45:55,657, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:45:55,659, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:45:55,663, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:45:55,669, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:45:55,907, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:45:55,908, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:45:56,692, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:46:01,633, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:46:01,839, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:46:01,840, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:46:01.840465', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:46:01,968, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:46:01,972, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:46:02,108, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:46:02,109, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:46:02,648, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:46:04,905, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:46:19,108, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:46:19,123, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:46:19,365, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:46:19,365, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:46:19,370, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:46:19,376, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:46:19,658, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:46:19,660, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:46:20,621, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:46:33,298, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:46:33,545, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:46:33,546, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:46:33.546542', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:46:33,677, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:46:33,680, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:46:33,821, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:46:33,822, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:46:34,303, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:46:39,947, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:46:52,485, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:46:52,504, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:46:52,743, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:46:52,746, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:46:52,751, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:46:52,756, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:46:52,992, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:46:52,994, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:46:53,940, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:47:06,661, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:47:06,865, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:47:06,866, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:47:06.866101', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:47:07,010, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:47:07,015, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:47:07,154, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:47:07,155, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:47:07,636, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:47:13,818, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:47:26,381, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:47:26,399, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:47:26,641, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:47:26,645, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:47:26,649, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:47:26,656, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:47:26,928, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:47:26,930, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:47:27,946, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:47:40,870, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:47:41,082, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:47:41,083, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:47:41.083001', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:47:41,213, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:47:41,216, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:47:41,352, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:47:41,353, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:47:41,837, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:47:47,434, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:48:00,059, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:48:00,075, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:48:00,280, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:48:00,285, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:48:00,291, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:48:00,297, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:48:00,568, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:48:00,569, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:48:01,509, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:48:15,333, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:48:15,537, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:48:15,539, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:48:15.539013', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:48:15,662, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:48:15,666, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:48:15,790, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:48:15,791, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:48:16,192, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:48:22,670, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:48:36,319, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:48:36,354, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:48:37,071, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:48:37,073, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:48:37,078, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:48:37,084, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:48:37,363, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:48:37,365, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:48:38,338, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:48:52,167, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:48:52,374, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:48:52,376, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:48:52.376234', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:48:52,498, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:48:52,502, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:48:52,636, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:48:52,637, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:48:53,143, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:48:59,624, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:49:14,782, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:49:14,834, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:49:15,543, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:49:15,545, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:49:15,549, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:49:15,555, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:49:15,824, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:49:15,825, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:49:16,829, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:49:30,729, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:49:30,944, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:49:30,945, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:49:30.945132', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:49:31,084, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:49:31,088, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:49:31,225, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:49:31,226, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:49:31,724, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:49:37,854, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:49:50,813, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:49:50,840, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:49:51,519, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:49:51,521, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:49:51,524, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:49:51,532, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:49:51,967, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:49:51,968, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:49:53,411, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:50:15,252, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:50:15,949, dictionary, INFO, built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions) ]
[2024-12-29 15:50:15,950, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions)", 'datetime': '2024-12-29T15:50:15.950891', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:50:16,257, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:50:16,265, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:50:16,616, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:50:16,617, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:50:17,993, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:50:39,737, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:50:52,236, text_analysis, INFO, 61 batches submitted to accumulate stats from 3904 documents (-93340 virtual) ]
[2024-12-29 15:50:52,972, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:50:53,010, text_analysis, INFO, accumulated word occurrence stats for 113005 virtual documents ]
[2024-12-29 15:51:54,261, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:51:54,272, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:51:54,642, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:51:54,643, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:51:55,975, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:52:26,924, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:52:26,934, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:52:27,413, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:52:27,415, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:52:28,914, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:57:11,727, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:57:11,730, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:57:11,736, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:57:11,737, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:57:11,743, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:57:11,750, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:57:11,990, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:57:11,993, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:57:12,868, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:57:30,141, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:57:30,368, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:57:30,370, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:57:30.370885', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:57:30,495, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:57:30,498, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:57:30,636, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:57:30,638, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:57:31,213, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:57:37,757, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:57:50,214, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:57:50,244, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:57:50,629, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:57:50,633, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:57:50,638, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:57:50,644, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:57:51,380, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:57:51,385, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:57:54,057, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:58:09,947, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:58:10,128, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:58:10,129, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:58:10.129328', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:58:10,238, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:58:10,241, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:58:10,358, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:58:10,360, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:58:10,778, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:58:18,158, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:58:30,807, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:58:30,829, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 15:58:31,234, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:58:31,236, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:58:31,240, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:58:31,247, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:58:31,525, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:58:31,527, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:58:32,578, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:58:48,572, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:58:48,760, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 15:58:48,760, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T15:58:48.760028', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:58:48,868, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:58:48,871, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:58:48,985, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:58:48,986, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:58:49,377, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:58:56,326, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:59:08,527, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:59:08,555, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 15:59:08,999, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:59:09,000, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:59:09,004, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:59:09,011, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:59:09,291, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:59:09,293, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:59:10,240, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:59:21,610, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:59:21,782, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 15:59:21,783, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T15:59:21.783664', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:59:21,895, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:59:21,899, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:59:22,012, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:59:22,014, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:59:22,436, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:59:27,606, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 15:59:40,003, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 15:59:40,020, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 15:59:40,165, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 15:59:40,169, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 15:59:40,173, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:59:40,179, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:59:40,465, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:59:40,466, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:59:41,416, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:59:52,558, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 15:59:52,727, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 15:59:52,728, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T15:59:52.728893', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 15:59:52,830, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 15:59:52,833, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 15:59:52,943, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 15:59:52,945, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 15:59:53,339, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 15:59:58,625, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:00:10,329, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:00:10,344, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:00:10,847, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:00:10,854, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:00:10,867, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:00:10,889, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:00:11,894, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:00:11,898, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:00:13,030, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:00:24,578, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:00:24,757, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:00:24,758, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:00:24.758416', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:00:24,873, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:00:24,876, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:00:24,992, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:00:24,993, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:00:25,405, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:00:30,358, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:00:42,332, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:00:42,345, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:00:42,854, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:00:42,865, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:00:42,881, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:00:42,904, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:00:44,095, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:00:44,101, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:00:45,020, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:01:00,604, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:01:00,783, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:01:00,784, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:01:00.784344', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:01:00,896, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:01:00,900, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:01:01,013, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:01:01,014, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:01:01,481, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:01:08,738, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:01:21,627, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:01:21,654, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:01:22,100, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:01:22,101, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:01:22,106, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:01:22,110, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:01:22,402, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:01:22,403, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:01:23,351, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:01:41,056, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:01:41,229, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:01:41,230, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:01:41.229846', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:01:41,343, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:01:41,347, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:01:41,459, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:01:41,461, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:01:41,862, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:01:48,295, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:02:00,549, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:02:00,597, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:02:01,043, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:02:01,045, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:02:01,049, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:02:01,055, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:02:01,408, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:02:01,410, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:02:02,338, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:02:18,225, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:02:18,402, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:02:18,403, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:02:18.403213', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:02:18,505, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:02:18,508, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:02:18,623, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:02:18,624, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:02:19,088, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:02:26,207, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:02:39,116, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:02:39,134, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:02:39,582, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:02:39,584, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:02:39,588, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:02:39,594, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:02:39,866, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:02:39,867, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:02:40,862, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:02:56,482, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:02:56,687, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:02:56,689, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:02:56.689442', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:02:56,811, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:02:56,815, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:02:56,950, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:02:56,952, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:02:57,440, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:03:04,664, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:03:16,688, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:03:16,700, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:03:16,868, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:03:16,872, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:03:16,876, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:03:16,882, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:03:17,117, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:03:17,118, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:03:19,730, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:03:35,802, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:03:36,003, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:03:36,004, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:03:36.004334', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:03:36,127, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:03:36,130, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:03:36,261, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:03:36,263, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:03:36,768, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:03:43,563, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:03:55,931, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:03:55,946, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:03:56,147, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:03:56,149, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:03:56,153, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:03:56,159, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:03:56,456, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:03:56,457, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:03:57,415, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:04:13,650, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:04:13,826, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:04:13,827, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:04:13.827015', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:04:13,928, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:04:13,931, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:04:14,047, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:04:14,048, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:04:14,474, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:04:21,470, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:04:33,162, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:04:33,176, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:04:33,800, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:04:33,806, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:04:33,819, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:04:33,839, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:04:34,806, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:04:34,810, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:04:36,484, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:04:52,258, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:04:52,466, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:04:52,468, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:04:52.468579', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:04:52,586, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:04:52,590, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:04:52,735, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:04:52,737, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:04:53,221, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:05:00,044, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:05:11,741, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:05:11,754, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:05:11,960, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:05:11,962, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:05:11,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:05:11,972, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:05:12,256, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:05:12,257, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:05:13,084, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:05:28,670, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:05:28,873, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:05:28,874, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:05:28.874800', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:05:28,990, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:05:28,994, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:05:29,135, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:05:29,137, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:05:29,558, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:05:36,144, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:05:48,340, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:05:48,352, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:05:48,521, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:05:48,524, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:05:48,530, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:05:48,537, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:05:48,781, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:05:48,782, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:05:49,685, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:06:05,983, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:06:06,738, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:06:06,742, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:06:06.742708', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:06:07,150, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:06:07,162, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:06:07,707, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:06:07,713, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:06:08,916, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:06:15,868, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:06:27,377, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:06:27,394, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:06:27,598, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:06:27,601, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:06:27,605, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:06:27,610, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:06:27,853, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:06:27,854, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:06:30,410, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:06:46,340, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:06:46,569, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:06:46,570, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:06:46.570076', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:06:46,693, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:06:46,697, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:06:46,835, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:06:46,836, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:06:47,327, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:06:54,231, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:07:06,446, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:07:06,458, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:07:06,633, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:07:06,637, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:07:06,642, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:07:06,649, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:07:06,927, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:07:06,928, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:07:07,897, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:07:23,353, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:07:23,555, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:07:23,556, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:07:23.556954', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:07:23,702, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:07:23,706, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:07:23,840, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:07:23,841, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:07:24,308, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:07:30,936, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:07:42,644, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:07:42,658, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:07:42,798, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:07:42,802, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:07:42,805, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:07:42,811, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:07:43,034, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:07:43,035, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:07:43,926, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:07:59,910, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:08:00,123, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:08:00,125, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:08:00.125200', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:08:00,229, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:08:00,232, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:08:00,348, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:08:00,350, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:08:00,742, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:08:07,992, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:08:20,830, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:08:20,844, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:08:20,998, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:08:21,003, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:08:21,008, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:08:21,013, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:08:21,287, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:08:21,288, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:08:22,254, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:08:33,290, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:08:33,536, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:08:33,537, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:08:33.537772', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:08:33,639, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:08:33,642, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:08:33,758, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:08:33,760, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:08:34,156, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:08:39,030, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:08:51,361, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:08:51,384, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:08:51,768, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:08:51,770, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:08:51,774, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:08:51,780, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:08:52,052, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:08:52,053, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:08:53,083, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:09:03,624, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:09:03,831, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:09:03,832, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:09:03.832057', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:09:03,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:09:03,970, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:09:04,104, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:09:04,106, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:09:04,668, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:09:09,835, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:09:22,220, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:09:22,255, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:09:22,568, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:09:22,570, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:09:22,573, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:09:22,579, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:09:22,821, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:09:22,822, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:09:23,836, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:09:34,142, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:09:34,409, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:09:34,411, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:09:34.410500', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:09:34,525, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:09:34,529, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:09:34,646, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:09:34,647, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:09:35,102, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:09:40,041, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:09:54,652, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:09:54,681, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:09:55,066, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:09:55,067, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:09:55,071, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:09:55,078, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:09:55,365, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:09:55,366, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:09:56,260, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:10:13,181, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:10:13,388, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:10:13,390, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:10:13.390555', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:10:13,516, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:10:13,519, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:10:13,660, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:10:13,661, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:10:14,131, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:10:21,049, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:10:35,110, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:10:35,126, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:10:35,373, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:10:35,376, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:10:35,380, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:10:35,386, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:10:35,662, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:10:35,663, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:10:36,608, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:10:53,176, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:10:53,383, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:10:53,385, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:10:53.384655', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:10:53,505, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:10:53,508, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:10:53,625, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:10:53,626, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:10:54,028, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:11:01,149, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:11:13,556, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:11:13,572, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:11:13,784, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:11:13,787, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:11:13,791, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:11:13,796, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:11:14,066, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:11:14,067, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:11:15,024, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:11:31,644, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:11:31,872, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:11:31,873, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:11:31.873071', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:11:32,000, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:11:32,003, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:11:32,139, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:11:32,141, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:11:32,625, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:11:39,502, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:11:53,301, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:11:53,384, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:11:54,203, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:11:54,215, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:11:54,229, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:11:54,256, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:11:54,580, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:11:54,581, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:11:55,516, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:12:11,001, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:12:11,213, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:12:11,215, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:12:11.215834', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:12:11,318, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:12:11,323, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:12:11,435, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:12:11,437, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:12:11,843, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:12:18,942, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:12:31,632, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:12:31,652, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:12:31,995, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:12:31,997, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:12:32,001, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:12:32,007, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:12:32,279, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:12:32,280, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:12:33,410, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:12:50,119, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:12:50,349, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:12:50,349, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:12:50.349976', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:12:50,468, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:12:50,472, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:12:50,605, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:12:50,607, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:12:51,024, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:12:58,127, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:13:10,290, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:13:10,306, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:13:10,626, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:13:10,630, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:13:10,634, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:13:10,640, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:13:10,912, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:13:10,913, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:13:11,934, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:13:27,788, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:13:27,996, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:13:27,997, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:13:27.997438', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:13:28,122, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:13:28,126, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:13:28,271, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:13:28,272, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:13:28,754, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:13:36,345, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:13:48,818, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:13:48,841, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:13:49,170, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:13:49,172, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:13:49,176, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:13:49,182, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:13:49,452, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:13:49,454, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:13:50,557, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:14:06,282, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:14:06,478, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:14:06,479, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:14:06.479480', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:14:06,601, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:14:06,605, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:14:06,741, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:14:06,743, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:14:07,178, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:14:13,647, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:14:26,187, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:14:26,221, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:14:26,629, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:14:26,631, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:14:26,635, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:14:26,640, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:14:26,912, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:14:26,914, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:14:28,187, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:14:43,687, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:14:43,860, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:14:43,861, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:14:43.861956', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:14:43,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:14:43,969, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:14:44,084, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:14:44,086, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:14:44,494, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:14:51,995, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:15:08,397, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:15:08,416, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:15:08,870, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:15:08,872, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:15:08,877, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:15:08,883, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:15:09,159, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:15:09,160, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:15:10,305, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:15:27,743, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:15:27,933, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:15:27,934, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:15:27.934805', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:15:28,041, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:15:28,044, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:15:28,162, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:15:28,163, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:15:29,047, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:15:37,030, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:15:49,786, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:15:49,821, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:15:50,171, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:15:50,173, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:15:50,175, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:15:50,183, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:15:50,606, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:15:50,607, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:15:52,020, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:16:18,194, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:16:18,741, dictionary, INFO, built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions) ]
[2024-12-29 16:16:18,742, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions)", 'datetime': '2024-12-29T16:16:18.742938', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:16:19,101, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:16:19,110, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:16:19,507, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:16:19,508, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:16:21,000, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:16:46,263, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:16:58,287, text_analysis, INFO, 61 batches submitted to accumulate stats from 3904 documents (-93340 virtual) ]
[2024-12-29 16:16:58,717, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:16:58,729, text_analysis, INFO, accumulated word occurrence stats for 113005 virtual documents ]
[2024-12-29 16:16:58,978, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:16:58,988, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:16:59,369, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:16:59,373, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:17:02,700, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:20:40,328, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:20:40,332, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:23:01,467, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:23:01,471, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:23:14,388, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:23:14,391, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:23:14,415, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:23:14,417, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:23:14,421, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:23:14,427, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:23:14,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:23:14,669, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:23:20,406, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:23:24,479, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:23:24,698, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:23:24,699, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:23:24.699258', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:23:24,830, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:23:24,834, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:23:24,991, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:23:24,994, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:23:25,547, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:23:27,533, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:23:41,246, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:23:41,277, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:23:41,946, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:23:41,948, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:23:41,952, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:23:41,959, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:23:42,240, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:23:42,241, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:23:43,243, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:23:46,982, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:23:47,380, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:23:47,384, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:23:47.384826', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:23:47,817, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:23:47,831, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:23:48,331, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:23:48,335, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:23:49,173, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:23:50,908, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:24:05,943, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:24:05,973, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:24:06,668, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:24:06,670, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:24:06,674, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:24:06,681, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:24:06,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:24:06,949, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:24:08,128, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:24:12,178, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:24:12,386, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:24:12,387, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:24:12.387451', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:24:12,510, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:24:12,514, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:24:12,650, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:24:12,652, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:24:13,135, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:24:14,827, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:24:28,357, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:24:28,389, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:24:29,105, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:24:29,107, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:24:29,111, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:24:29,117, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:24:29,384, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:24:29,385, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:24:30,325, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:24:34,358, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:24:34,584, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:24:34,585, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:24:34.585079', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:24:34,690, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:24:34,693, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:24:34,808, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:24:34,809, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:24:35,231, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:24:36,961, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:24:50,990, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:24:51,023, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:24:51,805, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:24:51,807, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:24:51,811, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:24:51,817, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:24:52,090, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:24:52,091, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:24:53,051, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:24:57,155, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:24:57,363, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:24:57,365, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:24:57.365584', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:24:57,488, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:24:57,492, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:24:57,619, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:24:57,619, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:24:58,013, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:24:59,950, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:25:13,676, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:25:13,719, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:25:14,532, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:25:14,534, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:25:14,538, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:25:14,544, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:25:14,835, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:25:14,837, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:25:15,787, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:25:19,767, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:25:19,946, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:25:19,946, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:25:19.946995', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:25:20,071, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:25:20,075, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:25:20,211, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:25:20,212, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:25:20,695, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:25:22,411, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:25:36,513, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:25:36,537, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:25:37,343, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:25:37,345, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:25:37,349, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:25:37,355, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:25:37,624, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:25:37,625, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:25:38,581, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:25:42,747, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:25:42,921, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:25:42,923, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:25:42.923890', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:25:43,029, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:25:43,032, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:25:43,148, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:25:43,149, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:25:43,613, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:25:45,142, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:25:58,779, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:25:58,804, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:25:59,673, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:25:59,675, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:25:59,679, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:25:59,685, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:25:59,986, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:25:59,988, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:26:00,975, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:26:05,001, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:26:05,183, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:26:05,183, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:26:05.183438', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:26:05,292, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:26:05,296, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:26:05,410, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:26:05,411, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:26:05,913, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:26:07,777, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:26:21,788, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:26:21,830, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:26:22,667, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:26:22,670, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:26:22,675, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:26:22,681, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:26:22,956, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:26:22,957, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:26:23,752, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:26:27,609, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:26:27,793, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:26:27,794, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:26:27.794540', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:26:28,030, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:26:28,034, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:26:28,145, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:26:28,146, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:26:28,568, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:26:30,352, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:26:43,783, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:26:43,809, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:26:44,650, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:26:44,651, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:26:44,655, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:26:44,661, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:26:44,968, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:26:44,969, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:26:45,980, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:26:53,325, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:26:53,517, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:26:53,517, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:26:53.517454', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:26:53,625, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:26:53,628, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:26:53,740, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:26:53,741, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:26:54,159, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:26:56,356, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:27:09,014, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:27:09,032, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:27:09,239, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:27:09,240, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:27:09,244, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:27:09,250, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:27:09,480, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:27:09,481, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:27:10,319, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:27:15,382, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:27:15,557, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:27:15,558, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:27:15.558335', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:27:15,665, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:27:15,668, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:27:15,780, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:27:15,781, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:27:16,177, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:27:18,178, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:27:31,015, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:27:31,035, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:27:31,245, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:27:31,248, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:27:31,252, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:27:31,258, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:27:31,525, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:27:31,527, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:27:32,501, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:27:38,044, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:27:38,256, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:27:38,257, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:27:38.257309', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:27:38,387, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:27:38,390, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:27:38,527, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:27:38,528, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:27:39,013, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:27:41,209, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:27:53,377, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:27:53,394, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:27:53,621, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:27:53,622, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:27:53,626, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:27:53,631, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:27:53,865, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:27:53,866, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:27:54,695, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:28:01,188, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:28:01,363, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:28:01,364, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:28:01.364111', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:28:01,475, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:28:01,478, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:28:01,595, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:28:01,597, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:28:02,040, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:28:03,929, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:28:17,238, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:28:17,276, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:28:17,715, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:28:17,717, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:28:17,721, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:28:17,727, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:28:18,000, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:28:18,002, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:28:18,975, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:28:23,669, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:28:23,881, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:28:23,881, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:28:23.881840', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:28:24,006, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:28:24,009, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:28:24,146, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:28:24,148, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:28:24,615, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:28:26,711, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:28:42,004, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:28:42,027, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:28:42,477, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:28:42,478, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:28:42,482, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:28:42,488, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:28:42,756, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:28:42,757, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:28:43,702, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:28:48,636, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:28:48,847, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:28:48,848, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:28:48.848492', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:28:49,002, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:28:49,006, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:28:49,147, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:28:49,148, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:28:49,636, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:28:51,606, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:29:04,569, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:29:04,590, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:29:05,066, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:29:05,068, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:29:05,072, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:29:05,077, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:29:05,378, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:29:05,379, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:29:06,384, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:29:11,037, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:29:11,210, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:29:11,211, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:29:11.211204', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:29:11,316, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:29:11,319, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:29:11,451, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:29:11,453, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:29:11,867, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:29:13,748, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:29:27,479, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:29:27,530, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:29:28,371, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:29:28,373, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:29:28,377, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:29:28,383, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:29:28,655, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:29:28,656, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:29:29,657, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:29:34,016, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:29:34,194, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:29:34,195, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:29:34.195557', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:29:34,307, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:29:34,311, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:29:34,444, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:29:34,445, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:29:34,967, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:29:36,874, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:29:50,654, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:29:50,697, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:29:51,596, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:29:51,598, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:29:51,602, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:29:51,608, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:29:51,839, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:29:51,840, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:29:52,625, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:29:57,246, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:29:57,452, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:29:57,454, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:29:57.454870', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:29:57,593, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:29:57,597, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:29:57,743, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:29:57,744, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:29:58,229, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:30:00,217, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:30:14,437, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:30:14,476, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:30:15,429, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:30:15,431, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:30:15,435, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:30:15,441, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:30:15,709, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:30:15,710, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:30:16,758, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:30:20,904, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:30:21,105, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:30:21,106, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:30:21.106079', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:30:21,213, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:30:21,217, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:30:21,337, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:30:21,338, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:30:21,844, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:30:23,440, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:30:37,015, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:30:37,042, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:30:37,763, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:30:37,764, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:30:37,769, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:30:37,775, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:30:38,045, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:30:38,046, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:30:38,994, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:30:43,206, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:30:43,412, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:30:43,413, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:30:43.413334', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:30:43,537, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:30:43,541, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:30:43,677, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:30:43,678, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:30:44,172, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:30:46,082, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:30:59,285, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:30:59,326, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:31:00,073, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:31:00,075, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:31:00,079, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:31:00,085, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:31:00,370, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:31:00,371, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:31:01,329, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:31:05,507, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:31:05,752, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:31:05,753, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:31:05.753253', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:31:05,914, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:31:05,919, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:31:06,061, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:31:06,062, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:31:06,547, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:31:08,299, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:31:22,000, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:31:22,023, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:31:22,742, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:31:22,743, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:31:22,747, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:31:22,753, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:31:23,026, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:31:23,027, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:31:23,979, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:31:28,171, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:31:28,380, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:31:28,381, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:31:28.381253', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:31:28,507, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:31:28,511, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:31:28,648, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:31:28,650, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:31:29,135, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:31:31,010, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:31:44,905, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:31:44,941, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:31:45,787, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:31:45,789, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:31:45,794, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:31:45,800, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:31:46,073, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:31:46,074, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:31:47,031, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:31:51,098, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:31:51,315, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:31:51,316, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:31:51.316131', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:31:51,442, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:31:51,445, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:31:51,580, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:31:51,581, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:31:52,033, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:31:53,952, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:32:07,405, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:32:07,440, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:32:08,284, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:32:08,285, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:32:08,290, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:32:08,296, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:32:08,568, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:32:08,569, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:32:09,604, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:32:13,782, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:32:14,003, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:32:14,005, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:32:14.005523', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:32:14,126, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:32:14,129, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:32:14,282, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:32:14,284, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:32:14,806, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:32:16,456, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:32:30,739, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:32:30,765, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:32:31,668, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:32:31,670, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:32:31,674, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:32:31,680, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:32:31,952, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:32:31,953, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:32:32,898, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:32:36,192, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:32:36,368, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:32:36,370, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:32:36.369550', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:32:36,479, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:32:36,482, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:32:36,603, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:32:36,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:32:37,038, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:32:38,442, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:32:51,276, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:32:51,297, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:32:51,787, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:32:51,789, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:32:51,793, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:32:51,799, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:32:52,108, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:32:52,110, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:32:53,064, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:32:56,405, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:32:56,614, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:32:56,615, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:32:56.615716', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:32:56,738, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:32:56,742, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:32:56,878, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:32:56,879, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:32:57,376, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:32:58,799, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:33:11,472, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:33:11,491, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:33:12,001, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:33:12,003, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:33:12,008, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:33:12,013, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:33:12,285, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:33:12,286, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:33:13,246, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:33:16,543, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:33:16,753, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:33:16,755, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:33:16.755634', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:33:16,879, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:33:16,882, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:33:17,025, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:33:17,027, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:33:17,519, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:33:18,918, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:33:31,697, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:33:31,713, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:33:32,488, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:33:32,490, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:33:32,494, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:33:32,499, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:33:32,767, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:33:32,768, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:33:33,757, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:33:37,874, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:33:38,082, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:33:38,083, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:33:38.083750', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:33:38,212, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:33:38,215, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:33:38,352, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:33:38,353, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:33:38,794, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:33:40,849, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:33:54,309, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:33:54,343, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:33:55,101, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:33:55,103, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:33:55,108, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:33:55,114, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:33:55,384, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:33:55,386, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:33:56,364, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:34:00,692, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:34:00,922, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:34:00,924, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:34:00.924521', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:34:01,057, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:34:01,060, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:34:01,206, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:34:01,207, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:34:01,705, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:34:03,726, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:34:16,971, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:34:17,006, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:34:17,747, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:34:17,749, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:34:17,753, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:34:17,759, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:34:18,030, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:34:18,032, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:34:19,013, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:34:23,188, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:34:23,439, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:34:23,440, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:34:23.440528', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:34:23,577, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:34:23,581, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:34:23,717, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:34:23,719, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:34:24,288, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:34:25,983, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:34:41,795, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:34:41,814, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:34:42,903, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:34:42,905, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:34:42,908, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:34:42,916, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:34:43,370, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:34:43,371, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:34:44,888, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:34:51,407, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:34:52,039, dictionary, INFO, built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions) ]
[2024-12-29 16:34:52,040, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions)", 'datetime': '2024-12-29T16:34:52.040643', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:34:52,343, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:34:52,350, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:34:52,708, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:34:52,709, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:34:53,949, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:35:00,144, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:35:12,973, text_analysis, INFO, 61 batches submitted to accumulate stats from 3904 documents (-93340 virtual) ]
[2024-12-29 16:35:13,735, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:35:13,789, text_analysis, INFO, accumulated word occurrence stats for 113005 virtual documents ]
[2024-12-29 16:35:42,348, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:35:42,358, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:35:42,718, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:35:42,720, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:35:44,193, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:36:58,212, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:36:58,214, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:39:09,487, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:39:09,490, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:39:09,522, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:39:09,524, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:39:09,528, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:39:09,534, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:39:09,807, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:39:09,808, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:39:10,716, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:39:20,771, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:39:20,984, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:39:20,986, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:39:20.986345', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:39:21,105, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:39:21,108, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:39:21,244, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:39:21,245, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:39:21,779, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:39:24,514, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:39:37,525, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:39:37,541, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:39:37,890, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:39:37,892, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:39:37,896, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:39:37,903, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:39:38,180, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:39:38,181, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:39:39,166, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:39:45,888, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:39:46,093, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:39:46,094, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:39:46.094483', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:39:46,219, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:39:46,223, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:39:46,353, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:39:46,354, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:39:46,834, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:39:49,989, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:40:02,957, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:40:02,973, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:40:03,255, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:40:03,256, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:40:03,260, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:40:03,266, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:40:03,507, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:40:03,509, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:40:04,341, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:40:09,524, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:40:09,701, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:40:09,703, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:40:09.703512', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:40:09,809, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:40:09,813, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:40:09,933, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:40:09,935, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:40:10,345, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:40:13,233, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:40:25,788, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:40:25,804, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:40:26,155, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:40:26,157, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:40:26,161, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:40:26,170, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:40:26,471, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:40:26,472, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:40:27,513, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:40:36,207, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:40:36,390, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:40:36,391, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:40:36.391055', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:40:36,501, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:40:36,504, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:40:36,628, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:40:36,630, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:40:37,046, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:40:41,180, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:40:53,768, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:40:53,784, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:40:54,131, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:40:54,133, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:40:54,136, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:40:54,142, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:40:54,376, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:40:54,377, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:40:55,227, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:41:03,614, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:41:03,789, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:41:03,791, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:41:03.791520', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:41:04,018, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:41:04,022, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:41:04,144, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:41:04,145, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:41:04,714, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:41:08,870, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:41:20,771, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:41:20,799, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:41:21,204, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:41:21,206, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:41:21,211, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:41:21,216, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:41:21,490, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:41:21,491, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:41:22,466, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:41:31,065, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:41:31,278, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:41:31,279, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:41:31.279304', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:41:31,407, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:41:31,410, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:41:31,548, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:41:31,550, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:41:32,012, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:41:35,706, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:41:47,802, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:41:47,831, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:41:48,226, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:41:48,228, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:41:48,231, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:41:48,237, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:41:48,506, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:41:48,507, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:41:49,551, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:41:56,020, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:41:56,274, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:41:56,275, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:41:56.275288', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:41:56,401, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:41:56,405, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:41:56,542, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:41:56,543, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:41:57,024, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:41:59,949, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:42:11,995, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:42:12,014, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:42:12,356, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:42:12,357, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:42:12,361, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:42:12,368, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:42:12,600, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:42:12,601, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:42:13,468, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:42:20,008, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:42:20,183, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:42:20,185, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:42:20.185022', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:42:20,291, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:42:20,293, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:42:20,405, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:42:20,407, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:42:20,824, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:42:23,809, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:42:36,378, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:42:36,392, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:42:36,706, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:42:36,707, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:42:36,712, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:42:36,719, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:42:36,989, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:42:36,990, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:42:38,077, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:42:44,724, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:42:44,915, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:42:44,916, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:42:44.916607', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:42:45,025, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:42:45,028, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:42:45,147, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:42:45,149, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:42:45,631, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:42:48,449, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:43:02,678, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:43:02,704, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:43:03,037, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:43:03,038, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:43:03,042, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:43:03,048, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:43:03,316, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:43:03,317, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:43:04,222, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:43:13,164, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:43:13,375, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:43:13,376, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:43:13.376211', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:43:13,514, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:43:13,518, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:43:13,644, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:43:13,645, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:43:14,072, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:43:18,440, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:43:30,760, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:43:30,769, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:43:30,895, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:43:30,897, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:43:30,900, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:43:30,907, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:43:31,136, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:43:31,137, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:43:31,969, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:43:40,512, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:43:40,688, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:43:40,689, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:43:40.689742', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:43:40,808, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:43:40,811, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:43:40,946, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:43:40,947, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:43:41,424, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:43:45,459, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:43:57,128, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:43:57,139, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:43:57,274, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:43:57,276, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:43:57,279, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:43:57,285, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:43:57,516, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:43:57,517, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:43:58,351, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:44:07,567, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:44:07,823, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:44:07,824, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:44:07.824268', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:44:07,945, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:44:07,949, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:44:08,090, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:44:08,092, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:44:08,539, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:44:12,651, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:44:24,937, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:44:24,947, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:44:25,092, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:44:25,094, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:44:25,098, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:44:25,103, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:44:25,348, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:44:25,350, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:44:26,197, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:44:34,624, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:44:34,803, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:44:34,803, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:44:34.803999', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:44:34,925, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:44:34,930, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:44:35,044, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:44:35,047, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:44:35,463, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:44:39,482, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:44:51,605, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:44:51,617, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:44:51,859, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:44:51,861, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:44:51,865, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:44:51,870, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:44:52,119, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:44:52,121, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:44:52,966, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:45:01,531, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:45:01,746, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:45:01,747, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:45:01.747365', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:45:01,878, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:45:01,882, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:45:02,033, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:45:02,035, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:45:02,502, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:45:06,430, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:45:18,697, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:45:18,711, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:45:18,978, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:45:18,980, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:45:18,983, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:45:18,989, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:45:19,270, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:45:19,271, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:45:20,305, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:45:29,155, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:45:29,370, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:45:29,372, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:45:29.372772', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:45:29,498, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:45:29,501, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:45:29,636, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:45:29,638, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:45:30,118, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:45:34,161, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:45:48,416, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:45:48,430, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:45:48,726, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:45:48,728, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:45:48,732, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:45:48,739, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:45:49,011, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:45:49,012, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:45:49,990, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:45:57,184, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:45:57,398, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:45:57,399, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:45:57.399778', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:45:57,527, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:45:57,531, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:45:57,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:45:57,669, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:45:58,164, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:46:01,425, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:46:15,512, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:46:15,532, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:46:15,972, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:46:15,974, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:46:15,978, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:46:15,984, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:46:16,262, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:46:16,263, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:46:17,322, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:46:25,285, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:46:25,494, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:46:25,496, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:46:25.496400', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:46:25,602, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:46:25,606, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:46:25,722, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:46:25,723, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:46:26,191, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:46:29,877, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:46:42,405, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:46:42,421, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:46:42,841, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:46:42,843, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:46:42,847, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:46:42,853, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:46:43,123, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:46:43,125, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:46:44,078, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:46:51,214, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:46:51,419, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:46:51,420, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:46:51.420175', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:46:51,527, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:46:51,530, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:46:51,644, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:46:51,645, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:46:52,058, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:46:55,666, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:47:07,660, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:47:07,679, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:47:08,111, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:47:08,113, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:47:08,116, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:47:08,121, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:47:08,394, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:47:08,395, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:47:09,403, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:47:16,333, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:47:16,555, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:47:16,557, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:47:16.557628', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:47:16,687, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:47:16,691, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:47:16,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:47:16,835, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:47:17,305, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:47:20,427, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:47:32,791, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:47:32,806, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:47:33,136, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:47:33,138, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:47:33,144, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:47:33,150, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:47:33,422, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:47:33,424, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:47:34,326, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:47:40,799, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:47:40,975, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:47:40,977, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:47:40.977588', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:47:41,092, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:47:41,095, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:47:41,208, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:47:41,209, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:47:42,143, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:47:46,668, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:47:58,960, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:47:58,974, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:47:59,256, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:47:59,259, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:47:59,262, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:47:59,266, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:47:59,502, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:47:59,503, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:48:00,362, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:48:06,884, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:48:07,060, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:48:07,061, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:48:07.061035', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:48:07,177, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:48:07,180, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:48:07,323, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:48:07,324, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:48:07,810, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:48:10,785, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:48:23,383, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:48:23,395, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:48:23,709, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:48:23,710, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:48:23,714, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:48:23,720, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:48:24,017, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:48:24,017, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:48:24,969, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:48:31,564, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:48:31,775, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:48:31,776, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:48:31.776436', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:48:31,902, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:48:31,906, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:48:32,042, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:48:32,043, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:48:32,536, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:48:35,610, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:48:47,840, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:48:47,858, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:48:48,241, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:48:48,243, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:48:48,248, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:48:48,254, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:48:48,525, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:48:48,526, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:48:49,548, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:48:55,155, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:48:55,334, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:48:55,335, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:48:55.335223', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:48:55,445, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:48:55,450, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:48:55,562, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:48:55,564, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:48:55,972, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:48:58,911, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:49:11,327, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:49:11,342, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:49:11,682, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:49:11,684, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:49:11,688, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:49:11,694, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:49:11,924, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:49:11,926, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:49:13,011, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:49:19,677, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:49:19,887, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:49:19,888, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:49:19.888030', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:49:20,040, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:49:20,044, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:49:20,196, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:49:20,197, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:49:20,824, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:49:23,952, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:49:36,386, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:49:36,403, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:49:36,813, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:49:36,814, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:49:36,819, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:49:36,825, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:49:37,115, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:49:37,117, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:49:38,160, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:49:42,821, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:49:43,009, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:49:43,010, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:49:43.010643', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:49:43,136, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:49:43,140, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:49:43,284, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:49:43,285, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:49:43,794, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:49:45,940, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:49:57,808, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:49:57,838, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:49:58,195, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:49:58,197, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:49:58,201, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:49:58,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:49:58,477, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:49:58,478, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:49:59,589, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:50:04,322, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:50:04,531, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:50:04,533, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:50:04.532415', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:50:04,657, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:50:04,661, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:50:04,797, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:50:04,798, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:50:05,330, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:50:07,548, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:50:20,478, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:50:20,495, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:50:20,851, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:50:20,853, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:50:20,856, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:50:20,862, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:50:21,141, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:50:21,142, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:50:22,161, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:50:26,523, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:50:26,713, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:50:26,714, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:50:26.714940', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:50:26,826, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:50:26,829, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:50:26,946, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:50:26,947, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:50:27,366, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:50:29,584, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:50:41,773, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:50:41,790, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:50:42,418, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:50:42,420, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:50:42,424, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:50:42,429, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:50:42,698, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:50:42,699, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:50:43,736, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:50:50,154, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:50:50,325, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 16:50:50,326, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T16:50:50.326301', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:50:50,554, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:50:50,557, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:50:50,674, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:50:50,676, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:50:51,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:50:56,343, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:51:08,316, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:51:08,331, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 16:51:08,643, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:51:08,645, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:51:08,649, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:51:08,655, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:51:08,888, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:51:08,890, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:51:09,832, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:51:15,800, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:51:16,005, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 16:51:16,006, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T16:51:16.006965', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:51:16,143, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:51:16,146, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:51:16,283, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:51:16,285, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:51:16,768, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:51:19,887, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:51:32,157, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:51:32,172, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 16:51:32,512, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:51:32,514, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:51:32,518, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:51:32,524, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:51:32,793, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:51:32,794, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:51:33,738, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:51:40,122, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:51:40,352, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 16:51:40,353, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T16:51:40.353379', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:51:40,480, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:51:40,483, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:51:40,618, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:51:40,619, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:51:41,098, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:51:44,308, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:51:56,374, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:51:56,388, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 16:51:56,937, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 16:51:56,939, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 16:51:56,941, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:51:56,948, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:51:57,285, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:51:57,286, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:51:58,626, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:52:05,879, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 16:52:06,454, dictionary, INFO, built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions) ]
[2024-12-29 16:52:06,455, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions)", 'datetime': '2024-12-29T16:52:06.455544', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 16:52:06,819, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:52:06,827, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:52:07,227, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:52:07,228, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:52:08,745, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:52:15,990, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 16:52:28,470, text_analysis, INFO, 61 batches submitted to accumulate stats from 3904 documents (-93340 virtual) ]
[2024-12-29 16:52:28,934, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 16:52:28,965, text_analysis, INFO, accumulated word occurrence stats for 113005 virtual documents ]
[2024-12-29 16:52:29,327, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:52:29,339, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:52:29,761, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:52:29,762, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:52:31,315, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 16:53:06,305, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 16:53:06,314, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 16:53:06,748, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 16:53:06,749, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 16:53:08,340, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:03:41,051, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:03:41,054, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:03:41,089, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:03:41,092, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:03:41,098, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:03:41,106, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:03:41,384, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:03:41,385, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:03:42,268, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:03:48,649, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:03:48,850, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:03:48,852, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:03:48.852065', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:03:48,951, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:03:48,954, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:03:49,069, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:03:49,071, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:03:49,487, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:03:52,674, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:04:06,330, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:04:06,347, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:04:06,719, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:04:06,722, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:04:06,726, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:04:06,732, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:04:07,010, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:04:07,011, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:04:08,042, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:04:13,946, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:04:14,157, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:04:14,159, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:04:14.158563', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:04:14,282, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:04:14,286, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:04:14,411, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:04:14,413, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:04:14,879, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:04:17,619, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:04:30,097, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:04:30,113, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:04:30,386, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:04:30,388, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:04:30,391, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:04:30,397, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:04:30,619, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:04:30,620, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:04:31,474, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:04:37,416, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:04:37,610, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:04:37,611, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:04:37.611505', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:04:37,715, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:04:37,720, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:04:37,837, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:04:37,839, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:04:38,272, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:04:43,204, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:04:56,641, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:04:56,657, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:04:56,925, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:04:56,927, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:04:56,930, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:04:56,935, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:04:57,149, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:04:57,150, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:04:57,913, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:05:05,994, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:05:06,181, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:05:06,182, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:05:06.182287', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:05:06,289, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:05:06,291, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:05:06,405, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:05:06,406, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:05:06,831, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:05:10,356, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:05:22,248, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:05:22,267, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:05:22,599, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:05:22,601, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:05:22,605, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:05:22,610, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:05:22,838, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:05:22,840, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:05:23,641, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:05:31,067, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:05:31,239, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:05:31,240, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:05:31.240194', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:05:31,341, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:05:31,345, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:05:31,457, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:05:31,458, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:05:31,840, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:05:35,245, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:05:46,765, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:05:46,783, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:05:47,107, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:05:47,109, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:05:47,112, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:05:47,117, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:05:47,342, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:05:47,343, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:05:48,317, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:05:55,823, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:05:55,993, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:05:55,994, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:05:55.994506', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:05:56,097, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:05:56,100, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:05:56,212, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:05:56,213, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:05:56,633, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:06:00,161, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:06:12,056, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:06:12,073, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:06:12,416, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:06:12,418, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:06:12,421, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:06:12,426, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:06:12,649, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:06:12,650, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:06:13,467, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:06:21,211, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:06:21,380, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:06:21,381, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:06:21.381117', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:06:21,482, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:06:21,485, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:06:21,597, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:06:21,599, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:06:21,997, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:06:24,705, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:06:36,223, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:06:36,240, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:06:36,532, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:06:36,535, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:06:36,538, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:06:36,543, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:06:36,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:06:36,785, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:06:37,685, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:06:42,794, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:06:43,012, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:06:43,014, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:06:43.013244', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:06:43,114, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:06:43,117, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:06:43,231, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:06:43,233, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:06:43,648, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:06:46,287, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:07:00,172, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:07:00,188, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:07:00,492, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:07:00,494, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:07:00,499, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:07:00,505, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:07:00,732, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:07:00,733, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:07:01,555, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:07:08,882, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:07:09,067, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:07:09,069, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:07:09.069846', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:07:09,177, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:07:09,181, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:07:09,297, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:07:09,298, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:07:09,756, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:07:12,419, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:07:24,136, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:07:24,171, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:07:24,556, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:07:24,557, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:07:24,561, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:07:24,568, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:07:24,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:07:24,834, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:07:25,785, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:07:37,137, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:07:37,360, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:07:37,361, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:07:37.361715', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:07:37,497, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:07:37,502, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:07:37,639, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:07:37,640, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:07:38,118, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:07:41,786, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:07:53,297, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:07:53,309, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:07:53,452, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:07:53,454, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:07:53,458, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:07:53,463, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:07:53,697, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:07:53,698, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:07:54,557, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:08:02,479, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:08:02,657, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:08:02,658, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:08:02.658731', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:08:02,763, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:08:02,767, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:08:02,883, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:08:02,885, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:08:03,301, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:08:06,731, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:08:18,448, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:08:18,461, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:08:18,609, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:08:18,611, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:08:18,615, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:08:18,620, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:08:18,870, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:08:18,872, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:08:19,797, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:08:27,782, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:08:27,960, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:08:27,961, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:08:27.961527', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:08:28,083, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:08:28,087, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:08:28,203, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:08:28,204, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:08:28,638, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:08:32,154, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:08:43,813, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:08:43,826, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:08:43,967, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:08:43,969, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:08:43,972, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:08:43,977, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:08:44,250, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:08:44,252, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:08:45,092, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:08:52,729, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:08:52,903, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:08:52,904, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:08:52.904079', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:08:53,009, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:08:53,012, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:08:53,128, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:08:53,129, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:08:53,547, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:08:57,123, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:09:10,013, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:09:10,090, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:09:10,959, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:09:10,961, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:09:10,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:09:10,971, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:09:11,198, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:09:11,199, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:09:12,054, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:09:20,044, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:09:20,259, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:09:20,260, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:09:20.260704', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:09:20,362, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:09:20,366, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:09:20,482, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:09:20,483, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:09:20,907, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:09:24,526, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:09:36,177, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:09:36,192, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:09:36,410, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:09:36,412, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:09:36,415, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:09:36,419, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:09:36,668, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:09:36,669, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:09:37,524, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:09:45,047, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:09:45,224, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:09:45,225, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:09:45.225436', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:09:45,330, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:09:45,333, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:09:45,445, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:09:45,446, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:09:45,859, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:09:49,619, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:10:01,331, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:10:01,346, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:10:01,599, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:10:01,600, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:10:01,604, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:10:01,610, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:10:01,861, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:10:01,862, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:10:02,721, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:10:09,407, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:10:09,587, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:10:09,588, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:10:09.588698', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:10:09,695, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:10:09,698, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:10:09,816, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:10:09,817, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:10:10,230, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:10:13,438, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:10:26,709, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:10:26,730, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:10:27,091, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:10:27,093, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:10:27,097, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:10:27,102, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:10:27,332, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:10:27,333, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:10:28,164, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:10:34,994, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:10:35,184, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:10:35,185, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:10:35.185135', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:10:35,311, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:10:35,314, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:10:35,445, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:10:35,446, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:10:35,884, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:10:39,140, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:10:51,181, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:10:51,199, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:10:51,584, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:10:51,585, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:10:51,590, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:10:51,602, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:10:51,881, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:10:51,882, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:10:53,019, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:11:03,600, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:11:03,811, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:11:03,812, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:11:03.812218', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:11:03,939, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:11:03,942, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:11:04,066, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:11:04,067, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:11:04,573, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:11:07,765, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:11:19,487, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:11:19,505, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:11:19,860, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:11:19,862, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:11:19,866, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:11:19,871, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:11:20,103, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:11:20,104, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:11:21,075, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:11:27,458, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:11:27,631, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:11:27,632, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:11:27.632729', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:11:27,737, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:11:27,741, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:11:27,852, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:11:27,853, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:11:28,262, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:11:30,954, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:11:42,707, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:11:42,726, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:11:43,030, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:11:43,032, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:11:43,036, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:11:43,041, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:11:43,274, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:11:43,275, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:11:44,112, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:11:50,351, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:11:50,525, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:11:50,526, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:11:50.526138', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:11:50,630, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:11:50,633, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:11:50,760, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:11:50,762, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:11:51,199, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:11:54,144, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:12:06,054, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:12:06,084, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:12:06,506, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:12:06,508, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:12:06,512, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:12:06,517, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:12:06,760, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:12:06,761, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:12:07,593, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:12:13,450, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:12:13,633, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:12:13,635, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:12:13.634113', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:12:13,740, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:12:13,744, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:12:13,860, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:12:13,861, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:12:14,288, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:12:17,180, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:12:31,381, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:12:31,397, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:12:31,688, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:12:31,690, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:12:31,692, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:12:31,697, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:12:31,932, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:12:31,934, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:12:34,776, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:12:40,729, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:12:40,910, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:12:40,911, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:12:40.911916', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:12:41,014, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:12:41,018, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:12:41,135, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:12:41,136, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:12:41,572, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:12:44,247, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:12:55,837, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:12:55,854, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:12:56,169, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:12:56,171, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:12:56,175, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:12:56,181, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:12:56,422, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:12:56,423, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:12:57,319, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:13:02,725, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:13:02,901, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:13:02,902, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:13:02.902930', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:13:03,011, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:13:03,014, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:13:03,129, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:13:03,131, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:13:03,524, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:13:06,496, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:13:18,031, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:13:18,045, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:13:18,380, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:13:18,381, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:13:18,385, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:13:18,390, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:13:18,618, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:13:18,619, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:13:19,407, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:13:25,827, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:13:26,021, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:13:26,022, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:13:26.022706', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:13:26,135, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:13:26,138, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:13:26,259, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:13:26,260, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:13:26,676, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:13:29,486, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:13:41,392, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:13:41,410, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:13:41,739, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:13:41,741, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:13:41,744, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:13:41,750, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:13:42,001, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:13:42,003, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:13:42,862, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:13:46,980, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:13:47,203, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:13:47,204, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:13:47.204014', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:13:47,319, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:13:47,322, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:13:47,446, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:13:47,447, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:13:47,868, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:13:49,884, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:14:01,458, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:14:01,476, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:14:01,801, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:14:01,803, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:14:01,806, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:14:01,812, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:14:02,043, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:14:02,044, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:14:02,876, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:14:07,242, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:14:07,420, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:14:07,421, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:14:07.421114', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:14:07,527, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:14:07,531, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:14:07,648, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:14:07,649, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:14:08,120, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:14:10,096, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:14:22,007, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:14:22,025, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:14:22,337, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:14:22,338, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:14:22,341, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:14:22,347, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:14:22,601, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:14:22,602, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:14:23,536, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:14:27,723, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:14:27,900, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:14:27,901, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:14:27.901536', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:14:28,013, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:14:28,015, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:14:28,131, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:14:28,133, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:14:28,534, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:14:30,604, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:14:44,471, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:14:44,489, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:14:45,011, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:14:45,013, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:14:45,016, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:14:45,021, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:14:45,246, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:14:45,247, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:14:46,122, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:14:52,279, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:14:52,460, dictionary, INFO, built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions) ]
[2024-12-29 17:14:52,461, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19072 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 1875 documents (total 160165 corpus positions)", 'datetime': '2024-12-29T17:14:52.461761', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:14:52,583, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:14:52,586, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:14:52,705, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:14:52,706, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:14:53,145, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:14:55,825, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:15:07,920, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:15:07,946, text_analysis, INFO, accumulated word occurrence stats for 38508 virtual documents ]
[2024-12-29 17:15:08,271, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:15:08,273, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:15:08,276, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:15:08,280, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:15:08,516, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:15:08,518, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:15:09,358, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:15:15,528, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:15:15,725, dictionary, INFO, built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions) ]
[2024-12-29 17:15:15,726, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19321 unique tokens: ['....simple', 'a', 'also', 'an', 'and']...> from 1875 documents (total 157038 corpus positions)", 'datetime': '2024-12-29T17:15:15.726507', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:15:15,833, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:15:15,836, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:15:15,952, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:15:15,953, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:15:16,346, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:15:19,039, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:15:30,780, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:15:30,797, text_analysis, INFO, accumulated word occurrence stats for 36250 virtual documents ]
[2024-12-29 17:15:31,107, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:15:31,109, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:15:31,113, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:15:31,119, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:15:31,354, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:15:31,356, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:15:32,175, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:15:38,306, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:15:38,490, dictionary, INFO, built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions) ]
[2024-12-29 17:15:38,491, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19361 unique tokens: [',excellent', 'Akira', 'BUT', 'Chirashi.Lamb', 'Garlic.']...> from 1875 documents (total 159779 corpus positions)", 'datetime': '2024-12-29T17:15:38.491509', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:15:38,596, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:15:38,599, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:15:38,722, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:15:38,723, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:15:39,133, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:15:41,938, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:15:53,693, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:15:53,712, text_analysis, INFO, accumulated word occurrence stats for 38247 virtual documents ]
[2024-12-29 17:15:54,334, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:15:54,335, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:15:54,339, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:15:54,345, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:15:54,685, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:15:54,686, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:15:56,014, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:16:06,920, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:16:07,449, dictionary, INFO, built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions) ]
[2024-12-29 17:16:07,450, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38648 unique tokens: ['Amazing', 'Ambience', 'Food', 'Leather', 'Mexican']...> from 5625 documents (total 476982 corpus positions)", 'datetime': '2024-12-29T17:16:07.450907', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:16:07,781, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:16:07,788, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:16:08,141, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:16:08,143, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:16:09,389, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:16:19,874, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:16:31,799, text_analysis, INFO, 61 batches submitted to accumulate stats from 3904 documents (-93340 virtual) ]
[2024-12-29 17:16:32,280, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:16:32,310, text_analysis, INFO, accumulated word occurrence stats for 113005 virtual documents ]
[2024-12-29 17:16:32,726, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:16:32,739, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:16:33,108, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:16:33,110, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:16:34,608, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:20:57,574, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:20:57,576, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:21:48,777, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:21:48,779, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:21:48,813, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:21:48,815, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:21:48,819, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:21:48,822, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:21:48,851, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:21:48,853, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:21:48,952, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:21:49,794, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:21:49,818, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:21:49,820, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:21:49.820297', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:21:49,834, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:21:49,837, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:21:49,853, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:21:49,855, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:21:49,906, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:21:50,261, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:22:02,312, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:22:02,325, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:22:02,490, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:22:02,492, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:22:02,496, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:02,496, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:02,524, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:02,525, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:02,621, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:03,330, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:22:03,355, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:22:03,356, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:22:03.356682', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:22:03,371, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:03,373, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:03,388, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:03,389, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:03,436, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:03,759, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:22:15,241, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:22:15,252, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:22:15,429, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:22:15,430, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:22:15,434, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:15,435, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:15,463, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:15,464, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:15,556, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:16,299, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:22:16,322, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:22:16,324, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:22:16.324490', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:22:16,339, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:16,340, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:16,355, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:16,356, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:16,427, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:16,749, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:22:28,146, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:22:28,156, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:22:28,320, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:22:28,321, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:22:28,325, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:28,326, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:28,353, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:28,354, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:28,444, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:29,269, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:22:29,292, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:22:29,293, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:22:29.293671', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:22:29,308, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:29,310, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:29,324, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:29,326, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:29,373, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:29,737, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:22:41,023, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:22:41,035, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:22:41,207, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:22:41,208, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:22:41,212, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:41,214, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:41,242, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:41,243, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:41,340, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:42,160, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:22:42,183, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:22:42,185, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:22:42.185370', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:22:42,199, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:42,201, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:42,215, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:42,216, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:42,264, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:42,634, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:22:53,778, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:22:53,788, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:22:53,962, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:22:53,963, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:22:53,966, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:53,968, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:53,995, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:53,996, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:54,090, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:54,947, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:22:54,970, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:22:54,971, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:22:54.971352', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:22:54,986, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:22:54,987, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:22:55,002, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:22:55,004, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:22:55,054, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:22:55,413, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:23:06,770, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:23:06,782, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:23:06,983, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:23:06,984, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:23:06,987, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:06,989, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:07,022, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:07,023, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:07,115, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:07,857, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:23:07,881, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:23:07,883, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:23:07.883442', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:23:07,897, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:07,899, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:07,913, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:07,915, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:07,963, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:08,289, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:23:19,495, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:23:19,506, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:23:19,692, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:23:19,693, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:23:19,696, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:19,698, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:19,726, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:19,727, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:19,823, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:20,557, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:23:20,583, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:23:20,585, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:23:20.585086', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:23:20,600, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:20,601, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:20,616, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:20,617, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:20,662, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:20,996, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:23:32,088, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:23:32,099, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:23:32,296, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:23:32,298, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:23:32,302, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:32,303, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:32,331, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:32,332, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:32,425, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:33,174, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:23:33,198, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:23:33,200, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:23:33.200418', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:23:33,214, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:33,216, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:33,231, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:33,232, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:33,281, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:33,600, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:23:44,802, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:23:44,813, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:23:44,991, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:23:44,993, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:23:44,996, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:44,997, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:45,025, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:45,026, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:45,119, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:45,965, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:23:45,990, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:23:45,991, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:23:45.991057', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:23:46,005, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:46,007, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:46,022, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:46,023, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:46,069, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:46,413, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:23:57,781, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:23:57,792, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:23:57,901, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:23:57,903, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:23:57,906, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:57,908, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:57,936, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:57,938, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:58,032, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:58,803, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:23:58,826, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:23:58,827, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:23:58.827970', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:23:58,840, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:23:58,843, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:23:58,857, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:23:58,859, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:23:58,905, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:23:59,255, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:24:10,694, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:24:10,705, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:24:10,831, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:24:10,832, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:24:10,836, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:24:10,838, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:24:10,864, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:24:10,866, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:24:10,956, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:24:11,803, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:24:11,827, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:24:11,828, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:24:11.828463', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:24:11,843, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:24:11,845, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:24:11,859, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:24:11,861, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:24:11,909, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:24:12,249, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:24:23,640, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:24:23,651, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:24:23,755, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:24:23,756, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:24:23,759, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:24:23,761, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:24:23,789, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:24:23,791, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:24:23,883, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:24:24,731, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:24:24,755, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:24:24,756, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:24:24.756950', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:24:24,771, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:24:24,773, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:24:24,788, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:24:24,789, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:24:24,837, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:24:25,177, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:24:36,478, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:24:36,487, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:24:36,632, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:24:36,634, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:24:36,637, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:24:36,638, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:24:36,666, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:24:36,667, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:24:36,761, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:24:37,719, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:24:37,745, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:24:37,747, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:24:37.747172', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:24:37,766, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:24:37,768, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:24:37,783, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:24:37,785, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:24:37,831, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:24:38,192, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:24:49,476, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:24:49,487, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:24:49,627, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:24:49,630, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:24:49,633, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:24:49,634, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:24:49,663, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:24:49,664, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:24:49,756, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:24:50,622, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:24:50,646, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:24:50,647, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:24:50.647539', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:24:50,661, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:24:50,663, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:24:50,678, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:24:50,680, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:24:50,726, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:24:51,084, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:25:02,302, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:25:02,313, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:25:02,454, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:25:02,456, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:25:02,458, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:02,460, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:02,488, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:02,490, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:02,583, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:03,354, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:25:03,378, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:25:03,379, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:25:03.379078', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:25:03,393, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:03,396, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:03,411, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:03,412, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:03,457, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:03,805, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:25:15,317, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:25:15,329, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:25:15,535, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:25:15,536, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:25:15,539, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:15,540, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:15,572, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:15,574, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:15,674, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:16,462, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:25:16,485, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:25:16,487, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:25:16.487227', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:25:16,503, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:16,504, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:16,519, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:16,520, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:16,567, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:16,908, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:25:28,207, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:25:28,217, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:25:28,411, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:25:28,413, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:25:28,415, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:28,417, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:28,444, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:28,445, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:28,538, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:29,392, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:25:29,416, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:25:29,417, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:25:29.417343', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:25:29,432, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:29,434, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:29,448, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:29,449, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:29,497, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:29,837, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:25:41,014, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:25:41,025, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:25:41,211, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:25:41,213, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:25:41,216, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:41,218, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:41,246, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:41,247, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:41,378, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:42,172, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:25:42,196, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:25:42,196, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:25:42.196323', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:25:42,212, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:42,214, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:42,228, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:42,229, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:42,289, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:42,636, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:25:53,847, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:25:53,858, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:25:54,026, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:25:54,028, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:25:54,031, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:54,032, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:54,061, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:54,062, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:54,158, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:54,893, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:25:54,914, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:25:54,914, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:25:54.914959', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:25:54,931, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:25:54,933, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:25:54,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:25:54,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:25:54,994, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:25:55,339, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:26:07,008, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:26:07,020, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:26:07,195, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:26:07,197, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:26:07,200, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:26:07,201, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:26:07,229, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:26:07,230, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:26:07,322, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:26:08,097, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:26:08,128, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:26:08,129, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:26:08.129693', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:26:08,145, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:26:08,146, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:26:08,161, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:26:08,162, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:26:08,210, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:26:08,552, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:26:19,897, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:26:19,910, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:26:20,098, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:26:20,099, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:26:20,103, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:26:20,105, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:26:20,132, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:26:20,133, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:26:20,226, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:26:21,007, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:26:21,030, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:26:21,032, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:26:21.032345', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:26:21,047, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:26:21,049, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:26:21,063, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:26:21,065, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:26:21,114, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:26:21,437, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:26:32,682, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:26:32,693, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:26:32,871, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:26:32,873, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:26:32,876, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:26:32,878, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:26:32,905, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:26:32,906, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:26:33,010, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:26:33,774, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:26:33,799, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:26:33,800, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:26:33.800936', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:26:33,813, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:26:33,815, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:26:33,830, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:26:33,833, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:26:33,881, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:26:34,221, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:26:45,425, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:26:45,436, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:26:45,607, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:26:45,609, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:26:45,613, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:26:45,616, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:26:45,643, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:26:45,644, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:26:45,737, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:26:46,515, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:26:46,539, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:26:46,539, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:26:46.539954', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:26:46,555, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:26:46,555, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:26:46,571, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:26:46,572, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:26:46,776, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:26:48,210, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:27:01,550, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:27:01,562, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:27:01,748, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:27:01,749, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:27:01,751, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:01,753, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:01,781, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:01,782, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:01,875, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:02,636, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:27:02,660, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:27:02,662, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:27:02.662831', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:27:02,676, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:02,678, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:02,693, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:02,694, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:02,741, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:03,074, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:27:14,224, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:27:14,235, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:27:14,437, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:27:14,439, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:27:14,441, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:14,444, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:14,471, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:14,472, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:14,564, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:15,359, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:27:15,382, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:27:15,383, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:27:15.383215', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:27:15,397, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:15,397, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:15,413, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:15,414, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:15,470, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:15,821, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:27:26,861, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:27:26,871, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:27:27,036, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:27:27,037, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:27:27,040, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:27,042, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:27,070, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:27,070, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:27,161, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:27,879, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:27:27,902, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:27:27,903, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:27:27.903520', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:27:27,917, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:27,920, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:27,934, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:27,935, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:27,982, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:28,301, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:27:39,381, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:27:39,393, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:27:39,801, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:27:39,803, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:27:39,807, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:39,808, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:39,836, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:39,837, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:39,930, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:40,671, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:27:40,693, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:27:40,694, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:27:40.694925', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:27:40,708, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:40,710, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:40,725, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:40,726, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:40,776, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:41,137, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:27:54,396, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:27:54,406, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:27:54,583, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:27:54,585, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:27:54,587, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:54,589, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:54,617, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:54,618, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:54,712, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:55,467, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:27:55,489, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:27:55,491, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:27:55.491865', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:27:55,505, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:27:55,506, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:27:55,520, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:27:55,522, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:27:55,566, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:27:55,901, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:28:07,041, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:28:07,053, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:28:07,236, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:28:07,238, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:28:07,241, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:28:07,243, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:28:07,271, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:28:07,272, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:28:07,364, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:28:08,141, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:28:08,165, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:28:08,166, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:28:08.166499', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:28:08,180, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:28:08,182, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:28:08,197, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:28:08,198, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:28:08,245, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:28:08,586, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:28:19,634, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:28:19,645, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:28:20,107, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:28:20,109, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:28:20,111, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:28:20,113, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:28:20,210, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:28:20,211, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:28:20,355, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:28:21,683, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:28:21,750, dictionary, INFO, built Dictionary<7297 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 1804 documents (total 54802 corpus positions) ]
[2024-12-29 17:28:21,751, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7297 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 1804 documents (total 54802 corpus positions)", 'datetime': '2024-12-29T17:28:21.751654', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:28:21,792, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:28:21,794, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:28:21,833, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:28:21,834, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:28:21,973, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:28:23,229, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:28:34,558, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:28:34,585, text_analysis, INFO, accumulated word occurrence stats for 1887 virtual documents ]
[2024-12-29 17:28:35,032, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:28:35,036, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:28:35,084, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:28:35,085, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:28:35,226, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:31:45,185, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:31:45,188, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:31:47,055, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:31:47,057, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:31:47,089, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:31:47,092, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:31:47,097, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:31:47,098, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:31:47,134, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:31:47,135, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:31:47,245, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:31:48,020, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:31:48,048, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:31:48,049, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:31:48.049364', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:31:48,064, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:31:48,066, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:31:48,081, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:31:48,083, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:31:48,131, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:31:48,470, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:32:01,329, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:32:01,351, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:32:01,567, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:32:01,569, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:32:01,571, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:01,574, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:01,604, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:01,605, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:01,736, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:02,565, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:32:02,590, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:32:02,592, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:32:02.592352', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:32:02,608, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:02,611, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:02,627, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:02,629, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:02,686, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:03,044, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:32:14,766, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:32:14,779, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:32:14,987, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:32:14,990, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:32:14,993, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:14,994, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:15,028, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:15,030, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:15,125, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:15,866, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:32:15,890, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:32:15,891, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:32:15.891245', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:32:15,908, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:15,910, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:15,927, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:15,929, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:15,980, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:16,341, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:32:28,283, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:32:28,299, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:32:28,521, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:32:28,523, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:32:28,526, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:28,528, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:28,556, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:28,558, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:28,652, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:29,595, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:32:29,620, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:32:29,622, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:32:29.622852', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:32:29,638, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:29,640, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:29,656, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:29,658, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:29,708, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:30,077, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:32:41,902, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:32:41,919, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:32:42,112, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:32:42,114, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:32:42,117, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:42,119, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:42,149, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:42,150, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:42,247, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:43,147, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:32:43,180, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:32:43,182, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:32:43.182525', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:32:43,202, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:43,205, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:43,225, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:43,226, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:43,283, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:43,675, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:32:55,959, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:32:55,970, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:32:56,156, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:32:56,158, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:32:56,161, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:56,164, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:56,193, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:56,194, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:56,291, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:57,142, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:32:57,168, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:32:57,169, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:32:57.169070', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:32:57,184, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:32:57,186, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:32:57,202, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:32:57,204, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:32:57,258, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:32:57,658, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:33:10,154, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:33:10,165, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:33:10,360, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:33:10,362, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:33:10,366, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:33:10,367, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:33:10,397, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:33:10,399, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:33:10,504, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:33:11,268, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:33:11,297, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:33:11,299, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:33:11.299070', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:33:11,312, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:33:11,314, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:33:11,330, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:33:11,331, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:33:11,382, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:33:11,716, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:33:22,916, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:33:22,927, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:33:23,151, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:33:23,154, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:33:23,157, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:33:23,159, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:33:23,187, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:33:23,188, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:33:23,287, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:33:24,078, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:33:24,103, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:33:24,105, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:33:24.104289', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:33:24,120, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:33:24,122, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:33:24,137, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:33:24,138, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:33:24,198, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:33:24,542, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:33:36,010, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:33:36,021, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:33:36,214, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:33:36,216, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:33:36,219, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:33:36,221, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:33:36,290, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:33:36,293, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:33:36,409, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:33:37,182, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:33:37,215, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:33:37,216, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:33:37.216854', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:33:37,233, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:33:37,236, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:33:37,253, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:33:37,255, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:33:37,318, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:33:37,738, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:33:50,091, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:33:50,102, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:33:50,296, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:33:50,298, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:33:50,300, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:33:50,302, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:33:50,331, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:33:50,332, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:33:50,425, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:33:51,356, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:33:51,389, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:33:51,390, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:33:51.390843', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:33:51,408, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:33:51,411, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:33:51,429, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:33:51,430, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:33:51,505, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:33:51,938, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:34:03,705, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:34:03,717, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:34:03,842, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:34:03,844, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:34:03,848, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:03,850, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:03,884, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:03,885, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:04,053, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:05,012, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:34:05,036, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:34:05,037, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:34:05.037290', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:34:05,053, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:05,056, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:05,071, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:05,072, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:05,125, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:05,486, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:34:17,140, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:34:17,151, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:34:17,269, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:34:17,271, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:34:17,274, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:17,276, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:17,303, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:17,305, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:17,396, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:18,322, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:34:18,351, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:34:18,353, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:34:18.353648', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:34:18,371, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:18,373, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:18,395, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:18,397, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:18,453, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:18,876, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:34:30,690, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:34:30,702, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:34:30,822, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:34:30,824, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:34:30,827, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:30,829, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:30,867, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:30,868, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:30,962, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:31,972, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:34:32,001, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:34:32,003, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:34:32.003563', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:34:32,019, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:32,021, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:32,040, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:32,041, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:32,099, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:32,525, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:34:44,192, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:34:44,204, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:34:44,373, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:34:44,375, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:34:44,378, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:44,379, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:44,413, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:44,414, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:44,538, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:45,478, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:34:45,506, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:34:45,507, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:34:45.507691', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:34:45,524, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:45,525, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:45,543, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:45,544, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:45,604, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:46,016, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:34:57,621, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:34:57,631, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:34:57,769, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:34:57,771, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:34:57,774, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:57,776, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:57,804, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:57,805, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:57,909, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:58,793, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:34:58,824, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:34:58,825, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:34:58.825753', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:34:58,840, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:34:58,841, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:34:58,858, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:34:58,859, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:34:58,913, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:34:59,287, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:35:11,264, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:35:11,275, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:35:11,420, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:35:11,421, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:35:11,424, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:35:11,425, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:35:11,454, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:35:11,455, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:35:11,552, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:35:12,492, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:35:12,521, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:35:12,522, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:35:12.522755', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:35:12,539, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:35:12,541, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:35:12,559, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:35:12,560, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:35:12,617, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:35:13,021, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:35:24,636, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:35:24,647, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:35:24,877, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:35:24,879, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:35:24,882, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:35:24,884, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:35:24,918, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:35:24,919, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:35:25,035, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:35:25,941, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:35:25,964, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:35:25,965, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:35:25.965987', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:35:25,983, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:35:25,985, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:35:26,000, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:35:26,001, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:35:26,048, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:35:26,374, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:35:38,151, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:35:38,161, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:35:38,345, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:35:38,347, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:35:38,350, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:35:38,352, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:35:38,379, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:35:38,380, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:35:38,476, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:35:39,337, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:35:39,366, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:35:39,367, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:35:39.367395', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:35:39,384, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:35:39,385, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:35:39,404, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:35:39,405, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:35:39,475, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:35:39,887, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:35:51,555, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:35:51,568, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:35:51,791, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:35:51,792, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:35:51,795, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:35:51,797, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:35:51,831, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:35:51,832, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:35:51,943, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:35:52,899, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:35:52,931, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:35:52,932, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:35:52.932387', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:35:52,950, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:35:52,952, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:35:52,970, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:35:52,971, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:35:53,037, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:35:53,404, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:36:05,129, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:36:05,139, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:36:05,319, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:36:05,321, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:36:05,324, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:36:05,325, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:36:05,355, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:36:05,357, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:36:05,475, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:36:06,251, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:36:06,275, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:36:06,276, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:36:06.276883', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:36:06,292, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:36:06,295, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:36:06,310, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:36:06,311, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:36:06,363, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:36:06,689, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:36:18,604, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:36:18,614, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:36:18,779, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:36:18,781, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:36:18,783, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:36:18,785, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:36:18,813, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:36:18,814, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:36:18,908, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:36:19,862, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:36:19,891, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:36:19,893, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:36:19.893270', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:36:19,909, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:36:19,911, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:36:19,930, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:36:19,932, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:36:19,989, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:36:20,414, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:36:33,583, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:36:33,593, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:36:33,765, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:36:33,767, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:36:33,769, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:36:33,771, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:36:33,800, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:36:33,801, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:36:33,893, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:36:34,634, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:36:34,663, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:36:34,664, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:36:34.664582', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:36:34,682, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:36:34,684, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:36:34,702, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:36:34,703, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:36:34,762, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:36:35,167, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:36:49,015, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:36:49,025, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:36:49,199, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:36:49,202, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:36:49,205, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:36:49,207, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:36:49,236, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:36:49,237, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:36:49,336, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:36:50,147, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:36:50,181, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:36:50,182, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:36:50.182092', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:36:50,199, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:36:50,201, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:36:50,219, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:36:50,219, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:36:50,276, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:36:50,688, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:37:02,587, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:37:02,597, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:37:02,778, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:37:02,779, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:37:02,783, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:02,784, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:02,811, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:02,812, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:02,909, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:37:03,656, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:37:03,680, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:37:03,682, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:37:03.682285', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:37:03,697, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:03,700, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:03,715, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:03,717, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:03,768, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:37:04,090, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:37:16,031, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:37:16,041, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:37:16,241, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:37:16,243, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:37:16,246, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:16,248, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:16,275, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:16,276, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:16,372, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:37:17,267, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:37:17,307, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:37:17,308, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:37:17.308151', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:37:17,326, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:17,328, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:17,345, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:17,346, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:17,402, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:37:17,822, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:37:29,457, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:37:29,468, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:37:29,703, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:37:29,705, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:37:29,708, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:29,710, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:29,742, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:29,744, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:29,864, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:37:30,722, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:37:30,752, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:37:30,753, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:37:30.753381', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:37:30,770, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:30,771, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:30,791, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:30,793, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:30,851, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:37:31,247, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:37:42,960, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:37:42,971, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:37:43,138, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:37:43,140, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:37:43,143, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:43,145, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:43,174, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:43,175, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:43,275, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:37:44,028, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:37:44,057, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:37:44,058, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:37:44.058187', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:37:44,076, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:44,077, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:44,094, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:44,096, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:44,156, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:37:44,553, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:37:56,521, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:37:56,531, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:37:57,052, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:37:57,054, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:37:57,056, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:37:57,058, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:37:57,086, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:37:57,087, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:37:57,464, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:38:00,064, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:38:00,089, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:38:00,090, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:38:00.090949', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:38:00,106, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:38:00,108, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:38:00,124, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:38:00,125, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:38:00,174, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:38:00,515, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:38:13,845, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:38:13,865, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:38:14,048, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:38:14,050, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:38:14,053, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:38:14,055, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:38:14,088, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:38:14,089, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:38:14,209, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:38:15,178, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:38:15,207, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:38:15,208, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:38:15.208732', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:38:15,231, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:38:15,232, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:38:15,251, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:38:15,252, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:38:15,310, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:38:15,728, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:38:27,311, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:38:27,324, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:38:27,962, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:38:27,968, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:38:27,981, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:38:27,987, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:38:28,127, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:38:28,131, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:38:28,554, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:38:30,712, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:38:30,737, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:38:30,738, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:38:30.738009', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:38:30,753, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:38:30,754, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:38:30,771, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:38:30,772, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:38:30,836, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:38:31,181, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:38:45,335, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:38:45,347, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:38:45,904, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:38:45,906, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:38:45,908, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:38:45,911, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:38:45,960, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:38:45,961, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:38:46,155, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:38:47,735, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:38:47,805, dictionary, INFO, built Dictionary<7297 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 1804 documents (total 54802 corpus positions) ]
[2024-12-29 17:38:47,809, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7297 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 1804 documents (total 54802 corpus positions)", 'datetime': '2024-12-29T17:38:47.809781', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:38:47,861, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:38:47,864, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:38:47,905, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:38:47,906, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:38:48,073, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:38:49,734, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:39:02,918, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:39:02,937, text_analysis, INFO, accumulated word occurrence stats for 1887 virtual documents ]
[2024-12-29 17:39:03,397, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:39:03,400, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:39:03,460, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:39:03,461, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:39:03,636, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:12,798, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:42:12,801, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:42:12,835, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:42:12,837, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:42:12,840, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:42:12,842, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:42:12,926, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:42:12,927, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:42:13,022, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:13,808, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:42:13,839, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:42:13,840, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:42:13.840276', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:42:13,858, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:42:13,859, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:42:13,878, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:42:13,879, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:42:13,940, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:14,350, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:42:27,156, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:42:27,167, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:42:27,349, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:42:27,350, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:42:27,353, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:42:27,355, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:42:27,384, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:42:27,385, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:42:27,488, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:28,387, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:42:28,411, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:42:28,412, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:42:28.412118', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:42:28,426, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:42:28,427, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:42:28,443, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:42:28,444, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:42:28,496, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:28,855, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:42:40,230, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:42:40,240, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:42:40,412, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:42:40,413, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:42:40,416, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:42:40,418, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:42:40,446, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:42:40,447, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:42:40,542, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:41,467, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:42:41,495, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:42:41,496, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:42:41.496788', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:42:41,513, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:42:41,514, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:42:41,532, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:42:41,534, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:42:41,590, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:41,987, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:42:53,453, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:42:53,463, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:42:53,637, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:42:53,638, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:42:53,642, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:42:53,645, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:42:53,672, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:42:53,673, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:42:53,765, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:54,551, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:42:54,579, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:42:54,580, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:42:54.580618', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:42:54,598, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:42:54,600, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:42:54,617, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:42:54,618, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:42:54,674, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:42:55,070, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:43:07,315, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:43:07,343, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:43:07,567, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:43:07,569, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:43:07,572, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:43:07,574, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:43:07,612, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:43:07,613, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:43:07,730, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:43:08,561, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:43:08,583, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:43:08,584, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:43:08.584021', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:43:08,599, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:43:08,600, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:43:08,615, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:43:08,616, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:43:08,662, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:43:08,993, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:43:20,549, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:43:20,561, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:43:20,813, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:43:20,815, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:43:20,818, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:43:20,820, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:43:20,853, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:43:20,854, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:43:20,966, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:43:21,856, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:43:21,882, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:43:21,884, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:43:21.884595', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:43:21,900, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:43:21,902, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:43:21,917, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:43:21,918, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:43:21,964, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:43:22,280, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:43:35,091, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:43:35,102, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:43:35,281, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:43:35,282, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:43:35,285, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:43:35,286, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:43:35,316, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:43:35,317, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:43:35,414, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:43:36,148, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:43:36,172, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:43:36,173, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:43:36.173789', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:43:36,186, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:43:36,189, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:43:36,203, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:43:36,205, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:43:36,252, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:43:36,617, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:43:48,617, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:43:48,628, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:43:48,844, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:43:48,845, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:43:48,849, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:43:48,850, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:43:48,883, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:43:48,885, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:43:48,996, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:43:49,894, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:43:49,923, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:43:49,924, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:43:49.924097', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:43:49,941, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:43:49,942, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:43:49,959, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:43:49,960, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:43:50,014, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:43:50,411, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:44:01,377, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:44:01,387, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:44:01,583, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:44:01,585, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:44:01,588, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:01,590, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:01,623, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:01,624, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:01,736, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:02,760, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:44:02,788, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:44:02,789, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:44:02.789007', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:44:02,808, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:02,808, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:02,826, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:02,827, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:02,884, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:03,283, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:44:15,100, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:44:15,112, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:44:15,309, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:44:15,311, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:44:15,314, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:15,315, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:15,349, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:15,350, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:15,468, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:16,349, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:44:16,377, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:44:16,380, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:44:16.380469', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:44:16,397, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:16,398, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:16,416, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:16,417, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:16,476, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:16,878, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:44:29,480, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:44:29,492, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:44:29,595, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:44:29,596, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:44:29,599, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:29,600, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:29,628, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:29,629, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:29,726, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:30,539, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:44:30,570, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:44:30,571, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:44:30.571409', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:44:30,587, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:30,589, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:30,642, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:30,643, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:30,708, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:31,110, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:44:43,240, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:44:43,253, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:44:43,373, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:44:43,375, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:44:43,378, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:43,380, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:43,412, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:43,413, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:43,520, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:44,456, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:44:44,479, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:44:44,480, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:44:44.480244', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:44:44,494, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:44,495, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:44,511, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:44,512, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:44,558, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:44,880, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:44:56,885, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:44:56,897, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:44:57,012, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:44:57,013, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:44:57,016, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:57,017, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:57,043, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:57,045, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:57,136, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:57,873, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:44:57,896, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:44:57,897, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:44:57.897356', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:44:57,912, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:44:57,914, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:44:57,929, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:44:57,930, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:44:57,975, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:44:58,306, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:45:10,463, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:45:10,482, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:45:10,654, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:45:10,656, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:45:10,658, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:45:10,660, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:45:10,690, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:45:10,691, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:45:10,798, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:45:11,719, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:45:11,748, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:45:11,749, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:45:11.749334', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:45:11,767, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:45:11,769, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:45:11,785, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:45:11,786, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:45:11,844, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:45:12,246, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:45:24,222, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:45:24,233, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:45:24,369, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:45:24,371, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:45:24,375, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:45:24,377, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:45:24,403, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:45:24,404, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:45:24,507, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:45:25,406, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:45:25,433, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:45:25,434, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:45:25.434593', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:45:25,454, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:45:25,455, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:45:25,474, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:45:25,475, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:45:25,533, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:45:25,924, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:45:37,313, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:45:37,324, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:45:37,467, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:45:37,468, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:45:37,471, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:45:37,472, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:45:37,500, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:45:37,501, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:45:37,595, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:45:38,399, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:45:38,428, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:45:38,429, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:45:38.429671', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:45:38,446, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:45:38,447, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:45:38,465, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:45:38,466, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:45:38,525, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:45:38,930, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:45:53,091, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:45:53,102, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:45:53,306, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:45:53,307, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:45:53,310, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:45:53,313, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:45:53,340, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:45:53,341, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:45:53,431, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:45:54,158, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:45:54,182, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:45:54,183, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:45:54.183832', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:45:54,197, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:45:54,200, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:45:54,213, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:45:54,215, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:45:54,260, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:45:54,579, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:46:05,967, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:46:05,977, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:46:06,165, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:46:06,167, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:46:06,170, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:46:06,172, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:46:06,199, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:46:06,200, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:46:06,293, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:46:07,117, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:46:07,146, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:46:07,148, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:46:07.148787', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:46:07,165, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:46:07,167, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:46:07,184, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:46:07,186, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:46:07,243, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:46:07,649, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:46:19,380, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:46:19,393, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:46:19,579, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:46:19,581, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:46:19,584, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:46:19,586, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:46:19,614, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:46:19,615, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:46:19,714, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:46:20,529, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:46:20,561, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:46:20,562, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:46:20.562207', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:46:20,579, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:46:20,580, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:46:20,598, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:46:20,600, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:46:20,656, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:46:21,049, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:46:33,149, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:46:33,160, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:46:33,336, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:46:33,338, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:46:33,341, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:46:33,343, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:46:33,371, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:46:33,372, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:46:33,464, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:46:34,168, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:46:34,191, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:46:34,192, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:46:34.192348', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:46:34,206, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:46:34,209, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:46:34,222, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:46:34,223, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:46:34,287, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:46:34,613, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:46:46,533, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:46:46,546, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:46:46,735, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:46:46,736, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:46:46,740, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:46:46,741, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:46:46,774, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:46:46,775, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:46:46,887, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:46:47,781, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:46:47,810, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:46:47,811, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:46:47.811112', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:46:47,827, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:46:47,829, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:46:47,846, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:46:47,848, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:46:47,906, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:46:48,224, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:46:59,932, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:46:59,944, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:47:00,168, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:47:00,170, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:47:00,173, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:00,175, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:00,208, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:00,209, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:00,318, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:01,202, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:47:01,230, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:47:01,231, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:47:01.231418', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:47:01,249, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:01,250, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:01,268, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:01,269, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:01,325, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:01,714, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:47:13,586, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:47:13,598, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:47:13,808, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:47:13,810, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:47:13,813, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:13,815, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:13,848, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:13,849, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:13,951, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:14,754, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:47:14,783, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:47:14,784, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:47:14.784250', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:47:14,802, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:14,803, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:14,821, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:14,822, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:14,875, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:15,270, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:47:26,921, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:47:26,932, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:47:27,107, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:47:27,109, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:47:27,113, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:27,114, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:27,141, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:27,142, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:27,255, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:28,069, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:47:28,097, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:47:28,098, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:47:28.098696', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:47:28,115, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:28,117, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:28,134, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:28,136, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:28,190, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:28,577, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:47:41,862, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:47:41,872, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:47:42,064, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:47:42,066, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:47:42,069, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:42,071, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:42,097, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:42,098, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:42,191, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:42,942, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:47:42,972, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:47:42,973, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:47:42.973623', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:47:42,990, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:42,992, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:43,009, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:43,010, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:43,072, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:43,481, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:47:55,407, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:47:55,417, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:47:55,630, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:47:55,632, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:47:55,635, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:55,637, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:55,670, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:55,671, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:55,780, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:56,664, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:47:56,692, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:47:56,693, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:47:56.693616', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:47:56,709, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:47:56,711, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:47:56,728, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:47:56,730, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:47:56,784, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:47:57,121, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:48:09,376, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:48:09,388, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:48:09,593, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:48:09,594, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:48:09,598, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:48:09,599, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:48:09,634, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:48:09,635, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:48:09,745, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:48:10,625, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:48:10,648, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:48:10,648, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:48:10.648666', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:48:10,663, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:48:10,664, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:48:10,680, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:48:10,681, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:48:10,728, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:48:11,053, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:48:22,928, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:48:22,939, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:48:23,398, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:48:23,400, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:48:23,402, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:48:23,404, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:48:23,431, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:48:23,432, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:48:23,537, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:48:24,298, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:48:24,320, dictionary, INFO, built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions) ]
[2024-12-29 17:48:24,321, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3614 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 602 documents (total 18434 corpus positions)", 'datetime': '2024-12-29T17:48:24.321453', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:48:24,336, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:48:24,338, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:48:24,353, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:48:24,354, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:48:24,402, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:48:24,737, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:48:36,312, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:48:36,322, text_analysis, INFO, accumulated word occurrence stats for 641 virtual documents ]
[2024-12-29 17:48:36,537, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:48:36,539, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:48:36,542, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:48:36,545, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:48:36,571, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:48:36,572, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:48:36,659, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:48:37,409, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:48:37,436, dictionary, INFO, built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions) ]
[2024-12-29 17:48:37,437, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3386 unique tokens: ['It', 'Must', 'Service', 'Visit', 'Waiters']...> from 601 documents (total 18032 corpus positions)", 'datetime': '2024-12-29T17:48:37.437989', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:48:37,454, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:48:37,457, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:48:37,473, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:48:37,475, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:48:37,528, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:48:37,961, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:48:49,775, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:48:49,784, text_analysis, INFO, accumulated word occurrence stats for 602 virtual documents ]
[2024-12-29 17:48:49,986, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:48:49,988, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:48:49,991, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:48:49,993, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:48:50,020, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:48:50,021, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:48:50,111, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:48:50,958, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:48:50,981, dictionary, INFO, built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions) ]
[2024-12-29 17:48:50,982, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<3594 unique tokens: ['Food', 'The', 'Would', 'attentive.', 'definitely']...> from 601 documents (total 18336 corpus positions)", 'datetime': '2024-12-29T17:48:50.982333', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:48:50,997, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:48:50,999, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:48:51,014, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:48:51,015, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:48:51,066, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:48:51,399, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:49:03,235, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:49:03,245, text_analysis, INFO, accumulated word occurrence stats for 644 virtual documents ]
[2024-12-29 17:49:03,717, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:49:03,719, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:49:03,722, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:49:03,725, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:49:03,765, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:49:03,766, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:49:03,907, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:49:05,086, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:49:05,155, dictionary, INFO, built Dictionary<7297 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 1804 documents (total 54802 corpus positions) ]
[2024-12-29 17:49:05,156, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<7297 unique tokens: ['a', 'and', 'breakfast', 'buffet', 'by']...> from 1804 documents (total 54802 corpus positions)", 'datetime': '2024-12-29T17:49:05.156132', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:49:05,197, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:49:05,199, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:49:05,247, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:49:05,248, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:49:05,387, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:49:06,804, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:49:18,643, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:49:18,663, text_analysis, INFO, accumulated word occurrence stats for 1887 virtual documents ]
[2024-12-29 17:49:19,127, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:49:19,130, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:49:19,183, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:49:19,185, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:49:19,349, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:50:30,682, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:50:30,685, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:50:30,717, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:50:30,719, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:50:30,724, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:50:30,733, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:50:31,000, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:50:31,001, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:50:31,836, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:50:35,340, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:50:35,532, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:50:35,533, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:50:35.533888', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:50:35,641, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:50:35,646, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:50:35,757, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:50:35,759, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:50:36,150, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:50:37,523, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:50:50,564, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:50:50,582, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:50:50,948, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:50:50,950, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:50:50,954, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:50:50,960, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:50:51,218, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:50:51,219, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:50:52,130, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:50:55,475, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:50:55,660, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:50:55,661, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:50:55.661939', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:50:55,769, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:50:55,772, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:50:55,894, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:50:55,896, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:50:56,415, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:50:57,995, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:51:10,618, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:51:10,647, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:51:11,019, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:51:11,021, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:51:11,025, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:51:11,030, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:51:11,296, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:51:11,297, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:51:12,155, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:51:15,535, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:51:15,704, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:51:15,705, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:51:15.705767', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:51:15,804, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:51:15,807, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:51:15,929, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:51:15,930, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:51:16,392, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:51:18,055, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:51:30,232, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:51:30,658, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:51:30,677, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:51:31,000, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:51:31,002, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:51:31,007, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:51:31,011, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:51:31,225, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:51:31,226, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:51:32,035, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:51:35,227, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:51:35,389, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:51:35,390, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:51:35.390917', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:51:35,483, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:51:35,487, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:51:35,599, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:51:35,600, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:51:35,982, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:51:37,434, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:51:49,659, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:51:49,679, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:51:50,073, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:51:50,075, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:51:50,079, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:51:50,084, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:51:50,348, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:51:50,349, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:51:51,260, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:51:54,744, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:51:54,918, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:51:54,920, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:51:54.920141', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:51:55,014, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:51:55,017, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:51:55,116, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:51:55,118, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:51:55,475, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:51:57,013, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:52:09,927, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:52:09,956, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:52:10,353, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:52:10,354, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:52:10,359, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:52:10,364, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:52:10,620, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:52:10,621, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:52:11,503, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:52:14,867, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:52:15,062, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:52:15,064, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:52:15.064641', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:52:15,179, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:52:15,183, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:52:15,313, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:52:15,315, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:52:15,769, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:52:17,478, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:52:30,854, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:52:31,400, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:52:31,428, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:52:31,775, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:52:31,777, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:52:31,780, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:52:31,785, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:52:32,035, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:52:32,036, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:52:33,051, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:52:36,685, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:52:36,878, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:52:36,879, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:52:36.879469', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:52:36,995, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:52:36,998, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:52:37,123, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:52:37,124, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:52:37,573, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:52:39,097, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:52:51,933, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:52:51,953, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:52:52,347, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:52:52,349, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:52:52,353, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:52:52,359, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:52:52,614, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:52:52,615, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:52:53,567, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:52:57,138, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:52:57,323, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:52:57,324, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:52:57.324601', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:52:57,435, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:52:57,438, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:52:57,558, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:52:57,559, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:52:57,982, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:52:59,655, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:53:13,847, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:53:13,880, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:53:14,276, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:53:14,277, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:53:14,281, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:53:14,286, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:53:14,532, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:53:14,533, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:53:15,419, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:53:18,913, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:53:19,113, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:53:19,114, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:53:19.114464', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:53:19,231, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:53:19,235, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:53:19,366, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:53:19,367, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:53:19,963, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:53:21,664, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:53:33,526, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:53:33,997, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:53:34,034, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:53:34,457, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:53:34,459, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:53:34,462, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:53:34,467, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:53:34,729, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:53:34,730, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:53:35,613, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:53:39,503, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:53:39,696, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:53:39,697, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:53:39.697332', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:53:39,809, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:53:39,812, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:53:39,938, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:53:39,940, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:53:40,455, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:53:41,984, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:53:53,950, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:53:53,963, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:53:54,113, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:53:54,115, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:53:54,119, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:53:54,124, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:53:54,382, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:53:54,384, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:53:55,346, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:53:58,865, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:53:59,024, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:53:59,025, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:53:59.025752', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:53:59,118, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:53:59,121, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:53:59,221, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:53:59,222, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:53:59,663, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:54:01,275, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:54:13,518, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:54:13,530, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:54:13,681, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:54:13,683, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:54:13,688, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:54:13,692, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:54:13,964, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:54:13,965, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:54:14,733, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:54:18,271, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:54:18,467, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:54:18,468, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:54:18.468224', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:54:18,724, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:54:18,727, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:54:18,881, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:54:18,882, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:54:19,410, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:54:20,889, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:54:32,633, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:54:32,872, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:54:32,884, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:54:33,024, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:54:33,026, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:54:33,029, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:54:33,034, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:54:33,258, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:54:33,260, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:54:34,125, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:54:38,055, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:54:38,247, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:54:38,248, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:54:38.248517', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:54:38,360, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:54:38,363, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:54:38,467, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:54:38,468, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:54:38,841, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:54:40,255, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:54:52,462, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:54:52,479, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:54:52,763, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:54:52,765, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:54:52,769, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:54:52,775, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:54:53,044, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:54:53,045, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:54:53,959, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:54:57,488, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:54:57,673, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:54:57,674, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:54:57.674724', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:54:57,792, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:54:57,795, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:54:57,919, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:54:57,921, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:54:58,352, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:54:59,933, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:55:12,194, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:55:12,213, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:55:12,454, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:55:12,456, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:55:12,460, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:55:12,465, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:55:12,676, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:55:12,677, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:55:13,431, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:55:16,687, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:55:16,855, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:55:16,856, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:55:16.856665', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:55:16,952, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:55:16,955, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:55:17,058, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:55:17,060, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:55:17,441, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:55:18,887, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:55:30,291, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:55:30,620, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:55:30,638, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:55:30,919, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:55:30,920, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:55:30,924, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:55:30,929, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:55:31,178, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:55:31,180, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:55:32,053, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:55:35,672, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:55:35,866, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:55:35,867, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:55:35.867335', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:55:35,979, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:55:35,982, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:55:36,118, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:55:36,119, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:55:36,493, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:55:38,020, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:55:51,122, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:55:51,142, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:55:51,587, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:55:51,589, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:55:51,593, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:55:51,599, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:55:51,887, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:55:51,888, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:55:52,835, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:55:56,393, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:55:56,624, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:55:56,625, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:55:56.625937', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:55:56,771, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:55:56,775, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:55:56,902, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:55:56,904, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:55:57,327, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:55:58,962, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:56:11,122, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:56:11,148, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:56:11,582, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:56:11,584, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:56:11,588, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:56:11,593, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:56:11,842, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:56:11,843, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:56:12,705, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:56:16,507, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:56:16,698, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:56:16,699, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:56:16.699288', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:56:16,813, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:56:16,816, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:56:16,945, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:56:16,946, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:56:17,402, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:56:19,005, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:56:31,079, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:56:31,627, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:56:31,649, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:56:32,096, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:56:32,098, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:56:32,101, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:56:32,107, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:56:32,358, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:56:32,360, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:56:33,320, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:56:37,026, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:56:37,217, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:56:37,218, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:56:37.218306', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:56:37,335, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:56:37,338, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:56:37,442, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:56:37,443, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:56:37,811, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:56:39,440, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:56:51,393, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:56:51,411, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:56:51,785, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:56:51,787, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:56:51,791, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:56:51,797, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:56:52,051, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:56:52,052, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:56:52,936, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:56:56,430, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:56:56,613, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:56:56,614, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:56:56.614350', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:56:56,722, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:56:56,725, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:56:56,845, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:56:56,846, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:56:57,242, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:56:58,831, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:57:10,525, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:57:10,544, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:57:10,897, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:57:10,899, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:57:10,903, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:57:10,908, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:57:11,155, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:57:11,156, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:57:12,138, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:57:15,571, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:57:15,736, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:57:15,737, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:57:15.737890', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:57:15,835, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:57:15,837, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:57:15,946, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:57:15,947, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:57:16,337, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:57:17,900, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:57:29,947, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:57:30,331, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:57:30,351, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:57:30,718, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:57:30,720, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:57:30,724, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:57:30,729, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:57:30,980, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:57:30,982, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:57:31,853, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:57:35,287, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:57:35,477, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:57:35,478, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:57:35.478918', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:57:35,590, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:57:35,593, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:57:35,718, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:57:35,719, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:57:36,163, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:57:37,670, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:57:51,727, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:57:51,757, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:57:52,146, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:57:52,148, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:57:52,152, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:57:52,158, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:57:52,365, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:57:52,367, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:57:53,116, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:57:56,757, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:57:56,917, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:57:56,918, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:57:56.918360', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:57:57,006, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:57:57,008, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:57:57,108, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:57:57,109, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:57:57,465, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:57:58,790, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:58:11,001, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:58:11,029, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:58:11,419, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:58:11,421, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:58:11,425, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:58:11,431, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:58:11,679, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:58:11,680, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:58:12,606, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:58:16,216, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:58:16,416, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:58:16,417, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:58:16.417468', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:58:16,529, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:58:16,533, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:58:16,635, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:58:16,636, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:58:17,011, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:58:18,573, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:58:30,782, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:58:31,229, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:58:31,250, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:58:31,655, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:58:31,657, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:58:31,660, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:58:31,666, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:58:31,914, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:58:31,915, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:58:32,999, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:58:36,936, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:58:37,124, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:58:37,125, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:58:37.125522', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:58:37,238, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:58:37,241, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:58:37,365, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:58:37,366, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:58:37,859, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:58:39,286, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:58:51,356, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:58:51,375, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:58:51,735, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:58:51,737, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:58:51,741, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:58:51,747, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:58:52,004, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:58:52,005, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:58:52,896, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:58:56,772, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:58:56,958, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:58:56,960, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:58:56.960624', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:58:57,068, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:58:57,071, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:58:57,191, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:58:57,192, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:58:57,609, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:58:59,201, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:59:11,663, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:59:11,682, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 17:59:12,033, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:59:12,035, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:59:12,040, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:59:12,045, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:59:12,291, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:59:12,293, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:59:13,214, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:59:17,132, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:59:17,329, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 17:59:17,330, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T17:59:17.330473', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:59:17,449, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:59:17,452, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:59:17,579, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:59:17,580, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:59:17,969, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:59:19,637, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:59:31,425, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 17:59:31,842, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:59:31,863, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 17:59:32,496, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:59:32,498, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:59:32,502, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:59:32,507, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:59:32,758, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:59:32,759, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:59:33,659, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:59:37,423, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:59:37,632, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 17:59:37,633, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T17:59:37.633093', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:59:37,747, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:59:37,750, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:59:37,898, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:59:37,900, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:59:38,409, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:59:40,212, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 17:59:52,754, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 17:59:52,773, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 17:59:53,172, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 17:59:53,174, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 17:59:53,179, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:59:53,184, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:59:53,439, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:59:53,440, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:59:54,327, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 17:59:58,009, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 17:59:58,193, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 17:59:58,195, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T17:59:58.195488', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 17:59:58,301, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 17:59:58,304, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 17:59:58,424, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 17:59:58,425, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 17:59:58,845, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:00:00,501, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:00:12,763, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:00:12,780, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:00:13,188, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:00:13,190, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:00:13,194, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:00:13,200, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:00:13,454, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:00:13,455, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:00:14,436, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:00:18,254, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:00:18,451, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:00:18,452, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:00:18.452384', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:00:18,569, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:00:18,572, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:00:18,701, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:00:18,702, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:00:19,079, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:00:20,752, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:00:32,968, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:00:33,515, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:00:33,554, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:00:34,370, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:00:34,372, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:00:34,375, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:00:34,382, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:00:34,803, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:00:34,805, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:00:36,339, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:00:42,352, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:00:42,925, dictionary, INFO, built Dictionary<38290 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 5370 documents (total 441735 corpus positions) ]
[2024-12-29 18:00:42,926, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38290 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 5370 documents (total 441735 corpus positions)", 'datetime': '2024-12-29T18:00:42.926639', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:00:43,251, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:00:43,258, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:00:43,628, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:00:43,630, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:00:44,974, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:00:50,837, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:01:03,117, text_analysis, INFO, 77 batches submitted to accumulate stats from 4928 documents (-131596 virtual) ]
[2024-12-29 18:01:03,500, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:01:03,529, text_analysis, INFO, accumulated word occurrence stats for 103610 virtual documents ]
[2024-12-29 18:01:03,963, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:01:03,973, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:01:04,368, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:01:04,369, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:01:05,816, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:06:12,439, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:06:12,443, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:06:22,729, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:06:22,731, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:06:22,755, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:06:22,757, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:06:22,762, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:06:22,768, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:06:23,008, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:06:23,009, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:06:28,757, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:06:32,229, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:06:32,438, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:06:32,439, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:06:32.439425', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:06:32,559, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:06:32,562, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:06:32,692, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:06:32,693, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:06:33,157, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:06:34,842, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:06:53,844, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:06:53,865, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:06:54,267, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:06:54,268, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:06:54,273, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:06:54,280, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:06:54,554, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:06:54,556, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:06:55,728, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:06:59,683, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:06:59,892, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:06:59,894, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:06:59.894054', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:07:00,019, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:07:00,023, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:07:00,159, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:07:00,161, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:07:00,647, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:07:02,499, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:07:15,842, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:07:15,875, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:07:16,270, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:07:16,272, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:07:16,276, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:07:16,282, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:07:16,539, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:07:16,540, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:07:17,548, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:07:21,510, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:07:21,712, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:07:21,714, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:07:21.714863', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:07:21,833, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:07:21,836, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:07:21,968, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:07:21,969, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:07:22,648, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:07:24,241, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:07:40,213, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:07:40,953, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:07:40,989, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:07:41,433, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:07:41,435, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:07:41,439, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:07:41,446, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:07:41,730, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:07:41,731, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:07:42,749, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:07:46,476, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:07:46,684, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:07:46,685, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:07:46.685245', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:07:46,822, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:07:46,826, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:07:46,958, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:07:46,961, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:07:47,450, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:07:50,555, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:08:05,174, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:08:05,222, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:08:05,666, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:08:05,668, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:08:05,673, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:08:05,679, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:08:05,947, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:08:05,948, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:08:06,937, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:08:11,345, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:08:11,666, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:08:11,668, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:08:11.668596', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:08:11,807, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:08:11,812, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:08:11,960, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:08:11,961, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:08:12,487, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:08:14,427, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:08:31,198, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:08:31,228, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:08:31,588, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:08:31,590, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:08:31,595, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:08:31,600, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:08:31,842, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:08:31,844, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:08:32,790, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:08:36,726, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:08:36,930, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:08:36,931, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:08:36.931512', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:08:37,047, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:08:37,050, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:08:37,180, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:08:37,181, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:08:37,671, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:08:39,463, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:08:52,813, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:08:53,344, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:08:53,367, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:08:53,748, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:08:53,749, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:08:53,753, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:08:53,758, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:08:54,031, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:08:54,032, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:08:55,066, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:08:58,946, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:08:59,156, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:08:59,158, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:08:59.158682', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:08:59,294, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:08:59,298, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:08:59,440, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:08:59,442, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:08:59,911, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:09:01,619, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:09:15,322, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:09:15,345, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:09:15,726, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:09:15,727, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:09:15,732, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:09:15,737, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:09:16,777, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:09:16,782, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:09:19,120, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:09:22,958, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:09:23,183, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:09:23,184, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:09:23.184666', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:09:23,302, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:09:23,305, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:09:23,435, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:09:23,437, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:09:23,894, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:09:25,531, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:09:39,664, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:09:39,684, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:09:40,106, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:09:40,108, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:09:40,113, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:09:40,118, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:09:40,394, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:09:40,395, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:09:41,592, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:09:45,551, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:09:45,784, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:09:45,785, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:09:45.785949', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:09:45,918, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:09:45,921, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:09:46,061, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:09:46,062, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:09:46,551, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:09:48,352, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:10:01,503, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:10:02,015, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:10:02,049, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:10:02,489, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:10:02,491, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:10:02,495, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:10:02,501, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:10:02,753, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:10:02,754, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:10:03,665, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:10:07,161, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:10:07,322, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:10:07,323, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:10:07.323666', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:10:07,418, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:10:07,421, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:10:07,528, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:10:07,530, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:10:07,932, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:10:09,487, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:10:22,475, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:10:22,487, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:10:22,632, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:10:22,633, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:10:22,637, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:10:22,642, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:10:22,873, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:10:22,874, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:10:23,703, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:10:27,632, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:10:27,812, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:10:27,813, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:10:27.813850', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:10:27,908, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:10:27,910, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:10:28,012, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:10:28,014, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:10:28,422, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:10:30,067, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:10:44,334, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:10:44,348, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:10:44,491, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:10:44,493, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:10:44,496, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:10:44,502, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:10:44,751, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:10:44,752, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:10:45,688, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:10:49,546, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:10:49,740, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:10:49,741, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:10:49.741759', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:10:49,878, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:10:49,881, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:10:50,009, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:10:50,011, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:10:50,467, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:10:51,982, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:11:04,088, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:11:04,312, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:11:04,325, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:11:04,479, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:11:04,481, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:11:04,484, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:11:04,490, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:11:04,737, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:11:04,738, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:11:05,642, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:11:09,595, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:11:09,832, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:11:09,833, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:11:09.833353', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:11:09,954, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:11:09,957, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:11:10,084, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:11:10,086, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:11:10,640, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:11:12,304, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:11:24,910, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:11:24,944, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:11:25,187, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:11:25,189, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:11:25,192, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:11:25,197, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:11:25,427, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:11:25,428, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:11:26,283, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:11:30,248, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:11:30,433, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:11:30,434, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:11:30.434356', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:11:30,544, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:11:30,547, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:11:30,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:11:30,668, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:11:31,093, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:11:32,843, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:11:47,805, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:11:47,822, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:11:48,098, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:11:48,100, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:11:48,104, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:11:48,109, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:11:48,360, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:11:48,362, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:11:49,270, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:11:52,878, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:11:53,073, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:11:53,074, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:11:53.074770', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:11:53,194, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:11:53,197, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:11:53,318, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:11:53,320, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:11:53,699, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:11:55,312, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:12:07,959, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:12:08,298, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:12:08,316, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:12:08,596, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:12:08,597, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:12:08,601, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:12:08,606, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:12:08,854, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:12:08,855, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:12:09,768, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:12:13,231, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:12:13,429, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:12:13,430, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:12:13.430384', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:12:13,544, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:12:13,547, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:12:13,672, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:12:13,673, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:12:14,125, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:12:15,816, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:12:28,669, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:12:28,695, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:12:29,131, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:12:29,133, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:12:29,137, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:12:29,143, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:12:29,397, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:12:29,398, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:12:30,323, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:12:34,117, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:12:34,303, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:12:34,304, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:12:34.304595', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:12:34,485, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:12:34,489, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:12:34,608, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:12:34,610, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:12:35,043, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:12:36,579, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:12:49,408, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:12:49,423, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:12:49,859, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:12:49,861, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:12:49,865, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:12:49,870, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:12:50,119, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:12:50,120, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:12:51,001, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:12:54,885, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:12:55,049, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:12:55,050, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:12:55.050466', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:12:55,162, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:12:55,164, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:12:55,293, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:12:55,294, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:12:55,732, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:12:57,440, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:13:10,056, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:13:10,646, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:13:10,675, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:13:11,122, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:13:11,124, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:13:11,127, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:13:11,132, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:13:11,379, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:13:11,380, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:13:12,273, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:13:15,982, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:13:16,159, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:13:16,160, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:13:16.160810', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:13:16,274, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:13:16,277, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:13:16,408, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:13:16,409, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:13:16,922, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:13:18,592, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:13:31,019, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:13:31,049, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:13:31,401, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:13:31,403, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:13:31,407, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:13:31,413, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:13:31,667, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:13:31,668, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:13:32,676, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:13:36,387, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:13:36,573, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:13:36,574, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:13:36.574504', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:13:36,684, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:13:36,687, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:13:36,806, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:13:36,808, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:13:37,247, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:13:38,903, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:13:51,541, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:13:51,563, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:13:51,935, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:13:51,937, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:13:51,940, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:13:51,946, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:13:52,189, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:13:52,191, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:13:53,068, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:13:56,836, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:13:57,015, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:13:57,016, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:13:57.016373', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:13:57,112, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:13:57,115, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:13:57,222, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:13:57,223, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:13:57,631, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:13:59,086, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:14:11,503, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:14:11,925, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:14:11,948, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:14:12,317, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:14:12,319, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:14:12,323, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:14:12,328, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:14:12,576, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:14:12,577, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:14:13,503, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:14:17,182, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:14:17,373, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:14:17,375, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:14:17.375179', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:14:17,487, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:14:17,491, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:14:17,616, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:14:17,617, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:14:18,070, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:14:19,547, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:14:32,429, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:14:32,455, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:14:32,797, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:14:32,798, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:14:32,802, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:14:32,808, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:14:33,046, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:14:33,048, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:14:33,857, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:14:37,556, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:14:37,741, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:14:37,742, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:14:37.742057', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:14:37,852, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:14:37,856, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:14:37,976, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:14:37,977, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:14:38,405, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:14:40,056, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:14:52,474, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:14:52,500, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:14:52,895, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:14:52,897, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:14:52,901, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:14:52,907, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:14:53,151, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:14:53,152, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:14:54,267, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:14:57,963, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:14:58,162, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:14:58,163, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:14:58.163391', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:14:58,282, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:14:58,285, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:14:58,413, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:14:58,415, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:14:58,873, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:15:00,551, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:15:13,154, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:15:13,641, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:15:13,662, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:15:14,980, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:15:14,986, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:15:14,998, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:15:15,016, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:15:15,991, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:15:15,994, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:15:16,872, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:15:20,663, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:15:20,853, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:15:20,853, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:15:20.853442', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:15:20,952, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:15:20,955, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:15:21,063, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:15:21,064, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:15:21,450, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:15:23,073, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:15:35,613, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:15:35,633, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:15:35,995, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:15:35,996, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:15:36,000, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:15:36,005, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:15:36,258, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:15:36,259, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:15:37,318, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:15:41,409, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:15:41,591, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:15:41,592, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:15:41.592184', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:15:41,761, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:15:41,766, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:15:41,911, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:15:41,913, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:15:42,405, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:15:44,229, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:15:56,813, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:15:56,843, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:15:57,212, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:15:57,214, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:15:57,218, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:15:57,223, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:15:57,491, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:15:57,492, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:15:58,503, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:16:02,334, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:16:02,544, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:16:02,546, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:16:02.545611', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:16:02,643, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:16:02,647, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:16:02,754, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:16:02,755, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:16:04,163, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:16:06,357, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:16:18,599, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:16:19,058, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:16:19,091, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:16:19,619, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:16:19,621, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:16:19,624, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:16:19,630, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:16:20,367, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:16:20,371, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:16:22,180, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:16:25,881, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:16:26,047, dictionary, INFO, built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions) ]
[2024-12-29 18:16:26,048, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19325 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 1790 documents (total 147869 corpus positions)", 'datetime': '2024-12-29T18:16:26.048611', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:16:26,150, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:16:26,153, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:16:26,264, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:16:26,265, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:16:26,669, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:16:28,119, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:16:41,270, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:16:41,313, text_analysis, INFO, accumulated word occurrence stats for 35042 virtual documents ]
[2024-12-29 18:16:41,658, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:16:41,659, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:16:41,664, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:16:41,669, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:16:41,914, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:16:41,915, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:16:42,848, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:16:46,918, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:16:47,104, dictionary, INFO, built Dictionary<18517 unique tokens: ['45', 'Beyond', 'Chinese', 'I', "I've"]...> from 1790 documents (total 141612 corpus positions) ]
[2024-12-29 18:16:47,105, utils, INFO, Dictionary lifecycle event {'msg': 'built Dictionary<18517 unique tokens: [\'45\', \'Beyond\', \'Chinese\', \'I\', "I\'ve"]...> from 1790 documents (total 141612 corpus positions)', 'datetime': '2024-12-29T18:16:47.105168', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:16:47,217, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:16:47,221, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:16:47,343, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:16:47,344, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:16:47,794, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:16:49,356, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:17:02,917, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:17:02,966, text_analysis, INFO, accumulated word occurrence stats for 30613 virtual documents ]
[2024-12-29 18:17:04,438, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:17:04,440, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:17:04,445, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:17:04,450, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:17:04,675, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:17:04,677, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:17:05,430, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:17:09,273, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:17:09,468, dictionary, INFO, built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions) ]
[2024-12-29 18:17:09,469, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<19281 unique tokens: ['&', 'Everything', 'The', 'average', 'damn']...> from 1790 documents (total 152254 corpus positions)", 'datetime': '2024-12-29T18:17:09.469456', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:17:09,584, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:17:09,587, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:17:09,713, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:17:09,715, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:17:10,182, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:17:11,782, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:17:23,461, text_analysis, INFO, 21 batches submitted to accumulate stats from 1344 documents (-30832 virtual) ]
[2024-12-29 18:17:23,925, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:17:23,960, text_analysis, INFO, accumulated word occurrence stats for 37955 virtual documents ]
[2024-12-29 18:17:24,606, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2024-12-29 18:17:24,608, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2024-12-29 18:17:24,610, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:17:24,618, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:17:25,018, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:17:25,019, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:17:26,353, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:17:32,343, dictionary, INFO, adding document #0 to Dictionary<0 unique tokens: []> ]
[2024-12-29 18:17:32,848, dictionary, INFO, built Dictionary<38290 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 5370 documents (total 441735 corpus positions) ]
[2024-12-29 18:17:32,849, utils, INFO, Dictionary lifecycle event {'msg': "built Dictionary<38290 unique tokens: ['Ambience', 'C.P)', 'Food', 'Nice(nothing', 'Staff']...> from 5370 documents (total 441735 corpus positions)", 'datetime': '2024-12-29T18:17:32.849742', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'} ]
[2024-12-29 18:17:33,126, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:17:33,134, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:17:33,457, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:17:33,459, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:17:34,846, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:17:40,540, probability_estimation, INFO, using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows ]
[2024-12-29 18:17:52,780, text_analysis, INFO, 77 batches submitted to accumulate stats from 4928 documents (-131596 virtual) ]
[2024-12-29 18:17:53,128, text_analysis, INFO, 7 accumulators retrieved from output queue ]
[2024-12-29 18:17:53,143, text_analysis, INFO, accumulated word occurrence stats for 103610 virtual documents ]
[2024-12-29 18:17:53,457, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:17:53,471, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:17:53,948, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:17:53,950, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:17:55,283, text_data_transformation, INFO, Lemmatization successful ]
[2024-12-29 18:29:18,576, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2024-12-29 18:29:18,584, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2024-12-29 18:29:18,918, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2024-12-29 18:29:18,919, text_data_transformation, INFO, Lemmatizing the words ]
[2024-12-29 18:29:20,243, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 19:59:45,421, data_ingestion, INFO, Initiating data ingestion ]
[2025-01-01 19:59:45,425, data_ingestion, INFO, Establising Connection With SQL Database ]
[2025-01-01 19:59:45,429, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2025-01-01 19:59:45,431, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2025-01-01 19:59:45,920, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2025-01-01 19:59:47,708, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2025-01-01 19:59:47,709, data_ingestion, INFO, Initiating train test split ]
[2025-01-01 19:59:49,448, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2025-01-01 19:59:49,484, text_data_transformation, INFO, Initiating the DataTransformation ]
[2025-01-01 19:59:49,484, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2025-01-01 19:59:50,516, text_data_transformation, INFO, Error in initiating the data transformation pipeline 'DataColumns' object has no attribute 'num_columns' ]
[2025-01-01 20:02:59,543, data_ingestion, INFO, Initiating data ingestion ]
[2025-01-01 20:02:59,545, data_ingestion, INFO, Establising Connection With SQL Database ]
[2025-01-01 20:02:59,547, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2025-01-01 20:02:59,549, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2025-01-01 20:02:59,946, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2025-01-01 20:03:01,687, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2025-01-01 20:03:01,688, data_ingestion, INFO, Initiating train test split ]
[2025-01-01 20:03:03,407, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2025-01-01 20:03:03,437, text_data_transformation, INFO, Initiating the DataTransformation ]
[2025-01-01 20:03:03,438, text_data_transformation, INFO, Initiatig data transformation pipeline ]
[2025-01-01 20:03:04,470, text_data_transformation, INFO, Error in initiating the data transformation pipeline 'DataColumns' object has no attribute 'num_columns' ]
[2025-01-01 20:03:56,566, data_ingestion, INFO, Initiating data ingestion ]
[2025-01-01 20:03:56,568, data_ingestion, INFO, Establising Connection With SQL Database ]
[2025-01-01 20:03:56,570, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2025-01-01 20:03:56,571, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2025-01-01 20:03:57,001, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2025-01-01 20:03:58,873, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2025-01-01 20:03:58,874, data_ingestion, INFO, Initiating train test split ]
[2025-01-01 20:04:00,641, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2025-01-01 20:04:00,675, text_data_transformation, INFO, Initiating the DataTransformation ]
[2025-01-01 20:04:00,677, text_data_transformation, INFO, Initiating data transformation pipeline ]
[2025-01-01 20:04:02,287, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2025-01-01 20:04:02,303, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2025-01-01 20:04:02,304, text_data_transformation, INFO, Text pipeline creation successful ]
[2025-01-01 20:04:02,305, text_data_transformation, INFO, Fitting and transforming training data ]
[2025-01-01 20:04:02,309, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-01 20:04:02,450, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-01 20:04:09,172, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-01 20:04:09,175, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-01 20:04:37,418, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 20:04:37,419, text_data_transformation, INFO, Transforming test data ]
[2025-01-01 20:04:37,429, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-01 20:04:37,482, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-01 20:04:40,247, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-01 20:04:40,249, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-01 20:04:50,512, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 20:04:50,514, text_data_transformation, INFO, Transforming test data ]
[2025-01-01 20:04:50,585, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2025-01-01 20:04:50,586, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2025-01-01 20:04:50,587, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2025-01-01 20:10:28,695, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 20:10:28,697, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 20:10:28,698, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 20:10:30,022, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T20:10:30.022387', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 20:10:30,023, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 20:10:30,024, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 20:10:30,705, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 20:10:30,706, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 20:10:30,711, data_clustering, ERROR, Error during embedding generation: object of type 'float' has no len() ]
[2025-01-01 20:10:30,712, data_clustering, ERROR, Error during clustering pipeline: object of type 'float' has no len() ]
[2025-01-01 20:18:51,743, data_ingestion, INFO, Initiating data ingestion ]
[2025-01-01 20:18:51,745, data_ingestion, INFO, Establising Connection With SQL Database ]
[2025-01-01 20:18:51,748, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2025-01-01 20:18:51,749, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2025-01-01 20:18:52,139, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2025-01-01 20:18:53,842, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2025-01-01 20:18:53,843, data_ingestion, INFO, Initiating train test split ]
[2025-01-01 20:18:55,538, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2025-01-01 20:18:55,565, text_data_transformation, INFO, Initiating the DataTransformation ]
[2025-01-01 20:18:55,566, text_data_transformation, INFO, Initiating data transformation pipeline ]
[2025-01-01 20:18:57,112, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2025-01-01 20:18:57,114, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2025-01-01 20:18:57,115, text_data_transformation, INFO, Text pipeline creation successful ]
[2025-01-01 20:18:57,116, text_data_transformation, INFO, Fitting and transforming training data ]
[2025-01-01 20:18:57,119, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-01 20:18:57,255, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-01 20:19:04,198, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-01 20:19:04,201, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-01 20:19:29,428, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 20:19:29,429, text_data_transformation, INFO, Transforming test data ]
[2025-01-01 20:19:29,438, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-01 20:19:29,488, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-01 20:19:32,320, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-01 20:19:32,323, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-01 20:19:42,717, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 20:19:42,718, text_data_transformation, INFO, Transforming test data ]
[2025-01-01 20:19:42,786, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2025-01-01 20:19:42,788, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2025-01-01 20:19:42,789, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2025-01-01 20:21:40,340, data_ingestion, INFO, Initiating data ingestion ]
[2025-01-01 20:21:40,342, data_ingestion, INFO, Establising Connection With SQL Database ]
[2025-01-01 20:21:40,344, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2025-01-01 20:21:40,345, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2025-01-01 20:21:40,782, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2025-01-01 20:21:42,535, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2025-01-01 20:21:42,537, data_ingestion, INFO, Initiating train test split ]
[2025-01-01 20:21:44,236, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2025-01-01 20:21:44,265, text_data_transformation, INFO, Initiating the DataTransformation ]
[2025-01-01 20:21:44,267, text_data_transformation, INFO, Initiating data transformation pipeline ]
[2025-01-01 20:21:45,782, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2025-01-01 20:21:45,785, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2025-01-01 20:21:45,786, text_data_transformation, INFO, Text pipeline creation successful ]
[2025-01-01 20:21:45,787, text_data_transformation, INFO, Fitting and transforming training data ]
[2025-01-01 20:21:45,791, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-01 20:21:45,934, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-01 20:21:52,464, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-01 20:21:52,467, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-01 20:22:21,964, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 20:22:21,966, text_data_transformation, INFO, Transforming test data ]
[2025-01-01 20:22:21,976, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-01 20:22:22,026, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-01 20:22:24,731, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-01 20:22:24,734, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-01 20:22:35,367, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 20:22:35,368, text_data_transformation, INFO, Transforming test data ]
[2025-01-01 20:22:35,434, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2025-01-01 20:22:35,436, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2025-01-01 20:22:36,566, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2025-01-01 20:23:15,172, data_ingestion, INFO, Initiating data ingestion ]
[2025-01-01 20:23:15,174, data_ingestion, INFO, Establising Connection With SQL Database ]
[2025-01-01 20:23:15,176, data_ingestion, INFO, Successfully connected to the SQLite database. ]
[2025-01-01 20:23:15,178, data_ingestion, INFO, Reading New_Delhi_Reviews table  ]
[2025-01-01 20:23:15,596, data_ingestion, INFO, Successfully read the New_Delhi_Reviews as pandas dataframe ]
[2025-01-01 20:23:17,266, data_ingestion, INFO, succesfully ingested the raw data as a csv file into artifacts\raw_data.csv ]
[2025-01-01 20:23:17,267, data_ingestion, INFO, Initiating train test split ]
[2025-01-01 20:23:19,133, data_ingestion, INFO, train and test data split successful and stored respectively as csv files at artifacts\train_data.csv, artifacts\test_data.csv ]
[2025-01-01 20:23:19,170, text_data_transformation, INFO, Initiating the DataTransformation ]
[2025-01-01 20:23:19,172, text_data_transformation, INFO, Initiating data transformation pipeline ]
[2025-01-01 20:23:20,721, text_data_transformation, INFO, Getting set of stopwords from NLTK for English language ]
[2025-01-01 20:23:20,725, text_data_transformation, INFO, Instantiating WordNetLemmatizer from NLTK ]
[2025-01-01 20:23:20,726, text_data_transformation, INFO, Text pipeline creation successful ]
[2025-01-01 20:23:20,728, text_data_transformation, INFO, Fitting and transforming training data ]
[2025-01-01 20:23:20,731, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-01 20:23:20,858, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-01 20:23:27,274, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-01 20:23:27,277, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-01 20:23:55,286, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 20:23:55,287, text_data_transformation, INFO, Transforming test data ]
[2025-01-01 20:23:55,298, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-01 20:23:55,349, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-01 20:23:58,013, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-01 20:23:58,015, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-01 20:24:08,238, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-01 20:24:08,240, text_data_transformation, INFO, Transforming test data ]
[2025-01-01 20:24:08,306, text_data_transformation, INFO, Saved fitted preprocessor to artifacts\text_preprocessor.joblib ]
[2025-01-01 20:24:08,308, text_data_transformation, INFO, saving transformed train data and test data at artifacts\transformed_train_data.csv and artifacts\transformed_test_data.csv respectively ]
[2025-01-01 20:24:09,376, text_data_transformation, INFO, Returning the transformed input train feature as an array and  transfomed test feature as array respectively ]
[2025-01-01 20:25:25,857, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 20:25:25,859, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 20:25:25,860, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 20:25:27,342, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T20:25:27.342845', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 20:25:27,344, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 20:25:27,346, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 20:25:27,991, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 20:25:27,992, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 20:27:23,722, data_clustering, ERROR, Error transforming embeddings: object of type 'numpy.float32' has no len() ]
[2025-01-01 20:27:23,723, data_clustering, ERROR, Error during clustering pipeline: object of type 'numpy.float32' has no len() ]
[2025-01-01 20:36:18,993, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 20:36:18,994, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 20:36:18,996, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 20:36:20,184, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T20:36:20.184288', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 20:36:20,185, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 20:36:20,186, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 20:36:20,781, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 20:36:20,782, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 20:38:03,807, data_clustering, ERROR, Error transforming embeddings: 'DataFrame' object has no attribute 'tolist' ]
[2025-01-01 20:38:03,809, data_clustering, ERROR, Error during clustering pipeline: 'DataFrame' object has no attribute 'tolist' ]
[2025-01-01 20:38:47,556, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 20:38:47,557, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 20:38:47,559, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 20:38:48,839, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T20:38:48.839069', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 20:38:48,841, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 20:38:48,843, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 20:38:49,447, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 20:38:49,448, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 20:40:33,973, data_clustering, ERROR, Error transforming embeddings: 'DataFrame' object has no attribute 'to_list' ]
[2025-01-01 20:40:33,974, data_clustering, ERROR, Error during clustering pipeline: 'DataFrame' object has no attribute 'to_list' ]
[2025-01-01 20:42:05,782, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 20:42:05,784, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 20:42:05,785, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 20:42:06,967, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T20:42:06.967919', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 20:42:06,969, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 20:42:06,970, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 20:42:07,579, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 20:42:07,582, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 20:43:54,245, data_clustering, ERROR, Error transforming embeddings: object of type 'numpy.float32' has no len() ]
[2025-01-01 20:43:54,246, data_clustering, ERROR, Error during clustering pipeline: object of type 'numpy.float32' has no len() ]
[2025-01-01 20:44:37,771, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 20:44:37,773, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 20:44:37,774, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 20:44:38,964, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T20:44:38.964450', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 20:44:38,965, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 20:44:38,966, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 20:44:39,490, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 20:44:39,492, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 20:46:28,872, data_clustering, ERROR, Error transforming embeddings: Shape of passed values is (103304, 1), indices imply (103304, 103304) ]
[2025-01-01 20:46:28,873, data_clustering, ERROR, Error during clustering pipeline: Shape of passed values is (103304, 1), indices imply (103304, 103304) ]
[2025-01-01 20:48:25,753, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 20:48:25,756, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 20:48:25,757, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 20:48:27,097, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T20:48:27.097317', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 20:48:27,099, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 20:48:27,101, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 20:48:27,988, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 20:48:27,994, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 20:50:33,310, data_clustering, ERROR, Error transforming embeddings: Shape of passed values is (103304, 1), indices imply (103304, 26) ]
[2025-01-01 20:50:33,311, data_clustering, ERROR, Error during clustering pipeline: Shape of passed values is (103304, 1), indices imply (103304, 26) ]
[2025-01-01 20:54:39,370, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 20:54:39,372, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 20:54:39,373, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 20:54:40,579, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T20:54:40.579297', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 20:54:40,581, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 20:54:40,583, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 20:54:41,150, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 20:54:41,152, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 20:56:35,153, data_clustering, ERROR, Error transforming embeddings: Shape of passed values is (103304, 1), indices imply (103304, 26) ]
[2025-01-01 20:56:35,155, data_clustering, ERROR, Error during clustering pipeline: Shape of passed values is (103304, 1), indices imply (103304, 26) ]
[2025-01-01 21:06:20,525, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 21:06:20,527, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 21:06:20,528, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 21:06:21,913, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T21:06:21.913256', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 21:06:21,914, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 21:06:21,915, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 21:06:22,457, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 21:06:22,459, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 21:08:16,684, data_clustering, ERROR, Error transforming embeddings: Shape of passed values is (103304, 1), indices imply (103304, 26) ]
[2025-01-01 21:08:16,685, data_clustering, ERROR, Error during clustering pipeline: Shape of passed values is (103304, 1), indices imply (103304, 26) ]
[2025-01-01 21:11:21,295, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 21:11:21,297, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 21:11:21,298, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 21:11:22,508, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T21:11:22.508413', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 21:11:22,510, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 21:11:22,511, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 21:11:23,022, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 21:11:23,023, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 21:13:18,541, data_clustering, ERROR, Error transforming embeddings: Shape of passed values is (103304, 1), indices imply (103304, 26) ]
[2025-01-01 21:13:18,542, data_clustering, ERROR, Error during clustering pipeline: Shape of passed values is (103304, 1), indices imply (103304, 26) ]
[2025-01-01 21:14:04,001, data_clustering, INFO, Initializing clustering pipeline ]
[2025-01-01 21:14:04,003, data_clustering, INFO, Loading Gensim Word2Vec model for embedding generation ]
[2025-01-01 21:14:04,004, keyedvectors, INFO, loading projection weights from C:\Users\karthikeya\New_Delhi_Reviews\ft_reviews_vectors.bin ]
[2025-01-01 21:14:05,147, utils, INFO, KeyedVectors lifecycle event {'msg': 'loaded (106574, 25) matrix of type float32 from C:\\Users\\karthikeya\\New_Delhi_Reviews\\ft_reviews_vectors.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-01T21:14:05.147227', 'gensim': '4.3.3', 'python': '3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:41:22) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'} ]
[2025-01-01 21:14:05,148, data_clustering, INFO, Initializing K-Means clustering model ]
[2025-01-01 21:14:05,149, data_clustering, INFO, Loading train and test datasets ]
[2025-01-01 21:14:05,721, data_clustering, INFO, Fitting and transforming train data ]
[2025-01-01 21:14:05,723, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 21:16:00,094, data_clustering, INFO, Fitting the K-Means model ]
[2025-01-01 21:16:01,520, data_clustering, INFO, Predicting cluster labels ]
[2025-01-01 21:16:01,534, data_clustering, INFO, Saving trained model at artifacts\cluster_model.joblib ]
[2025-01-01 21:16:02,600, data_clustering, INFO, Transforming test data ]
[2025-01-01 21:16:02,601, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-01 21:16:50,065, data_clustering, INFO, Predicting cluster labels ]
[2025-01-01 21:16:50,079, data_clustering, INFO, Attaching cluster labels to datasets ]
[2025-01-01 21:16:51,226, data_clustering, INFO, Clustering pipeline completed successfully ]
[2025-01-02 17:33:40,670, prediction_pipeline, INFO, Loading model... ]
[2025-01-02 17:33:49,158, prediction_pipeline, INFO, Model loaded successfully. ]
[2025-01-02 17:33:49,159, prediction_pipeline, INFO, Loading preprocessor... ]
[2025-01-02 17:33:49,257, prediction_pipeline, INFO, Preprocessor loaded successfully. ]
[2025-01-02 17:33:49,260, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-02 17:33:49,260, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-02 17:33:49,261, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-02 17:33:49,266, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-02 17:33:52,290, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-02 17:33:52,291, prediction_pipeline, ERROR, Error loading artifacts: This 'Pipeline' has no attribute 'predict' ]
[2025-01-02 17:36:36,703, prediction_pipeline, INFO, Loading model... ]
[2025-01-02 17:36:39,818, prediction_pipeline, INFO, Model loaded successfully. ]
[2025-01-02 17:36:39,818, prediction_pipeline, INFO, Loading preprocessor... ]
[2025-01-02 17:36:39,886, prediction_pipeline, INFO, Preprocessor loaded successfully. ]
[2025-01-02 17:36:39,889, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-02 17:36:39,889, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-02 17:36:39,891, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-02 17:36:39,895, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-02 17:36:42,900, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-02 17:36:42,901, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-02 17:36:42,903, data_clustering, INFO, Predicting cluster labels ]
[2025-01-02 17:48:09,003, prediction_pipeline, INFO, Loading model... ]
[2025-01-02 17:48:09,396, prediction_pipeline, INFO, Model loaded successfully. ]
[2025-01-02 17:48:09,396, prediction_pipeline, INFO, Loading preprocessor... ]
[2025-01-02 17:48:09,457, prediction_pipeline, INFO, Preprocessor loaded successfully. ]
[2025-01-02 17:48:09,459, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-02 17:48:09,460, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-02 17:48:09,461, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-02 17:48:09,465, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-02 17:48:09,466, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-02 17:48:09,467, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-02 17:48:09,469, data_clustering, INFO, Predicting cluster labels ]
[2025-01-02 19:15:01,011, prediction_pipeline, INFO, Loading model... ]
[2025-01-02 19:15:04,277, prediction_pipeline, INFO, Model loaded successfully. ]
[2025-01-02 19:15:04,277, prediction_pipeline, INFO, Loading preprocessor... ]
[2025-01-02 19:15:04,344, prediction_pipeline, INFO, Preprocessor loaded successfully. ]
[2025-01-02 19:15:04,347, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-02 19:15:04,348, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-02 19:15:04,349, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-02 19:15:04,354, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-02 19:15:07,360, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-02 19:15:07,362, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-02 19:15:07,364, data_clustering, INFO, Predicting cluster labels ]
[2025-01-02 19:16:08,668, prediction_pipeline, INFO, Loading model... ]
[2025-01-02 19:16:12,025, prediction_pipeline, INFO, Model loaded successfully. ]
[2025-01-02 19:16:12,026, prediction_pipeline, INFO, Loading preprocessor... ]
[2025-01-02 19:16:12,091, prediction_pipeline, INFO, Preprocessor loaded successfully. ]
[2025-01-02 19:16:12,095, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-02 19:16:12,096, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-02 19:16:12,098, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-02 19:16:12,102, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-02 19:16:15,011, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-02 19:16:15,012, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-02 19:16:15,015, data_clustering, INFO, Predicting cluster labels ]
[2025-01-02 19:16:54,769, prediction_pipeline, INFO, Loading model... ]
[2025-01-02 19:16:57,901, prediction_pipeline, INFO, Model loaded successfully. ]
[2025-01-02 19:16:57,901, prediction_pipeline, INFO, Loading preprocessor... ]
[2025-01-02 19:16:57,967, prediction_pipeline, INFO, Preprocessor loaded successfully. ]
[2025-01-02 19:16:57,971, text_data_transformation, INFO, Transforming all the letters into lowercase ]
[2025-01-02 19:16:57,972, text_data_transformation, INFO, Tokenizing and removing stopwords ]
[2025-01-02 19:16:57,974, text_data_transformation, INFO, Text cleaning and stopword removal successful ]
[2025-01-02 19:16:57,978, text_data_transformation, INFO, Lemmatizing the words ]
[2025-01-02 19:17:00,920, text_data_transformation, INFO, Lemmatization successful ]
[2025-01-02 19:17:00,921, data_clustering, INFO, Generating embeddings for the text data ]
[2025-01-02 19:17:00,924, data_clustering, INFO, Predicting cluster labels ]
